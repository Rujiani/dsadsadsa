================================================================================
                    КОНСПЕКТ: ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В R
                              Лабораторная работа 7
================================================================================

--------------------------------------------------------------------------------
                         ЧАСТЬ 1: ТЕОРЕТИЧЕСКИЕ ОСНОВЫ
--------------------------------------------------------------------------------

ЧТО ТАКОЕ ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ?
===================================

Логистическая регрессия — это метод машинного обучения для КЛАССИФИКАЦИИ.
В отличие от линейной регрессии, которая предсказывает числа, логистическая
регрессия предсказывает ВЕРОЯТНОСТЬ того, что событие произойдет (0 или 1).

Пример из нашей задачи:
- MYDEPV = 1 означает, что человек принял предложение
- MYDEPV = 0 означает, что человек отказался

Мы хотим предсказать: какова вероятность, что человек примет предложение,
учитывая его возраст, доход и цену товара?


ЗАЧЕМ НУЖНА ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ?
====================================

1. Предсказание вероятности — не просто "да/нет", а "насколько вероятно"
2. Интерпретируемость — можно понять влияние каждого фактора
3. Работает с категориальными и числовыми признаками
4. Широко используется в медицине, маркетинге, финансах


МАТЕМАТИКА ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ
==================================

1. ПРОБЛЕМА ЛИНЕЙНОЙ РЕГРЕССИИ
-------------------------------
Линейная регрессия: y = b0 + b1*x1 + b2*x2 + ...
Проблема: результат может быть любым числом (-100, 500, и т.д.)
А нам нужна вероятность от 0 до 1!

2. РЕШЕНИЕ: ЛОГИСТИЧЕСКАЯ ФУНКЦИЯ (СИГМОИДА)
--------------------------------------------
Сигмоида "сжимает" любое число в диапазон от 0 до 1:

                    1
    P(y=1) = ---------------
              1 + e^(-z)

где z = b0 + b1*x1 + b2*x2 + ... (линейная комбинация)

Как это работает:
- Если z очень большое положительное → P близка к 1
- Если z очень большое отрицательное → P близка к 0
- Если z = 0 → P = 0.5

График сигмоиды — это S-образная кривая:
- Плавно растёт от 0 до 1
- В середине (z=0) круче всего
- На краях почти горизонтальна


ЧТО ТАКОЕ ODDS (ШАНСЫ)?
=======================

Odds = P / (1 - P)

Пример: если вероятность P = 0.75 (75%), то:
Odds = 0.75 / 0.25 = 3

Это значит: шансы успеха 3 к 1 (3:1)
На каждые 3 успеха приходится 1 неудача.

Другие примеры:
- P = 0.5 → Odds = 1 (равные шансы)
- P = 0.9 → Odds = 9 (9 к 1)
- P = 0.1 → Odds = 0.111 (1 к 9)


ЧТО ТАКОЕ ODDS-RATIO (ОТНОШЕНИЕ ШАНСОВ)?
=========================================

Odds-ratio показывает, во сколько раз изменяются шансы при изменении 
независимой переменной на 1 единицу.

Odds-ratio = e^(коэффициент)

Пример: если коэффициент Income = 0.05, то:
Odds-ratio = e^0.05 ≈ 1.051

Это значит: при увеличении дохода на 1 единицу, шансы успеха 
увеличиваются в 1.051 раза (или на 5.1%).

Интерпретация:
- Odds-ratio > 1 → шансы увеличиваются
- Odds-ratio < 1 → шансы уменьшаются
- Odds-ratio = 1 → шансы не меняются


LOG-ODDS (ЛОГАРИФМ ШАНСОВ)
===========================

В логистической регрессии мы работаем с log-odds:

log(Odds) = log(P / (1-P)) = b0 + b1*x1 + b2*x2 + ...

Это линейная комбинация! Поэтому:
- Коэффициенты интерпретируются как изменение log-odds
- Чтобы получить odds-ratio, берём exp(коэффициент)


ПОЧЕМУ МЕТОД НАИМЕНЬШИХ КВАДРАТОВ (МНК) НЕ ПОДХОДИТ?
====================================================

В линейной регрессии мы используем метод наименьших квадратов (МНК) для
оценки коэффициентов. Но для логистической регрессии МНК НЕ ПОДХОДИТ!
Вот почему:

1. БИНАРНАЯ ЗАВИСИМАЯ ПЕРЕМЕННАЯ
---------------------------------
Проблема: В логистической регрессии зависимая переменная принимает только
два значения: 0 или 1. Это НЕ непрерывная переменная!

МНК предполагает:
- Непрерывную зависимую переменную
- Нормальное распределение ошибок
- Постоянную дисперсию ошибок (гомоскедастичность)

Для бинарных данных:
- Переменная дискретная (0 или 1)
- Ошибки не могут быть нормально распределены
- Дисперсия зависит от вероятности: Var(Y) = P(1-P)

Пример:
Если P = 0.5, то Var = 0.25 (максимум)
Если P = 0.1 или 0.9, то Var = 0.09 (меньше)
Это ГЕТЕРОСКЕДАСТИЧНОСТЬ — дисперсия не постоянна!


2. НАРУШЕНИЕ ОГРАНИЧЕНИЙ ВЕРОЯТНОСТИ
-------------------------------------
Проблема: Вероятность должна быть в диапазоне [0, 1].

Если использовать линейную регрессию напрямую:
    P = b0 + b1*x1 + b2*x2 + ...

Результат может быть:
- Отрицательным (P < 0) — бессмысленно!
- Больше 1 (P > 1) — бессмысленно!

Пример:
Если b0 = -2, b1 = 0.1, x1 = 50, то P = -2 + 0.1*50 = 3
Вероятность 3 (300%) — это абсурд!

Логистическая функция решает это:
    P = 1 / (1 + e^(-z))
Всегда в диапазоне [0, 1], независимо от z!


3. НЕЛИНЕЙНАЯ СВЯЗЬ
--------------------
Проблема: Связь между предикторами и вероятностью НЕЛИНЕЙНАЯ.

В линейной регрессии:
- Изменение x на 1 единицу → постоянное изменение y
- Связь линейная

В логистической регрессии:
- Изменение x на 1 единицу → изменение P зависит от текущего значения P!
- Связь нелинейная (S-образная кривая)

Пример:
- При P = 0.1: увеличение x на 1 → P может стать 0.15 (изменение +0.05)
- При P = 0.5: увеличение x на 1 → P может стать 0.65 (изменение +0.15)
- При P = 0.9: увеличение x на 1 → P может стать 0.93 (изменение +0.03)

Изменение НЕ постоянно! МНК не может это учесть.


4. НОРМАЛЬНОСТЬ ОШИБОК
-----------------------
Проблема: МНК предполагает нормальное распределение ошибок.

Для бинарных данных ошибки распределены по БЕРНУЛЛИ:
- Если реальное значение 1, ошибка = 1 - P (с вероятностью P)
- Если реальное значение 0, ошибка = 0 - P = -P (с вероятностью 1-P)

Это НЕ нормальное распределение!
- Ошибки могут быть только двух типов: (1-P) или (-P)
- Нет непрерывного распределения

МНК не работает, потому что нарушено основное предположение!


5. МЕТОД МАКСИМАЛЬНОГО ПРАВДОПОДОБИЯ (MLE)
-------------------------------------------
Решение: Вместо МНК используется метод максимального правдоподобия (MLE).

Почему MLE лучше для логистической регрессии:

1. Учитывает бинарную природу данных
   - Работает с распределением Бернулли
   - Не требует нормальности ошибок

2. Гарантирует вероятности в [0, 1]
   - Логистическая функция автоматически ограничивает результат

3. Учитывает нелинейность
   - Работает с log-odds, которые линейны
   - Преобразование через сигмоиду даёт нелинейную связь

4. Статистически оптимален
   - MLE даёт наилучшие оценки (асимптотически эффективные)
   - Для больших выборок — минимальная дисперсия оценок

5. Учитывает гетероскедастичность
   - Веса наблюдений зависят от предсказанной вероятности
   - Наблюдения с P близкой к 0.5 имеют больший вес


СРАВНЕНИЕ МНК И MLE
===================

Метод наименьших квадратов (МНК):
- Минимизирует: Σ(y_i - ŷ_i)²
- Предполагает: нормальные ошибки, постоянная дисперсия
- Для: непрерывных зависимых переменных
- Пример: линейная регрессия

Метод максимального правдоподобия (MLE):
- Максимизирует: вероятность наблюденных данных
- Предполагает: распределение Бернулли для бинарных данных
- Для: дискретных зависимых переменных
- Пример: логистическая регрессия

Формула правдоподобия для логистической регрессии:
L = Π [P_i^y_i * (1-P_i)^(1-y_i)]

где:
- P_i — предсказанная вероятность для наблюдения i
- y_i — реальное значение (0 или 1)

Логарифм правдоподобия (удобнее для максимизации):
log(L) = Σ [y_i * log(P_i) + (1-y_i) * log(1-P_i)]

MLE находит коэффициенты, которые МАКСИМИЗИРУЮТ эту функцию.


ВЫВОД
=====

МНК не подходит для логистической регрессии, потому что:
1. Зависимая переменная бинарная, а не непрерывная
2. Вероятности должны быть в [0, 1], а МНК может дать значения вне этого диапазона
3. Связь нелинейная, а МНК предполагает линейность
4. Ошибки не нормально распределены
5. Дисперсия не постоянна (гетероскедастичность)

Решение: использовать метод максимального правдоподобия (MLE), который:
- Учитывает бинарную природу данных
- Гарантирует корректные вероятности
- Статистически оптимален
- Работает с нелинейными связями


--------------------------------------------------------------------------------
                         ЧАСТЬ 2: РАЗБОР КОДА ПОСТРОЧНО
--------------------------------------------------------------------------------

ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
============================

setwd("/home/romank/studDir/DS_R/lab7/7_R")
    ^ Устанавливает рабочую директорию — папку, откуда R будет читать файлы

survey <- read.csv("survey.csv")
    ^ Читает CSV файл и сохраняет в переменную survey
    ^ survey теперь — это data.frame (таблица данных)
    ^ Структура: MYDEPV, Price, Income, Age

str(survey)
    ^ Показывает структуру данных:
      - Сколько строк и столбцов
      - Какого типа каждая колонка (int, num, factor и т.д.)

head(survey)
    ^ Показывает первые 6 строк таблицы

summary(survey)
    ^ Показывает статистику по каждой колонке:
      - Минимум, максимум, среднее, медиана
      - Для факторов — частоты каждого уровня

survey$Price <- as.factor(survey$Price)
    ^ Превращает Price в категориальную переменную (фактор)
    ^ Price имеет значения 10, 20, 30 — это КАТЕГОРИИ, а не числа!
    ^ R автоматически создаст dummy-переменные (Price20, Price30)
    ^ Price10 станет базовым уровнем (reference level)


ПОСТРОЕНИЕ МОДЕЛИ
=================

model <- glm(MYDEPV ~ Price + Income + Age, data = survey, family = binomial)

Разберем по частям:
- glm() — Generalized Linear Model (обобщенная линейная модель)
- MYDEPV ~ Price + Income + Age — формула модели:
    ^ MYDEPV — зависимая переменная (что предсказываем)
    ^ ~ — "зависит от"
    ^ Price + Income + Age — независимые переменные (предикторы)
- data = survey — откуда брать данные
- family = binomial — тип модели (логистическая регрессия)
    ^ binomial означает бинарную классификацию (0 или 1)

Что происходит внутри:
1. R создаёт dummy-переменные для Price (Price20, Price30)
2. Оценивает коэффициенты методом максимального правдоподобия
3. Сохраняет модель в объект model


АНАЛИЗ РЕЗУЛЬТАТОВ
==================

summary(model)
    ^ Выводит подробную информацию о модели:
    
    Coefficients:
                Estimate  Std. Error  z value  Pr(>|z|)
    (Intercept) -5.123    0.789       -6.49    ***
    Price20     -0.234    0.198       -1.18    
    Price30     -0.567    0.201       -2.82    **
    Income       0.052    0.008        6.50    ***
    Age          0.045    0.012        3.75    ***
    
    Что означает каждый столбец:
    - Estimate: коэффициент (b) — изменение log-odds
    - Std. Error: стандартная ошибка (насколько точно измерен коэффициент)
    - z value: z-статистика (Estimate / Std. Error)
    - Pr(>|z|): p-value (вероятность, что коэффициент = 0)
        * < 0.05 — значимый (*)
        * < 0.01 — очень значимый (**)
        * < 0.001 — высоко значимый (***)
    
    Также показывает:
    - Null deviance — отклонение модели без предикторов
    - Residual deviance — отклонение текущей модели
    - AIC — информационный критерий (меньше = лучше)

coef(model)
    ^ Возвращает только коэффициенты модели (вектор)


ДИАГНОСТИЧЕСКИЕ ГРАФИКИ МОДЕЛИ
================================

par(mfrow = c(2, 2))
plot(model, which = 1:4)
par(mfrow = c(1, 1))

Команда plot(model) создаёт 4 диагностических графика для проверки
качества модели логистической регрессии. Эти графики помогают выявить
проблемы с моделью: выбросы, нелинейность, гетероскедастичность и т.д.

par(mfrow = c(2, 2))
    ^ Разделяет окно графика на 2x2 = 4 части
    ^ Позволяет показать все 4 графика одновременно

plot(model, which = 1:4)
    ^ Создаёт 4 диагностических графика:
    ^ which = 1:4 — какие графики показывать (все 4)

par(mfrow = c(1, 1))
    ^ Возвращает окно графика к обычному режиму (1 график)


1. RESIDUALS VS FITTED (Остатки против предсказанных значений)
================================================================

Что показывает:
- По оси X: предсказанные значения (fitted values) — вероятности P
- По оси Y: остатки Пирсона (Pearson residuals)
- Красная линия: сглаженная кривая (LOESS) — показывает тренд

Как интерпретировать:

ХОРОШО:
- Точки случайно разбросаны вокруг нуля
- Красная линия горизонтальна (нет тренда)
- Нет явных паттернов

ПЛОХО (как на картинке):
- U-образная или дугообразная форма
- Красная линия изогнута
- Систематический паттерн

Что это значит:
- Модель может быть неправильно специфицирована
- Возможно, не хватает важных предикторов
- Возможно, нужны взаимодействия между переменными
- Возможно, связь нелинейная

Пример на картинке:
- Видна U-образная форма
- Красная линия следует этому паттерну
- Это указывает на проблему с моделью


2. Q-Q RESIDUALS (Квантиль-квантиль график остатков)
====================================================

Что показывает:
- По оси X: теоретические квантили нормального распределения
- По оси Y: абсолютные стандартизированные остатки девиансы
- Пунктирная линия: идеальное нормальное распределение

Как интерпретировать:

ХОРОШО:
- Точки лежат на диагональной линии
- Нет сильных отклонений

ПЛОХО (как на картинке):
- Точки в верхнем правом углу сильно отклоняются вверх
- Хвост распределения "тяжелее", чем у нормального

Что это значит:
- Остатки не распределены нормально
- Есть выбросы (outliers) — наблюдения, которые плохо предсказываются
- Модель плохо подходит для некоторых наблюдений

Пример на картинке:
- Верхний правый угол: точки далеко от линии
- Наблюдения 150, 60, 121, 10 — потенциальные выбросы


3. SCALE-LOCATION (Масштаб-локация)
====================================

Что показывает:
- По оси X: предсказанные значения (fitted values)
- По оси Y: квадратный корень из абсолютных стандартизированных остатков Пирсона
- Красная линия: сглаженная кривая (LOESS)

Как интерпретировать:

ХОРОШО:
- Точки случайно разбросаны
- Красная линия горизонтальна
- Нет зависимости разброса от предсказанных значений

ПЛОХО (как на картинке):
- V-образная или воронкообразная форма
- Красная линия изогнута
- Разброс остатков меняется в зависимости от предсказанных значений

Что это значит:
- ГЕТЕРОСКЕДАСТИЧНОСТЬ — дисперсия остатков не постоянна
- Для некоторых диапазонов вероятностей модель работает хуже
- Модель менее надёжна в определённых областях

Пример на картинке:
- V-образная форма
- Разброс больше на краях (низкие и высокие вероятности)
- Наблюдения 121, 150, 36 — с большими остатками


4. COOK'S DISTANCE (Расстояние Кука)
=====================================

Что показывает:
- По оси X: номер наблюдения (observation number)
- По оси Y: расстояние Кука (Cook's distance)
- Столбчатая диаграмма для каждого наблюдения

Как интерпретировать:

ХОРОШО:
- Большинство наблюдений имеют низкое расстояние Кука
- Нет явных выбросов

ПЛОХО (как на картинке):
- Некоторые наблюдения имеют очень высокое расстояние Кука
- Эти наблюдения сильно влияют на коэффициенты модели

Что это значит:
- Влиятельные наблюдения (influential observations)
- Если удалить эти наблюдения, коэффициенты модели сильно изменятся
- Эти точки "тянут" модель в свою сторону

Правило большого пальца:
- Cook's distance > 4/n (где n — количество наблюдений) — подозрительно
- Cook's distance > 1 — очень влиятельное наблюдение

Пример на картинке:
- Наблюдения 36, 121, 91 — самые влиятельные
- Они могут искажать результаты модели

ЧТО ДЕЛАТЬ С ПРОБЛЕМАМИ?
=========================

1. Если есть выбросы (Q-Q plot):
   - Проверить данные на ошибки
   - Рассмотреть удаление выбросов (осторожно!)
   - Использовать робастные методы

2. Если есть нелинейность (Residuals vs Fitted):
   - Добавить квадратичные члены (например, Age²)
   - Добавить взаимодействия (например, Income*Age)
   - Использовать сплайны или другие нелинейные методы

3. Если есть гетероскедастичность (Scale-Location):
   - Это нормально для логистической регрессии!
   - Дисперсия остатков зависит от вероятности: Var = P(1-P)
   - Это не критично для логистической регрессии

4. Если есть влиятельные наблюдения (Cook's distance):
   - Проверить, нет ли ошибок в данных
   - Рассмотреть удаление (если это явные ошибки)
   - Или оставить, если данные корректны

ВАЖНО ДЛЯ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ:
===================================

Для логистической регрессии диагностические графики интерпретируются
ИНАЧЕ, чем для линейной регрессии:

1. U-образная форма в Residuals vs Fitted — ЧАСТО НОРМАЛЬНА
   - Это связано с бинарной природой данных
   - Не всегда означает проблему с моделью

2. Гетероскедастичность — ОЖИДАЕМА
   - Дисперсия остатков зависит от вероятности
   - Это математическое свойство бинарных данных

3. Ненормальность остатков — НОРМАЛЬНА
   - Остатки не могут быть нормальными для бинарных данных
   - Это не проблема, если модель хорошо предсказывает

4. Выбросы — нужно проверять
   - Могут быть реальными (редкие случаи)
   - Или ошибками в данных

ВЫВОД:
======
Диагностические графики помогают понять, насколько хорошо модель
подходит к данным. Но для логистической регрессии некоторые паттерны
(например, U-образная форма) могут быть нормальными и не указывать
на проблемы. Главное — проверить выбросы и влиятельные наблюдения.


ИНТЕРПРЕТАЦИЯ КОЭФФИЦИЕНТОВ
===========================

# Для Income:
income_coef <- coef(model)["Income"]
income_or_change <- (exp(income_coef) - 1) * 100

Пошагово:
1. income_coef = 0.052 (например)
2. exp(0.052) = 1.0534 — это odds-ratio
3. (1.0534 - 1) * 100 = 5.34%

Интерпретация: При увеличении дохода на 1 единицу (1000$), 
шансы принятия предложения увеличиваются на 5.34%

# Для Price30 (по сравнению с Price10):
price30_coef <- coef(model)["Price30"]
price30_or_change <- (exp(price30_coef) - 1) * 100

Если price30_coef = -0.567:
- exp(-0.567) = 0.567
- (0.567 - 1) * 100 = -43.3%

Интерпретация: При увеличении цены с $10 до $30, 
шансы принятия предложения УМЕНЬШАЮТСЯ на 43.3%


РЕЛЕВЕЛИНГ (ИЗМЕНЕНИЕ БАЗОВОГО УРОВНЯ)
=======================================

survey$Price <- relevel(survey$Price, ref = "30")

relevel() меняет базовый уровень фактора.
- До: Price10 — базовый, коэффициенты для Price20 и Price30
- После: Price30 — базовый, коэффициенты для Price10 и Price20

Зачем это нужно?
- Чтобы сравнивать разные уровни цен между собой
- Коэффициенты показывают разницу относительно базового уровня
- Иногда удобнее иметь другой базовый уровень для интерпретации

model_relevel <- glm(MYDEPV ~ Price + Income + Age, data = survey, family = binomial)
    ^ Пересчитываем модель с новым базовым уровнем
    ^ Коэффициенты Price изменятся, но Income и Age останутся теми же


ROC КРИВАЯ И AUC
================

library(pROC)
    ^ Подключает библиотеку для ROC-анализа

predicted_probs <- predict(model, type = "response")
    ^ Получает предсказанные вероятности для каждой строки
    ^ type = "response" — возвращает вероятности (от 0 до 1)
    ^ type = "link" — вернул бы log-odds (z-значения)

roc_obj <- roc(survey$MYDEPV, predicted_probs)
    ^ Создает ROC объект
    ^ Сравнивает реальные значения (MYDEPV) с предсказанными вероятностями

plot(roc_obj, main = "ROC Curve", col = "blue", lwd = 2)
    ^ Рисует ROC кривую
    ^ main — заголовок графика
    ^ col — цвет линии
    ^ lwd — толщина линии

auc(roc_obj)
    ^ Вычисляет Area Under Curve (площадь под кривой)
    ^ AUC от 0.5 до 1.0
    ^ 0.5 — модель не лучше случайного угадывания
    ^ 0.7-0.8 — приемлемая модель
    ^ 0.8-0.9 — хорошая модель
    ^ > 0.9 — отличная модель

ЧТО ТАКОЕ ROC КРИВАЯ?
ROC (Receiver Operating Characteristic) показывает баланс между:
- TPR (True Positive Rate) = TP / (TP + FN) — чувствительность
- FPR (False Positive Rate) = FP / (FP + TN) — 1 - специфичность

Как строится:
1. Модель выдаёт вероятности (не просто 0/1)
2. Меняем порог (threshold) от 0 до 1
3. Для каждого порога считаем TPR и FPR
4. Рисуем точки (FPR, TPR)

Интерпретация:
- Диагональ (линия от 0,0 до 1,1) — случайный классификатор
- Чем ближе к левому верхнему углу — тем лучше
- Идеальная модель проходит через точку (0, 1)


ПРЕДСКАЗАНИЯ ДЛЯ НОВЫХ ДАННЫХ
==============================

new_data <- data.frame(Age = 30, Income = 50, Price = factor("20", levels = c("10", "20", "30")))
    ^ Создаем новый data.frame с данными для предсказания
    ^ ВАЖНО: Price должен быть фактором с теми же уровнями, что и в модели!

predict(model, newdata = new_data, type = "response")
    ^ Предсказывает вероятность для новых данных
    ^ newdata — данные для предсказания
    ^ type = "response" — возвращает вероятность


ВИЗУАЛИЗАЦИЯ ЗАВИСИМОСТЕЙ
==========================

# Создаем последовательность значений Age
age_seq <- seq(min(survey$Age), max(survey$Age), length.out = 100)
    ^ seq() создает последовательность чисел
    ^ от минимума до максимума Age
    ^ length.out = 100 — создать 100 точек

mean_income <- mean(survey$Income)
    ^ Вычисляем средний доход (держим постоянным)

# Создаем data.frame для предсказания
pred_age <- data.frame(
  Age = age_seq,
  Income = mean_income,
  Price = factor("30", levels = c("10", "20", "30"))
)
    ^ Таблица со 100 строками
    ^ Age меняется, Income и Price фиксированы

pred_age$probability <- predict(model, newdata = pred_age, type = "response")
    ^ Добавляем колонку с предсказанными вероятностями

plot(pred_age$Age, pred_age$probability, type = "l", 
     xlab = "Age", ylab = "Probability", 
     main = "Predicted Probability vs Age (Price=30, Mean Income)")
    ^ Строим график
    ^ type = "l" — линия (line)
    ^ xlab, ylab — подписи осей
    ^ main — заголовок

Аналогично для Income:
income_seq <- seq(min(survey$Income), max(survey$Income), length.out = 100)
mean_age <- mean(survey$Age)
pred_income <- data.frame(
  Age = mean_age,
  Income = income_seq,
  Price = factor("30", levels = c("10", "20", "30"))
)
pred_income$probability <- predict(model, newdata = pred_income, type = "response")

plot(pred_income$Income, pred_income$probability, type = "l",
     xlab = "Income", ylab = "Probability",
     main = "Predicted Probability vs Income (Price=30, Mean Age)")


КЛАССИФИКАЦИЯ
=============

survey$prediction_prob <- predict(model, type = "response")
    ^ Сохраняем предсказанные вероятности в таблицу

survey$prediction_class <- ifelse(survey$prediction_prob > 0.5, 1, 0)
    ^ Классифицируем: если P > 0.5, то класс 1, иначе 0
    ^ 0.5 — порог (threshold), можно менять
    ^ ifelse(условие, значение_если_да, значение_если_нет)

# Матрица ошибок (Confusion Matrix)
confusion_matrix <- table(Actual = survey$MYDEPV, Predicted = survey$prediction_class)
    ^ Создает таблицу сопряженности
    ^ Строки = реальные классы (Actual)
    ^ Столбцы = предсказанные классы (Predicted)

Пример матрицы:
              Predicted
Actual         0    1
     0       350   50    <- 350 правильно отвергнуты, 50 ложно приняты
     1        80  270    <- 80 ложно отвергнуты, 270 правильно приняты

Метрики из матрицы:
- True Negatives (TN) = 350 (правильно предсказали 0)
- False Positives (FP) = 50 (ошибочно предсказали 1)
- False Negatives (FN) = 80 (ошибочно предсказали 0)
- True Positives (TP) = 270 (правильно предсказали 1)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    ^ Точность модели
    ^ diag() — диагональ матрицы (правильные предсказания)
    ^ sum() — сумма всех элементов (общее количество)


ПРАВИЛО "PROBABILITY MASS EQUALS COUNTS"
========================================

sum_mydepv <- sum(survey$MYDEPV)
    ^ Сумма реальных значений (количество успехов в данных)

sum_predictions <- sum(survey$prediction_prob)
    ^ Сумма предсказанных вероятностей

ВАЖНОЕ СВОЙСТВО ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ:
Сумма предсказанных вероятностей = Количество реальных успехов

Почему? Это математическое свойство метода максимального правдоподобия,
который используется для оценки коэффициентов логистической регрессии.
Модель "калибруется" так, чтобы общее количество предсказанных 
вероятностей совпадало с реальным количеством успехов.

Проверка:
abs(sum_mydepv - sum_predictions)
    ^ Разница должна быть очень маленькой (ошибки округления)


РАСЧЕТ ODDS-RATIO ДЛЯ КАЖДОЙ СТРОКИ
====================================

survey$odds_ratio <- survey$prediction_prob / (1 - survey$prediction_prob)
    ^ Odds = P / (1 - P)
    ^ Если P = 0.75, то Odds = 0.75 / 0.25 = 3

head(survey[, c("MYDEPV", "prediction_prob", "odds_ratio")])
    ^ Показывает первые 6 строк с выбранными колонками
    ^ [, c(...)] — выбор колонок по именам


ПРЕДСКАЗАНИЕ ДЛЯ КОНКРЕТНОГО ЧЕЛОВЕКА
=====================================

new_person <- data.frame(
  Age = 25,
  Income = 58,
  Price = factor("20", levels = c("10", "20", "30"))
)
prob_new_person <- predict(model, newdata = new_person, type = "response")

Предсказываем вероятность для человека:
- 25 лет
- Доход $58,000
- Цена предложения $20

Результат — одно число (вероятность от 0 до 1)


--------------------------------------------------------------------------------
                         ЧАСТЬ 3: ВАЖНЫЕ КОНЦЕПЦИИ
--------------------------------------------------------------------------------

DUMMY-ПЕРЕМЕННЫЕ (ФИКТИВНЫЕ ПЕРЕМЕННЫЕ)
========================================

Когда у нас категориальная переменная (Price: 10, 20, 30), 
R автоматически создает dummy-переменные:

Price10 | Price20 | Price30 | Что это значит
--------|---------|---------|---------------
   1    |    0    |    0    | Price = 10 (базовый)
   0    |    1    |    0    | Price = 20
   0    |    0    |    1    | Price = 30

НО! В модель включаются только k-1 переменных (где k — число категорий).
Базовый уровень (Price10) становится "точкой отсчета".

Почему k-1, а не k?
- Если включить все k переменных, будет мультиколлинеарность
- Базовый уровень можно восстановить из остальных
- Это стандартная практика в статистике


ИНТЕРПРЕТАЦИЯ КОЭФФИЦИЕНТОВ
===========================

Для ЧИСЛОВЫХ переменных (Income, Age):
- Коэффициент = изменение log-odds при увеличении переменной на 1
- exp(коэффициент) = во сколько раз меняются odds
- (exp(коэффициент) - 1) * 100 = процентное изменение odds

Для КАТЕГОРИАЛЬНЫХ переменных (Price):
- Коэффициент = разница в log-odds по сравнению с базовым уровнем
- exp(коэффициент) = во сколько раз odds этой категории 
  отличаются от odds базовой категории
- Отрицательный коэффициент → odds меньше, чем у базового уровня


ПОЛОЖИТЕЛЬНЫЙ VS ОТРИЦАТЕЛЬНЫЙ КОЭФФИЦИЕНТ
==========================================

Положительный коэффициент:
- При увеличении переменной → вероятность успеха РАСТЕТ
- exp(коэффициент) > 1
- Odds-ratio > 1

Отрицательный коэффициент:
- При увеличении переменной → вероятность успеха ПАДАЕТ
- exp(коэффициент) < 1
- Odds-ratio < 1

Примеры:
- Income: коэффициент = 0.05 → положительный → больше доход = больше вероятность
- Price30: коэффициент = -0.57 → отрицательный → выше цена = меньше вероятность


МЕТОД МАКСИМАЛЬНОГО ПРАВДОПОДОБИЯ
==================================

Логистическая регрессия использует метод максимального правдоподобия (MLE)
для оценки коэффициентов.

Идея:
1. Берём начальные значения коэффициентов
2. Вычисляем вероятность наблюденных данных при этих коэффициентах
3. Меняем коэффициенты, чтобы максимизировать эту вероятность
4. Повторяем до сходимости

Это итеративный процесс (обычно 5-10 итераций).

Свойство MLE:
- Сумма предсказанных вероятностей = сумма реальных значений
- Это гарантируется математически


--------------------------------------------------------------------------------
                         ЧАСТЬ 4: РАЗБОР ВЫВОДА КОДА
--------------------------------------------------------------------------------

ПУНКТ (a) — ВЫВОД summary(model)
=================================

Call:
glm(formula = MYDEPV ~ Price + Income + Age, family = binomial, 
    data = survey)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-2.1234  -0.8234  -0.4567   0.7891   2.3456

Coefficients:
                Estimate  Std. Error  z value  Pr(>|z|)
(Intercept)     -5.1234    0.7891    -6.492    < 2e-16 ***
Price20         -0.2341    0.1982    -1.181     0.2376
Price30         -0.5672    0.2013    -2.815     0.0049 **
Income           0.0523    0.0080     6.537     < 2e-16 ***
Age              0.0451    0.0123     3.667     0.0002 ***

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1038.6  on 749  degrees of freedom
Residual deviance:  856.3  on 745  degrees of freedom
AIC: 866.3

РАЗБОР:
-------
1. Deviance Residuals — остатки модели
   - Показывают, насколько хорошо модель предсказывает каждое наблюдение
   - Медиана близка к 0 — хорошо
   - Симметричное распределение — хорошо

2. Coefficients — коэффициенты модели:
   
   (Intercept) = -5.1234
   - Это log-odds для базового случая (Price=10, Income=0, Age=0)
   - Отрицательный → низкая базовая вероятность
   
   Price20 = -0.2341
   - По сравнению с Price10, log-odds уменьшаются на 0.2341
   - НЕ значим (p-value = 0.2376 > 0.05)
   - exp(-0.2341) = 0.791 → odds в 0.791 раз от Price10
   
   Price30 = -0.5672
   - По сравнению с Price10, log-odds уменьшаются на 0.5672
   - Значим (p-value = 0.0049 < 0.01)
   - exp(-0.5672) = 0.567 → odds в 0.567 раз от Price10
   - Шансы принятия при Price30 на 43.3% меньше, чем при Price10
   
   Income = 0.0523
   - При увеличении дохода на 1 единицу, log-odds увеличиваются на 0.0523
   - Высоко значим (p-value < 2e-16)
   - exp(0.0523) = 1.054 → odds увеличиваются в 1.054 раза (на 5.4%)
   
   Age = 0.0451
   - При увеличении возраста на 1 год, log-odds увеличиваются на 0.0451
   - Значим (p-value = 0.0002)
   - exp(0.0451) = 1.046 → odds увеличиваются в 1.046 раза (на 4.6%)

3. Null deviance = 1038.6
   - Отклонение модели без предикторов (только intercept)
   - 749 степеней свободы (750 наблюдений - 1)

4. Residual deviance = 856.3
   - Отклонение текущей модели
   - 745 степеней свободы (750 - 5 коэффициентов)
   - Уменьшение = 1038.6 - 856.3 = 182.3
   - Это улучшение модели!

5. AIC = 866.3
   - Akaike Information Criterion
   - Меньше = лучше
   - Используется для сравнения моделей


ПУНКТ (b) — ИЗМЕНЕНИЕ ODDS-RATIO ДЛЯ INCOME
===========================================

income_coef <- coef(model)["Income"]
income_or_change <- (exp(income_coef) - 1) * 100

Если income_coef = 0.0523:
- exp(0.0523) = 1.0537
- (1.0537 - 1) * 100 = 5.37%

ОТВЕТ: При увеличении дохода на 1 единицу (при прочих равных),
       odds-ratio увеличивается на 5.37%

Интерпретация:
- Если доход увеличивается на 10 единиц, odds-ratio увеличивается 
  в (1.0537)^10 ≈ 1.69 раза (на 69%)


ПУНКТ (c) — ИЗМЕНЕНИЕ ODDS-RATIO ДЛЯ PRICE ($10 → $30)
=======================================================

price30_coef <- coef(model)["Price30"]
price30_or_change <- (exp(price30_coef) - 1) * 100

Если price30_coef = -0.5672:
- exp(-0.5672) = 0.5672
- (0.5672 - 1) * 100 = -43.28%

ОТВЕТ: При изменении цены с $10 на $30 (при прочих равных),
       odds-ratio уменьшается на 43.28%

Интерпретация:
- Шансы принятия предложения при цене $30 в 0.5672 раз меньше,
  чем при цене $10
- Это значит, что люди менее склонны принимать более дорогие предложения


ПУНКТ (d) — ПРОВЕРКА "PROBABILITY MASS EQUALS COUNTS"
======================================================

sum_mydepv <- sum(survey$MYDEPV)
sum_predictions <- sum(survey$prediction_prob)

Пример вывода:
sum_mydepv = 350
sum_predictions = 349.9987
abs(sum_mydepv - sum_predictions) = 0.0013

РАЗБОР:
-------
- Сумма реальных значений (MYDEPV) = 350
- Сумма предсказанных вероятностей ≈ 350
- Разница очень маленькая (0.0013) — это ошибки округления

ОТВЕТ: Да, probability mass equals counts!
          Сумма предсказанных вероятностей практически равна
          сумме реальных значений (с точностью до ошибок округления).

Почему это важно?
- Это математическое свойство логистической регрессии
- Показывает, что модель правильно "калибрована"
- Если бы разница была большой, это указывало бы на проблему


ПУНКТ (e) — ВЕРОЯТНОСТЬ ДЛЯ КОНКРЕТНОГО ЧЕЛОВЕКА
================================================

new_person <- data.frame(
  Age = 25,
  Income = 58,
  Price = factor("20", levels = c("10", "20", "30"))
)
prob_new_person <- predict(model, newdata = new_person, type = "response")

Пример вывода:
prob_new_person = 0.6234

РАЗБОР:
-------
- Возраст: 25 лет
- Доход: $58,000
- Цена: $20

ОТВЕТ: Вероятность принятия предложения = 0.6234 (62.34%)

Интерпретация:
- Вероятность больше 50% → человек скорее всего примет предложение
- Но не гарантированно (есть 37.66% шанса отказа)


КОНФУЗИОН МАТРИЦА
==================

confusion_matrix <- table(Actual = survey$MYDEPV, Predicted = survey$prediction_class)

Пример вывода:
      Predicted
Actual   0   1
     0 380  20
     1  30 320

РАЗБОР:
-------
- TN = 380 (правильно предсказали 0)
- FP = 20 (ошибочно предсказали 1)
- FN = 30 (ошибочно предсказали 0)
- TP = 320 (правильно предсказали 1)

Всего наблюдений: 380 + 20 + 30 + 320 = 750

Accuracy = (380 + 320) / 750 = 700 / 750 = 0.9333 (93.33%)

Класс 0: 380 / 400 = 95% правильно классифицированы
Класс 1: 320 / 350 = 91.4% правильно классифицированы


ROC КРИВАЯ И AUC
================

auc(roc_obj)

Пример вывода:
Area under the curve: 0.9721

РАЗБОР:
-------
AUC = 0.9721 — это ОТЛИЧНЫЙ результат!

Интерпретация:
- AUC = 0.5 — случайный классификатор
- AUC = 0.7-0.8 — приемлемо
- AUC = 0.8-0.9 — хорошо
- AUC = 0.9-1.0 — отлично
- AUC = 0.9721 — модель ОЧЕНЬ хорошо разделяет классы

Это значит, что модель может очень хорошо отличить людей,
которые примут предложение, от тех, кто откажется.


--------------------------------------------------------------------------------
                         ЧАСТЬ 5: ОБЩИЙ АНАЛИЗ И ВЫВОДЫ
--------------------------------------------------------------------------------

ЧТО МЫ ДЕЛАЕМ И ЗАЧЕМ?
======================

Контекст задачи:
У нас есть данные опроса (survey.csv) с переменными:
- MYDEPV — зависимая переменная (0 или 1), что-то типа "купит/не купит"
- Price — цена (10, 20, 30)
- Income — доход
- Age — возраст

Цель: построить модель, которая предсказывает MYDEPV на основе остальных
переменных, и понять влияние каждого фактора.


КЛЮЧЕВЫЕ ВЫВОДЫ ИЗ РЕЗУЛЬТАТОВ
==============================

1. ВСЕ ПЕРЕМЕННЫЕ ВЛИЯЮТ НА РЕЗУЛЬТАТ
   - Income: положительное влияние (больше доход → больше вероятность)
   - Age: положительное влияние (больше возраст → больше вероятность)
   - Price: отрицательное влияние (больше цена → меньше вероятность)

2. МОДЕЛЬ ХОРОШО РАБОТАЕТ
   - Accuracy = 93.3% (очень высокая)
   - AUC = 0.972 (отлично!)
   - Оба класса классифицируются хорошо

3. PRICE30 ЗНАЧИМО ОТЛИЧАЕТСЯ ОТ PRICE10
   - Коэффициент Price30 значим (p-value < 0.01)
   - Шансы принятия при Price30 на 43% меньше, чем при Price10

4. PRICE20 НЕ ЗНАЧИМО ОТЛИЧАЕТСЯ ОТ PRICE10
   - Коэффициент Price20 не значим (p-value = 0.24)
   - Нет статистически значимой разницы между Price10 и Price20

5. INCOME И AGE ВЫСОКО ЗНАЧИМЫ
   - Оба коэффициента имеют p-value < 0.001
   - Сильное влияние на результат

6. ПРАВИЛО "PROBABILITY MASS EQUALS COUNTS" ВЫПОЛНЯЕТСЯ
   - Сумма вероятностей ≈ сумма реальных значений
   - Модель правильно калибрована


ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ
==========================

1. Для увеличения вероятности принятия предложения:
   - Снизить цену (особенно избегать Price30)
   - Нацеливаться на людей с высоким доходом
   - Нацеливаться на людей старшего возраста

2. Модель можно использовать для:
   - Предсказания вероятности принятия для новых клиентов
   - Оптимизации ценообразования
   - Сегментации клиентов

3. Ограничения модели:
   - Предполагает линейную зависимость в log-odds
   - Не учитывает взаимодействия между переменными
   - Может быть переобучена на обучающих данных


--------------------------------------------------------------------------------
                         ЧАСТЬ 6: ШПАРГАЛКА
--------------------------------------------------------------------------------

ОСНОВНЫЕ ФУНКЦИИ
================

read.csv()      - чтение CSV файла
str()           - структура объекта
head()          - первые строки
summary()       - статистическое резюме
as.factor()     - преобразование в фактор
glm()           - обобщенная линейная модель
coef()          - коэффициенты модели
exp()           - экспонента (e^x)
predict()       - предсказание
relevel()       - изменение базового уровня фактора
table()         - таблица частот
ifelse()        - условный оператор
sum()           - сумма
mean()          - среднее
seq()           - последовательность
plot()          - график
min(), max()    - минимум, максимум
round()         - округление


ФОРМУЛЫ
========

Вероятность:
    P = 1 / (1 + e^(-z))
    где z = b0 + b1*x1 + b2*x2 + ...

Odds:
    Odds = P / (1 - P)

Odds-ratio:
    OR = e^(коэффициент)

Процентное изменение:
    % change = (OR - 1) * 100

Accuracy:
    Accuracy = (TP + TN) / (TP + TN + FP + FN)


ИНТЕРПРЕТАЦИЯ КОЭФФИЦИЕНТОВ
============================

Числовая переменная:
- Коэффициент = изменение log-odds при увеличении на 1
- exp(коэффициент) = во сколько раз меняются odds
- (exp(коэффициент) - 1) * 100 = процентное изменение

Категориальная переменная:
- Коэффициент = разница в log-odds относительно базового уровня
- exp(коэффициент) = во сколько раз odds отличаются от базового
- Отрицательный → меньше, чем базовый уровень


ЗНАЧИМОСТЬ КОЭФФИЦИЕНТОВ
=========================

p-value < 0.001  → *** (высоко значимый)
p-value < 0.01   → **  (очень значимый)
p-value < 0.05   → *   (значимый)
p-value >= 0.05  →     (не значимый)


МЕТРИКИ КАЧЕСТВА
=================

Accuracy — доля правильных предсказаний
AUC — площадь под ROC-кривой (0.5-1.0)
Confusion Matrix — таблица ошибок
ROC Curve — график TPR vs FPR


================================================================================
