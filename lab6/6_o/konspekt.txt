ЛИНЕЙНАЯ РЕГРЕССИЯ - КОНСПЕКТ

Задача: по входным данным X предсказать числовое значение y
Пример: по площади и комнатам предсказать цену дома

================================================================================
МОДЕЛЬ
================================================================================

h(x) = theta_0 + theta_1*x_1 + theta_2*x_2 + ...

В матричном виде для всех примеров:

    h = X * theta

X - матрица (m x n+1):  m примеров, n признаков + столбец единиц
theta - вектор (n+1 x 1): параметры модели
h - вектор (m x 1): предсказания

Столбец единиц нужен чтобы theta_0 участвовал в умножении:
[1  x1  x2]   [theta_0]     theta_0*1 + theta_1*x1 + theta_2*x2
[1  x1  x2] * [theta_1]  =  ...
[...]         [theta_2]

================================================================================
ФУНКЦИЯ СТОИМОСТИ (computeCost.m)
================================================================================

Показывает насколько плохо модель предсказывает. Чем меньше J - тем лучше.

Формула:  J = (1/2m) * SUM((h - y)^2)

КОД:
    m = length(y);
    h = X * theta;
    J = (1 / (2 * m)) * sum((h - y) .^ 2);

Что считаем:
1. h = X * theta
   Умножаем матрицу признаков на theta -> получаем вектор предсказаний

2. (h - y)
   Вычитаем реальные значения -> получаем вектор ошибок

3. .^ 2
   Возводим каждую ошибку в квадрат (точка = поэлементно)

4. sum(...)
   Суммируем все квадраты ошибок

5. / (2*m)
   Делим на 2m для нормировки

================================================================================
ГРАДИЕНТНЫЙ СПУСК (gradientDescent.m)
================================================================================

Итеративно меняем theta чтобы уменьшить J.
На каждом шаге двигаемся в направлении наибольшего убывания J.

Формула:  theta := theta - (alpha/m) * X' * (h - y)

КОД:
    for iter = 1:num_iters
        h = X * theta;
        theta = theta - (alpha / m) * (X' * (h - y));
        J_history(iter) = computeCost(X, y, theta);
    end

Что считаем:
1. h = X * theta
   Текущие предсказания

2. (h - y)
   Вектор ошибок (m x 1)

3. X' * (h - y)
   X' - транспонированная X, размер (n+1 x m)
   Результат: вектор (n+1 x 1) - градиент для каждого theta
   
   Что делает X' * (h - y)?
   Для каждого theta_j суммирует: ошибка_i * признак_j_i по всем примерам
   Это и есть производная J по theta_j (с точностью до 1/m)

4. (alpha / m) * ...
   Масштабируем градиент

5. theta - ...
   Вычитаем из theta -> двигаемся к минимуму

alpha - размер шага:
- большой: быстро, но может не сойтись
- маленький: медленно, но надежно

================================================================================
НОРМАЛИЗАЦИЯ (featureNormalize.m)
================================================================================

Приводим признаки к одному масштабу чтобы GD работал быстрее.
Без нормализации: площадь ~2000, комнаты ~3 -> GD плохо сходится.

Формула:  x_norm = (x - mean) / std

КОД:
    mu = mean(X);
    sigma = std(X);
    X_norm = (X - mu) ./ sigma;

Что считаем:
1. mean(X) - среднее по каждому столбцу -> вектор (1 x n)
2. std(X) - стандартное отклонение по столбцам -> вектор (1 x n)
3. (X - mu) ./ sigma - нормализуем каждый столбец

После: среднее = 0, std = 1, все признаки в диапазоне ~[-3, 3]

ВАЖНО: при предсказании использовать те же mu и sigma!
    x_test = ([1650, 3] - mu) ./ sigma;
    price = [1, x_test] * theta;

================================================================================
НОРМАЛЬНОЕ УРАВНЕНИЕ (normalEqn.m)
================================================================================

Аналитическое решение - находим минимум J за один шаг без итераций.

Формула:  theta = (X'X)^(-1) * X' * y

КОД:
    theta = pinv(X' * X) * X' * y;

Что считаем:
1. X' * X - матрица (n+1 x n+1)
2. pinv(...) - псевдообратная матрица (надежнее чем inv)
3. * X' * y - умножаем на X'y

Плюсы: точно, не нужен alpha, не нужна нормализация
Минусы: медленно при n > 10000 (обращение матрицы O(n^3))

================================================================================
ПОРЯДОК РАБОТЫ ex1.m (простая регрессия)
================================================================================

1. Загрузка данных: X = население, y = прибыль
2. plotData - рисуем точки
3. X = [ones(m,1), X] - добавляем столбец единиц
4. theta = zeros(2,1) - начальные параметры [0; 0]
5. computeCost - считаем начальную ошибку (~32)
6. gradientDescent - 1500 итераций, alpha=0.01
7. Получаем theta ≈ [-3.63; 1.17]
   Интерпретация: прибыль = -3.63 + 1.17 * население
8. Рисуем линию регрессии
9. Визуализация J: surf (3D), contour (контуры)

================================================================================
ПОРЯДОК РАБОТЫ ex1_multi.m (множественная регрессия)
================================================================================

1. Загрузка: X = [площадь, комнаты], y = цена
2. featureNormalize(X) -> X_norm, mu, sigma
3. X = [ones(m,1), X_norm] - столбец единиц ПОСЛЕ нормализации
4. gradientDescentMulti - 100 итераций, alpha=0.01
5. Предсказание с нормализацией:
   x_test = ([1650, 3] - mu) ./ sigma;
   price = [1, x_test] * theta;

6. Для сравнения - нормальное уравнение:
   - загружаем данные заново (без нормализации)
   - X = [ones(m,1), X]
   - theta = pinv(X'*X) * X' * y
   - price = [1, 1650, 3] * theta  (нормализация НЕ нужна)

================================================================================
КЛЮЧЕВЫЕ МОМЕНТЫ
================================================================================

.* vs * :
  * - матричное умножение
  .* - поэлементное (точка = element-wise)
  Аналогично: .^ vs ^, ./ vs /

Зачем столбец единиц?
  Чтобы theta_0 (свободный член) участвовал в X*theta

Зачем нормализация?
  Выравнивает масштабы -> GD сходится быстрее

GD vs Normal Equation:
  GD: много признаков, большие данные
  NE: мало признаков, нужно быстрое точное решение

================================================================================
ЗАПУСК:
    cd mlclass-ex1
    octave ex1.m
    octave ex1_multi.m
================================================================================
