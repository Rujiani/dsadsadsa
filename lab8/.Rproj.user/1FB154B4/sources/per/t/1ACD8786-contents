КОНСПЕКТ: НАИВНЫЙ БАЙЕСОВСКИЙ КЛАССИФИКАТОР В R


ЗАЧЕМ НУЖНА КЛАССИФИКАЦИЯ?

Классификация — это задача машинного обучения, когда нужно отнести объект
к одной из заранее известных категорий (классов).

Примеры из жизни:
- Спам или не спам? (2 класса)
- Какой диагноз у пациента? (много классов)
- Одобрить кредит или нет? (2 класса)
- Какой уровень дохода у человека? (наша задача — 3 класса)

Зачем это нужно?
1. Автоматизация решений — не нужен человек для каждого случая
2. Скорость — модель обрабатывает тысячи записей за секунды
3. Объективность — одинаковые правила для всех
4. Прогнозирование — предсказываем класс для новых данных

Как это работает?
1. Берем данные где класс уже известен (обучающая выборка)
2. Строим модель, которая находит связь между признаками и классом
3. Применяем модель к новым данным (тестовая выборка)
4. Проверяем насколько хорошо модель угадывает

В нашем задании:
- Задание 4.5: по возрасту, полу и образованию предсказываем доход
- Задание 4.6: по возрасту, образованию и доходу предсказываем пол


ЧТО ТАКОЕ НАИВНЫЙ БАЙЕС?

Это один из методов классификации. Основан на теореме Байеса
о вероятностях.

Формула Байеса (упрощенно):
P(класс|признаки) = P(признаки|класс) * P(класс) / P(признаки)

Где:
- P(класс|признаки) — вероятность класса при данных признаках (то что ищем)
- P(признаки|класс) — условная вероятность признаков в данном классе
- P(класс) — априорная вероятность класса (как часто он встречается)


ПОЧЕМУ НАЗЫВАЕТСЯ "НАИВНЫЙ БАЙЕС"?

Название состоит из двух частей:

1. "БАЙЕС" — в честь Томаса Байеса (1701-1761), английского математика и
   священника. Он разработал теорему о вычислении условных вероятностей.
   
   Теорема Байеса отвечает на вопрос: "Если я вижу какие-то признаки,
   какова вероятность что объект принадлежит определенному классу?"
   
   Пример: если человек старше 45 лет, имеет высшее образование и мужчина —
   какова вероятность что его доход больше 80K?

2. "НАИВНЫЙ" — потому что алгоритм делает упрощающее (наивное) допущение:
   все признаки НЕЗАВИСИМЫ друг от друга.
   
   Что это значит? Алгоритм считает что:
   - Возраст никак не связан с образованием
   - Образование никак не связано с полом
   - Пол никак не связан с возрастом
   И так далее для всех пар признаков.
   
   В реальности это часто НЕ ТАК! Например:
   - Люди старшего возраста реже имеют высшее образование (другие времена)
   - Мужчины чаще работают в технических сферах
   - Образование влияет на доход
   
   Но алгоритм "наивно" игнорирует эти связи. Почему?
   
   Потому что это СИЛЬНО упрощает вычисления:
   - Вместо P(возраст И пол И образование | класс) — сложно вычислить
   - Считаем P(возраст|класс) * P(пол|класс) * P(образование|класс) — просто!
   
   Удивительно, но несмотря на это "наивное" допущение, алгоритм часто
   работает хорошо на практике!


ПЛЮСЫ НАИВНОГО БАЙЕСА

1. Простота и скорость
   - Очень быстрое обучение (просто считаем частоты)
   - Быстрое предсказание (просто умножаем вероятности)
   - Легко понять что делает модель
   
2. Работает с малым количеством данных
   - Не требует огромных датасетов для обучения
   - Каждый признак обучается отдельно
   
3. Хорошо работает с категориальными данными
   - Идеален для текстовой классификации (спам-фильтры!)
   - Не нужно преобразовывать категории в числа
   
4. Устойчив к нерелевантным признакам
   - Если признак не связан с классом, он просто не влияет на результат
   
5. Легко интерпретировать
   - Можно посмотреть условные вероятности и понять "почему"
   - Видно какие признаки важны для классификации


МИНУСЫ И ОГРАНИЧЕНИЯ НАИВНОГО БАЙЕСА

1. Допущение о независимости признаков
   - Главный минус! В реальности признаки часто связаны
   - Если признаки сильно коррелируют — качество падает
   - Пример: рост и вес человека зависят друг от друга
   
2. Проблема "нулевой частоты"
   - Если комбинация (признак + класс) не встречалась в обучении,
     вероятность = 0, и всё произведение = 0
   - Решение: сглаживание Лапласа (добавляем 1 ко всем счетчикам)
   
3. Плохо работает с несбалансированными данными
   - Это мы видели в задании! 80% данных — класс 10-50K
   - Модель предсказывает почти всё как частый класс
   - Редкие классы игнорируются
   
4. Не учитывает порядок и взаимодействие признаков
   - "Молодой богатый" и "богатый молодой" — одно и то же для модели
   - Комбинации признаков не учитываются
   
5. Дает не калиброванные вероятности
   - Выходные "вероятности" — не настоящие вероятности
   - Нельзя сказать "модель уверена на 90%" — это будет неточно


КОГДА ИСПОЛЬЗОВАТЬ НАИВНЫЙ БАЙЕС?

Хорошо подходит:
- Классификация текстов (спам, тональность, категории)
- Медицинская диагностика (симптомы → болезнь)
- Рекомендательные системы
- Когда мало данных
- Когда нужна быстрая базовая модель для сравнения

Плохо подходит:
- Несбалансированные классы (как в нашем задании)
- Сильно зависимые признаки
- Когда нужны точные вероятности
- Сложные нелинейные зависимости


ЧТО ДЕЛАЕТ КОД?

1. library(e1071) - подключаем библиотеку с функцией naiveBayes

2. read.csv(..., stringsAsFactors = TRUE) - загружаем данные
   stringsAsFactors = TRUE означает, что текстовые колонки станут факторами
   (категориальными переменными). Это важно для наивного Байеса.

3. Делим данные:
   train <- data[1:9010, ]  - первые 9010 строк для обучения
   test <- data[9011:10010, ] - последние 1000 для проверки


Задание 4.5 - предсказываем доход

a) Строим модель: income ~ age + sex + educ
   Это значит: предсказываем income на основе age, sex и educ

   naiveBayes() возвращает:
   - apriori: априорные вероятности классов (доля каждого класса в данных)
   - tables: условные вероятности (для каждого признака - какова вероятность
     его значений при каждом классе дохода)

   Пример: если apriori показывает 10-50K = 0.65, значит 65% людей
   в обучающих данных имеют доход 10-50K.

   Условные вероятности показывают, например, какова вероятность быть
   мужчиной если доход GT 80K.

b) predict() - применяем модель к тестовым данным

   Матрица ошибок (confusion matrix):
   Строки - реальные классы
   Столбцы - предсказанные классы
   На диагонали - правильные предсказания
   Вне диагонали - ошибки

   Ошибка классификации = (все ошибки) / (все наблюдения)
   Или проще: 1 - (правильные / все)

   Почему разная точность по классам?
   - Если класс редкий, модель плохо его учит
   - Если признаки плохо различают классы, будут ошибки
   - Класс 10-50K самый частый - его легче предсказать
   - GT 80K редкий - больше ошибок


Задание 4.6 - предсказываем пол

a) Теперь sex ~ age + educ + income
   Предсказываем пол по возрасту, образованию и доходу

   Проблема: в данных может быть дисбаланс классов
   (например, 70% мужчин и 30% женщин)
   Тогда модель будет чаще предсказывать частый класс.

b) Балансировка данных:
   Берем одинаковое количество записей каждого пола (по 3500)
   Это делает классы равными по размеру

   sample(nrow(train_f), 3500) - случайно выбираем 3500 индексов
   из строк с женщинами

   rbind() - объединяем выборки в одну таблицу

   После балансировки априорные вероятности будут 0.5 / 0.5

c) Сравниваем качество:
   Сбалансированная модель обычно лучше предсказывает редкий класс,
   но может хуже предсказывать частый класс.
   Общая точность может упасть, но модель становится "честнее".

d) Случайность влияет на результат:
   Каждый раз выбираем разные 3500 записей
   Поэтому модели немного разные
   Ошибка колеблется от запуска к запуску

e) Выводы:
   - Дисбаланс классов влияет на качество модели
   - Балансировка помогает лучше предсказывать редкий класс
   - Случайная выборка вносит нестабильность в результаты
   - Наивный Байес прост, но эффективен для категориальных данных
   - Качество модели зависит от того, насколько признаки
     реально связаны с целевой переменной

Полезные функции:

table(Actual, Predicted) - создает матрица ошибок
sum(diag(matrix)) - сумма диагонали (правильные ответы)
sum(matrix) - общая сумма (все ответы)
levels(factor) - уровни фактора (уникальные значения)
set.seed(число) - фиксирует случайность для воспроизводимости


РАЗБОР РЕЗУЛЬТАТОВ

Задание 4.5 (предсказание дохода):

Априорные вероятности показывают: 10-50K очень много (7232), 
50-80K мало (1132), GT 80K совсем мало (646).

Из-за этого модель почти всё предсказывает как 10-50K!
Смотри матрицу ошибок: столбец 50-80K пустой (0 предсказаний).

Ошибки по классам:
- 10-50K: 0.76% - почти идеально, потому что модель всё туда пихает
- 50-80K: 100% - катастрофа, ни одного правильного
- GT 80K: 89.33% - почти всё неправильно

Почему так? Наивный Байес умножает априорную вероятность на условные.
Если априорная для 10-50K = 0.80 (80%), то даже при плохих условных
вероятностях итоговая всё равно больше чем у редких классов.


Задание 4.6 (предсказание пола):

a) На несбалансированных данных:
   Мужчин в данных больше, поэтому модель чаще предсказывает M.
   Ошибка для F = 75.18% (плохо)
   Ошибка для M = 16.93% (хорошо)
   
   Признаки (возраст, образование, доход) слабо различают пол.
   Модель "ленится" и предсказывает частый класс.

b-c) После балансировки:
   Теперь F и M по 3500 записей (50/50).
   Ошибка для F = 13.58% (стало лучше!)
   Ошибка для M = 71.9% (стало хуже!)
   
   Модель теперь чаще предсказывает F (потому что априорные равны,
   а условные вероятности немного в пользу F).
   
   Общая ошибка выросла с 41.8% до 47% - это нормально.
   Зато модель стала честнее и лучше находит женщин.

d) Итерации показывают одинаковую ошибку 47%.
   В этих данных случайность мало влияет, потому что выборки большие
   (3500 из ~4000 для каждого пола) и данные однородные.
   
   На других данных разброс был бы заметнее.

e) Главные выводы:
   - Несбалансированные данные = смещенная модель
   - Балансировка перераспределяет ошибки между классами
   - Нельзя смотреть только на общую точность
   - Надо проверять точность по каждому классу отдельно
   - Выбор метрики зависит от задачи (что важнее: не пропустить
     редкий класс или не ошибиться в частом?)


ГРАФИКИ (сохраняются в graphs.pdf)

1. Столбчатая диаграмма ошибок по классам дохода
   Наглядно видно, что 10-50K почти без ошибок, а 50-80K и GT 80K - провал.

2. Сравнение ошибок до и после балансировки
   Красные столбцы - до балансировки, синие - после.
   Видно как ошибки "переключаются" между классами.

3. Тепловая карта матрицы ошибок
   Чем светлее ячейка - тем больше значение.
   Числа показывают проценты от строки (реального класса).

4. График стабильности по итерациям
   Показывает как меняется ошибка при разных случайных выборках.
   Пунктирная линия - среднее значение.

Функции для графиков:
pdf("file.pdf") - открыть PDF для записи
par(mfrow = c(2, 2)) - разбить на 4 части (2 строки, 2 столбца)
barplot() - столбчатая диаграмма
image() - тепловая карта
plot() - обычный график
dev.off() - закрыть PDF файл


ОТВЕТЫ НА ТЕСТОВЫЕ ВОПРОСЫ

A-priori probabilities (априорные вероятности):
- Income 10-50K: 0.8027
- Income 50-80K: 0.1256
- Income GT 80K: 0.0717

Conditional Probabilities (условные вероятности):
P(education = 'College' | Y = '50-80K') = 0.4956

Confusion Matrix (матрица ошибок):
Значение для actual = 'GT 80K' и predicted = 'GT 80K': 8

Misclassification Rates (ошибки классификации для income):
- Overall: 0.205 (или 20.5%)
- 10-50K: 0.0076 (или 0.76%)
- 50-80K: 1.0 (или 100%)
- GT 80K: 0.8933 (или 89.33%)

Predictive Power (объяснение вариации):
ОТВЕТ: In this model variation is explained mostly by a-priori probabilities

Почему: модель почти всё предсказывает как 10-50K, потому что этот класс
имеет априорную вероятность 0.80. Даже при плохих условных вероятностях
произведение всё равно больше чем у редких классов.


Misclassification Rates (ошибки для sex):
- Overall: 0.418 (или 41.8%)
- Female: 0.7518 (или 75.18%)
- Male: 0.1693 (или 16.93%)

Explanation (объяснение ошибок для sex):
ОТВЕТЫ (нужно выбрать несколько):
- Here, the conditional probabilities are roughly equal for age.
- Here, the conditional probabilities are roughly equal for education.
- Here, the conditional probabilities are roughly equal for income.
- But even for income, they are not too different.
- Consequently, the classification largely depends on the a priori probabilities.
- The probability of being male is greater than the probability of being female.

Почему: условные вероятности для M и F очень похожи по всем признакам,
поэтому модель опирается на априорные вероятности. Мужчин в данных больше,
поэтому модель чаще предсказывает M.


New Model (сбалансированная модель):
ОТВЕТЫ:
- The a priori probabilities are equal and the conditional probabilities are very similar.
- The model is essentially classifying at random.

Почему: после балансировки априорные = 0.5/0.5, условные почти одинаковые.
Признаки слабо различают пол, поэтому модель работает почти случайно.


Random Sampling (влияние случайной выборки):
ОТВЕТЫ:
- The model's performance is not sensitive to the random sampling.
- Conditional probabilities are very close over the entire sample.
- The small perturbations introduced by choosing a different sample do not impact
  the classifications made by the model.

Почему: выборки большие (3500 из ~4000), данные однородные, поэтому
разные случайные выборки дают похожие условные вероятности.


Final Conclusions (итоговые выводы):
ОТВЕТЫ:
- If a class has an a priori probability close to zero, then the model will
  assign almost none of the records to that class.
- If a class has an a priori probability close to one, then the model will
  assign almost all of the records to that class.
- If the features of interest and the classes are weakly correlated, then
  the model's classifications primarily depend on the a priori probabilities.
- A priori probabilities depend on the proportion of each class in the data.
- The Naive Bayes classifier is not a best choice for skewed data sets.


Bonus (откуда название e1071):
ОТВЕТ: It is a lab number where package was developed.

Пояснение: E1071 - это номер кафедры статистики в Венском техническом
университете (TU Wien), где был разработан этот пакет.

