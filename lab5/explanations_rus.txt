================================================================================
              АССОЦИАТИВНЫЕ ПРАВИЛА (ASSOCIATION RULES) - ПОЛНЫЙ РАЗБОР
================================================================================

▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
              ЧАСТЬ 0: ОТВЕТЫ НА ТЕСТ (КОПИРУЙ СЮДА)
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

═══════════════════════════════════════════════════════════════════════════════
БАЗОВЫЙ АНАЛИЗ ДАННЫХ
═══════════════════════════════════════════════════════════════════════════════

Номер самого частого товара: 13
(item13 встречается в ~49.5% транзакций, около 4948 раз)

Размер самой большой транзакции: 26
(посмотри max_size в выводе кода, обычно 25-27)


═══════════════════════════════════════════════════════════════════════════════
ПОЛУЧЕНИЕ ПРАВИЛ
═══════════════════════════════════════════════════════════════════════════════

Количество правил при confidence=0%, support=1%: 11435

Количество правил при confidence=50%, support=1%: 1165

Как confidence влияет на количество правил:
→ ОТВЕТ ДЛЯ ТЕСТА: "The specified confidence, say 25%, reduces the number of rules 
   by only considering the transactions that have at least a pair of items 
   at least 25% of the time."

→ ПО-РУССКИ: Указанный confidence (например, 25%) уменьшает количество правил,
   оставляя только те, где пара товаров встречается вместе минимум в 25% случаев
   когда покупают первый товар.


═══════════════════════════════════════════════════════════════════════════════
ВИЗУАЛИЗАЦИЯ ПРАВИЛ: МАТРИЦА (Красный, Жёлтый, Зелёный)
═══════════════════════════════════════════════════════════════════════════════

Правило с тёмно-зелёным цветом (LHS -> RHS):
LHS (левая часть): item55
RHS (правая часть): item34

(lift = 7.69, confidence = 85.5% - это тёмно-зелёный на 2D палитре)
Тёмно-зелёный = баланс между высоким lift и высоким confidence


═══════════════════════════════════════════════════════════════════════════════
АНАЛИЗ МАТРИЦЫ (выбери ВСЕ правильные ответы)
═══════════════════════════════════════════════════════════════════════════════

ПРАВИЛЬНЫЕ ОТВЕТЫ ДЛЯ ТЕСТА:
✓ Rules colored by dark (deep) color are more interesting, as they have greater lift
  (Правила с тёмным цветом более интересны, так как имеют больший lift)

✓ Rules in dark (deep) green are in the balance of lift and confidence, 
  so they is more interesting rules for us
  (Тёмно-зелёные правила - баланс lift и confidence, самые интересные)

✓ Rules in a dark (deep) blue color suggest that we are likely to see these 
  itemsets paired together by coincidence making them interesting but not important rules
  (Тёмно-синие правила - скорее всего случайные совпадения, не важные)


═══════════════════════════════════════════════════════════════════════════════
ГРАФИЧЕСКИЙ АНАЛИЗ ПРАВИЛ (точечные диаграммы)
═══════════════════════════════════════════════════════════════════════════════

ПРАВИЛЬНЫЕ ОТВЕТЫ ДЛЯ ТЕСТА:

✓ The most important rules would be located on the top left of the second graph.
  (Самые важные правила находятся в ЛЕВОМ ВЕРХНЕМ углу второго графика)
  → Высокий lift + низкий support = редкие, но сильные связи

✓ Rules that would be considered coincidental would be on the bottom of the second graph.
  (Случайные правила находятся ВНИЗУ второго графика)
  → lift ≈ 1 = товары встречаются вместе случайно

✓ Lift and support have a strong inverse relationship.
  (Lift и support имеют СИЛЬНУЮ ОБРАТНУЮ зависимость)
  → Чем выше support, тем ниже lift

✓ According the first graph the interesting points reside mainly at some 
  substantial distance from support and confidence border.
  (На первом графике интересные точки находятся далеко от границ)
  → Правила с высоким lift не на краях графика


═══════════════════════════════════════════════════════════════════════════════
ТРИ ПРАВИЛА (support >= 10%, случайные по совпадению)
═══════════════════════════════════════════════════════════════════════════════

Правила с support >= 10% и lift ≈ 1 (случайные):

Правило 1:
LHS #1: item5
RHS #1: item13

Правило 2:
LHS #2: item30
RHS #2: item13

Правило 3:
LHS #3: item13
RHS #3: item5

ПОЧЕМУ ОНИ СЛУЧАЙНЫЕ:
→ Все имеют lift ~1.0-1.1
→ Lift ≈ 1 означает что товары встречаются вместе ПО СОВПАДЕНИЮ
→ Просто оба товара популярны, реальной связи между ними нет!


═══════════════════════════════════════════════════════════════════════════════
СЛУЧАЙНЫЕ ПРАВИЛА (10 с наименьшим lift из confidence > 0.8)
═══════════════════════════════════════════════════════════════════════════════

10 правил с наименьшим lift (из confidence > 0.8):
1.  {item30,item95,item96} => {item13}  lift=1.62
2.  {item23,item5} => {item13}          lift=1.70
3.  {item83} => {item13}                lift=1.71
4.  {item10,item44} => {item13}         lift=1.72  ← ПРАВИЛО #4
5.  {item82,item99} => {item13}         lift=1.73
6.  {item23} => {item13}                lift=1.74
7.  {item3,item84,item95} => {item13}   lift=1.77
8.  {item5,item82,item99} => {item13}   lift=1.81
9.  {item20,item23} => {item13}         lift=1.84
10. {item16,item25,item77} => {item5}   lift=2.18

Случайное правило #4:
LHS: item10,item44
RHS: item13

Являются ли они случайными (baseline lift < 2)?
→ ДА, правила 1-9 имеют lift < 2, поэтому они выглядят случайными
→ item13 очень популярен (50% транзакций), поэтому многие правила 
   ведут к нему с высоким confidence, но низким lift
→ Высокий confidence НЕ означает интересное правило!


═══════════════════════════════════════════════════════════════════════════════
ВИЗУАЛИЗАЦИЯ ПРАВИЛ: ГРАФ (3 правила с наивысшим lift)
═══════════════════════════════════════════════════════════════════════════════

Топ-3 правила по lift (из ВСЕХ правил):
1. {item15,item30,item56} => {item49}  lift=19.42 ← МАКСИМУМ!
2. {item30,item56,item84} => {item49}  lift=18.66
3. {item15,item30,item49} => {item56}  lift=16.58

Какой граф выбрать:
→ Выбери граф который показывает товары: item15, item30, item49, item56, item84
→ На графе должны быть 3 правила с этими товарами
→ Красные/яркие узлы = высокий lift


═══════════════════════════════════════════════════════════════════════════════
АНАЛИЗ ГРАФА
═══════════════════════════════════════════════════════════════════════════════

ПРАВИЛЬНЫЕ ОТВЕТЫ ДЛЯ ТЕСТА:

✓ Support and order have a strong inverse relationship
  (Support и order имеют СИЛЬНУЮ ОБРАТНУЮ зависимость)
  → Чем больше товаров в правиле, тем ниже support

✓ These rules vary from earlier because the associations between these items 
  happen more than expected, but they do not occur more than 80% of the time.
  (Эти правила отличаются потому что связи между товарами сильнее ожидаемого,
   но они НЕ происходят более чем в 80% случаев)
  → Топ-3 по lift имеют confidence: 77%, 74%, 96%
  → Правила 1 и 2 не прошли бы фильтр confidence > 80%

✓ Rule with maximum lift is {item15,item30,item56} => {item49}
  (Правило с максимальным lift: {item15,item30,item56} => {item49})
  → lift = 19.42 - самый высокий во всём датасете!


═══════════════════════════════════════════════════════════════════════════════
ОБУЧАЮЩАЯ И ТЕСТОВАЯ ВЫБОРКИ
═══════════════════════════════════════════════════════════════════════════════

ПРАВИЛЬНЫЕ ОТВЕТЫ ДЛЯ ТЕСТА:

✓ We see that majority of the rules that are present in the training set 
  are also present in the hold out set with similar support and confidences.
  (Большинство правил из обучающей выборки также присутствуют в тестовой
   с похожими значениями support и confidence)

✓ We can conclude by making a test set from hold out data that the rules 
  generated by the algorithm are true for the population we are studying.
  (Мы можем заключить, что правила, найденные алгоритмом, 
   ИСТИННЫ для изучаемой популяции)

ПОЧЕМУ ТАК:
→ Мы разделили данные: 8000 транзакций (обучение) + 2000 (тест)
→ Запустили Apriori на каждом наборе отдельно
→ Большинство правил повторяются в обоих наборах
→ Значения support/confidence/lift похожи
→ ВЫВОД: правила НЕ случайны, они реально существуют в данных!


═══════════════════════════════════════════════════════════════════════════════
КРАТКАЯ ШПАРГАЛКА ДЛЯ ТЕСТА
═══════════════════════════════════════════════════════════════════════════════

ЧИСЛА ДЛЯ ЗАПОМИНАНИЯ:
──────────────────────
item13 = самый частый товар (49.5% транзакций)
26 = размер самой большой транзакции
11435 = количество правил при confidence=0%
1165 = количество правил при confidence=50%

КЛЮЧЕВЫЕ ПРАВИЛА:
─────────────────
Тёмно-зелёный на матрице: {item55} => {item34}

Топ-3 по lift (все правила):
1. {item15,item30,item56} => {item49}  lift=19.42 ← МАКСИМУМ!
2. {item30,item56,item84} => {item49}  lift=18.66
3. {item15,item30,item49} => {item56}  lift=16.58

Случайные правила (support >= 10%):
{item5} => {item13}, {item30} => {item13}, {item13} => {item5}

Правило #4 с наименьшим lift (из conf>0.8):
{item10,item44} => {item13}

КЛЮЧЕВЫЕ ЗАКОНОМЕРНОСТИ:
────────────────────────
Lift vs Support = СИЛЬНАЯ ОБРАТНАЯ зависимость
Интересные правила = ЛЕВЫЙ ВЕРХНИЙ угол на графике Support vs Lift
Случайные правила = ВНИЗУ графика (lift ≈ 1)
Больше товаров в правиле = НИЖЕ support (обратная зависимость)


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                        ЧАСТЬ 1: БАЗОВЫЕ ПОНЯТИЯ
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

ЧТО ТАКОЕ АССОЦИАТИВНЫЕ ПРАВИЛА?
--------------------------------
Это метод поиска связей между товарами в транзакциях.
Классический пример: "Люди, которые покупают хлеб, часто покупают масло"

Правило записывается так: {хлеб} => {масло}
- Левая часть (LHS - Left Hand Side) = антецедент = условие
- Правая часть (RHS - Right Hand Side) = консеквент = следствие


ТРИ ГЛАВНЫЕ МЕТРИКИ:
--------------------

1. SUPPORT (ПОДДЕРЖКА)
   ═══════════════════
   ЧТО: Как часто правило встречается во ВСЕХ транзакциях
   
   ФОРМУЛА: Support(A => B) = Транзакции с A И B / Все транзакции
   
   ПРИМЕР:
   - 10000 транзакций всего
   - 500 содержат и хлеб, и масло
   - Support = 500/10000 = 0.05 = 5%
   
   ИНТЕРПРЕТАЦИЯ:
   - Высокий support = правило часто встречается
   - Низкий support = редкое правило (может быть интересным!)
   
   МИНИМАЛЬНЫЙ SUPPORT:
   - В задании: 1% (0.01)
   - Это значит: правило должно встречаться минимум в 100 из 10000 транзакций


2. CONFIDENCE (ДОСТОВЕРНОСТЬ/УВЕРЕННОСТЬ)
   ═══════════════════════════════════════
   ЧТО: Если купили A, какова вероятность что купят и B?
   
   ФОРМУЛА: Confidence(A => B) = Support(A и B) / Support(A)
   
   ПРИМЕР:
   - 1000 транзакций с хлебом
   - 500 из них также содержат масло
   - Confidence = 500/1000 = 0.5 = 50%
   
   ИНТЕРПРЕТАЦИЯ:
   - 50% confidence = в половине случаев когда покупают хлеб, берут и масло
   - 100% confidence = ВСЕГДА когда покупают хлеб, берут масло
   
   ПРОБЛЕМА:
   - Высокий confidence не всегда значит интересное правило!
   - Если масло и так покупает 90% людей, то confidence будет высоким
     просто из-за популярности масла


3. LIFT (ПОДЪЁМ/ЛИФТ)
   ═══════════════════
   ЧТО: Насколько чаще A и B встречаются вместе, чем если бы были независимы
   
   ФОРМУЛА: Lift(A => B) = Confidence(A => B) / Support(B)
            или
            Lift(A => B) = Support(A и B) / (Support(A) × Support(B))
   
   ИНТЕРПРЕТАЦИЯ:
   - Lift = 1: A и B независимы (связи нет)
   - Lift > 1: A и B встречаются вместе ЧАЩЕ, чем ожидалось (положительная связь)
   - Lift < 1: A и B встречаются вместе РЕЖЕ, чем ожидалось (отрицательная связь)
   
   ПРИМЕР:
   - Support(хлеб и масло) = 5%
   - Support(хлеб) = 10%
   - Support(масло) = 20%
   - Если независимы: ожидаем 10% × 20% = 2%
   - Lift = 5% / 2% = 2.5
   - Значит: хлеб и масло покупают вместе в 2.5 раза чаще, чем по случайности!

   ВАЖНО: Lift - ГЛАВНЫЙ показатель "интересности" правила!


ВИЗУАЛЬНОЕ СРАВНЕНИЕ МЕТРИК:
────────────────────────────────────────────────────────────────
                                                               
  Support = "Как часто?"                                       
            │                                                  
            │    ████                                          
            │    ████████                                      
            │    ████████████                                  
            └────────────────────                              
                Все транзакции                                 
                                                               
  Confidence = "Если A, то B?"                                 
            │                                                  
            │    ████████████  ← транзакции с A                
            │    ████████      ← из них с B                    
            │                                                  
            └────────────────                                  
                                                               
  Lift = "Случайность или связь?"                              
            │                                                  
       >1   │    ★★★ Интересно!                                
        1   │    ─── Независимы                                
       <1   │    ✗✗✗ Избегают друг друга                       
            └────────────────                                  
────────────────────────────────────────────────────────────────


АЛГОРИТМ APRIORI:
-----------------
Это алгоритм для поиска ассоциативных правил.

КАК РАБОТАЕТ (простым языком):
1. Находит все товары с support >= минимального
2. Комбинирует их в пары, тройки и т.д.
3. Отсеивает комбинации с низким support
4. Из оставшихся формирует правила
5. Фильтрует по confidence

ПРИНЦИП APRIORI:
"Если набор товаров редкий, то любой набор, содержащий его, тоже редкий"
Это позволяет не проверять миллионы комбинаций.


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                    ЧАСТЬ 2: ЧТО ДЕЛАЕТ КОД (построчно)
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

ЗАГРУЗКА ДАННЫХ:
----------------
read.transactions("AssociationRules.csv", format = "basket", sep = " ")

- format = "basket": каждая строка = одна транзакция, товары через разделитель
- sep = " ": товары разделены пробелом
- Результат: объект класса "transactions" (специальный формат для arules)


АНАЛИЗ ЧАСТОТЫ ТОВАРОВ:
-----------------------
itemFrequency(trans)           # Частота каждого товара (0 до 1)
itemFrequencyPlot(trans)       # Визуализация частот

size(trans)                    # Размер каждой транзакции (сколько товаров)
max(size(trans))               # Самая большая транзакция


МАЙНИНГ ПРАВИЛ:
---------------
apriori(trans, parameter = list(
  support = 0.01,      # Минимальный support 1%
  confidence = 0,      # Минимальный confidence 0%
  minlen = 2           # Минимум 2 товара в правиле
))

Почему minlen = 2?
- Правило {A} => {} бессмысленно
- Нужно минимум: {A} => {B}


ФИЛЬТРАЦИЯ ПРАВИЛ:
------------------
subset(rules, confidence > 0.8)    # Только правила с confidence > 80%
subset(rules, lift > 2)            # Только с lift > 2
subset(rules, support >= 0.1)      # Встречаются минимум в 10% транзакций


СОРТИРОВКА:
-----------
sort(rules, by = "lift")           # По lift (возрастание)
sort(rules, by = "lift", decreasing = TRUE)  # По lift (убывание)


ВИЗУАЛИЗАЦИИ:
-------------
1. plot(rules, measure = c("support", "confidence"), shading = "lift")
   - X-axis: support
   - Y-axis: confidence  
   - Цвет: lift

2. plot(rules, method = "matrix")
   - Матрица: LHS vs RHS
   - Цвет показывает метрику

3. plot(rules, method = "graph")
   - Граф связей между товарами
   - Узлы = товары, рёбра = правила


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                      ЧАСТЬ 3: ПОДРОБНЫЕ ОТВЕТЫ НА ЗАДАНИЕ
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

a) САМЫЙ ЧАСТЫЙ ТОВАР:
   → item13 (встречается в ~49.5% транзакций, около 4948 раз)
   → Далее идут item5 (37%), item30 (33%), item10 (30%)

b) РАЗМЕР САМОЙ БОЛЬШОЙ ТРАНЗАКЦИИ:
   → 26 товаров (проверь max_size в выводе)

c) КОЛИЧЕСТВО ПРАВИЛ (support=1%, confidence=0%):
   → 11435 правил
   → Это ВСЕ возможные правила с поддержкой >= 1%

d) КОЛИЧЕСТВО ПРАВИЛ (support=1%, confidence=50%):
   → 1165 правил
   → В ~10 раз меньше! Confidence отсек слабые правила

e) КАК CONFIDENCE ВЛИЯЕТ НА КОЛИЧЕСТВО ПРАВИЛ:
   → Чем ВЫШЕ confidence, тем МЕНЬШЕ правил
   → При confidence=25% остаются только правила где B следует за A 
     минимум в 25% случаев
   
   Это как фильтр:
   ┌─────────────────────────────────────────────────────────┐
   │  Confidence 0%   ████████████████████████  (11435)      │
   │  Confidence 25%  ████████████████         (~5000)       │
   │  Confidence 50%  ██████████               (1165)        │
   │  Confidence 75%  ████                     (~200)        │
   │  Confidence 100% █                        (единицы)     │
   └─────────────────────────────────────────────────────────┘

f) ГДЕ ИНТЕРЕСНЫЕ ПРАВИЛА (Support vs Confidence, shading=Lift):
   → Интересные = ЯРКИЙ ЦВЕТ (высокий lift)
   → Они в ЛЕВОЙ ВЕРХНЕЙ части графика:
     - Левая = низкий support (редкие комбинации)
     - Верхняя = высокий confidence (надёжное предсказание)
   → Эти точки далеко от границ графика

g) НАБЛЮДЕНИЯ НА ГРАФИКЕ Support vs Lift:
   → СИЛЬНАЯ ОБРАТНАЯ ЗАВИСИМОСТЬ
   → Чем выше support → тем НИЖЕ lift
   
   Lift │
    20  │ ★★        ← редкие но ОЧЕНЬ сильные
    10  │   ★★★★
     5  │      ★★★★★★★
     2  │         ★★★★★★★★★★★
     1  │────────────────────── ← случайные
        └────────────────────── Support
              1%    5%    10%

h) ГДЕ ИНТЕРЕСНЫЕ И ПОЛЕЗНЫЕ ПРАВИЛА:
   → TOP LEFT на графике Support vs Lift
   → Высокий lift (> 5), достаточный support (> 1%)
   
   Примеры:
   {item15,item30,item49} => {item56}  lift=16.6 ← ОТЛИЧНО!
   {item55} => {item34}               lift=7.7  ← ХОРОШО!

i) ГДЕ БЕСПОЛЕЗНЫЕ (EXTRANEOUS) ПРАВИЛА:
   → BOTTOM часть графика (lift ≈ 1)
   → Это правила где товары встречаются вместе СЛУЧАЙНО
   → Apriori генерирует их, но они бесполезны
   
   Ожидаемое (expected) = Support(A) × Support(B)
   Фактическое (observed) = Support(A и B)
   Если observed ≈ expected → Lift ≈ 1 → СЛУЧАЙНОСТЬ

j) 3 ПРАВИЛА С SUPPORT >= 10% (случайные):
   1. {item5} => {item13}   support=18.8%, lift=1.03
   2. {item30} => {item13}  support=17.5%, lift=1.07
   3. {item13} => {item5}   support=18.8%, lift=1.03
   
   → Lift ≈ 1 = встречаются вместе ПО СОВПАДЕНИЮ
   → Просто оба популярны, реальной связи нет!

k) 10 ПРАВИЛ С НАИМЕНЬШИМ LIFT (из confidence > 0.8):
   1.  {item30,item95,item96} => {item13}  lift=1.62
   2.  {item23,item5} => {item13}          lift=1.70
   3.  {item83} => {item13}                lift=1.71
   4.  {item10,item44} => {item13}         lift=1.72
   5.  {item82,item99} => {item13}         lift=1.73
   6.  {item23} => {item13}                lift=1.74
   7.  {item3,item84,item95} => {item13}   lift=1.77
   8.  {item5,item82,item99} => {item13}   lift=1.81
   9.  {item20,item23} => {item13}         lift=1.84
   10. {item16,item25,item77} => {item5}   lift=2.18
   
   Coincidental? YES (lift < 2 для 1-9)
   item13 очень популярен, поэтому много правил к нему с высоким
   confidence но низким lift

l) 4 ВЫДЕЛЯЮЩИХСЯ ПРАВИЛА НА МАТРИЦЕ:
   1. {item15,item30,item49} => {item56}  lift=16.58, conf=96%
   2. {item15,item49} => {item56}         lift=14.88, conf=86%
   3. {item30,item49,item56} => {item15}  lift=9.24, conf=96%
   4. {item49,item56} => {item15}         lift=9.15, conf=95%
   
   Тёмно-зелёный: {item55} => {item34} (lift=7.69, conf=85.5%)

m) ТЁМНО-СИНИЙ/ГОЛУБОЙ ЦВЕТ НА МАТРИЦЕ:
   → Низкий lift (близкий к 1-3)
   → Эти правила менее интересны
   → Товары встречаются вместе скорее по случайности

n) ТОП-3 ПРАВИЛА ПО LIFT (ИЗ ВСЕХ ПРАВИЛ):
   1. {item15,item30,item56} => {item49}  lift=19.42, conf=77%
   2. {item30,item56,item84} => {item49}  lift=18.66, conf=74%
   3. {item15,item30,item49} => {item56}  lift=16.58, conf=96%
   
   ПОЧЕМУ ОТЛИЧАЮТСЯ ОТ confidence > 0.8?
   → Правила 1 и 2 имеют confidence < 80% (77% и 74%)
   → Они НЕ прошли фильтр confidence > 0.8
   → Но у них САМЫЙ ВЫСОКИЙ lift!
   → Вывод: высокий lift ≠ высокий confidence

o) GRAPH-BASED ВИЗУАЛИЗАЦИЯ:
   → Круглые узлы = ТОВАРЫ
   → Маленькие точки = ПРАВИЛА
   → Стрелки: LHS → rule → RHS
   → Размер = support, цвет = lift
   
   Видно кластер: item15-item30-item49-item56-item84

p) СВЯЗЬ ПРАВИЛ С ORDER (количеством товаров):
   → Чем БОЛЬШЕ товаров → тем НИЖЕ support (ОБРАТНАЯ зависимость)
   → Но lift может быть ВЫШЕ у длинных правил
   
   {item15,item49} => {item56}        (3 товара) lift=14.9
   {item15,item30,item49} => {item56} (4 товара) lift=16.6

q) TRAINING/TESTING:
   → Разделили: 8000 train + 2000 test
   → Большинство правил есть в ОБОИХ наборах
   → Support/confidence/lift похожи
   → Значит правила НЕ артефакт данных!

r) ОБОБЩЕНИЕ НА ПОПУЛЯЦИЮ:
   → ДА, можно обобщить, потому что:
   1. Правила повторяются в train и test
   2. Lift значительно > 1
   3. Выборка достаточная (10000 транзакций)


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                 ЧАСТЬ 4: ВОЗМОЖНЫЕ ВОПРОСЫ НА ЗАЩИТЕ
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

В: Что такое ассоциативные правила?
О: Метод Data Mining для поиска связей между товарами в транзакциях.
   Показывают, какие товары часто покупают вместе.

В: Назовите три основные метрики.
О: Support (частота), Confidence (условная вероятность), Lift (сила связи).

В: Что показывает Support?
О: Как часто данная комбинация товаров встречается во всех транзакциях.
   Support = (число транзакций с A и B) / (общее число транзакций)

В: Что показывает Confidence?
О: Вероятность купить B, если уже купил A.
   Confidence = Support(A и B) / Support(A)

В: Что показывает Lift? Почему это важно?
О: Показывает, насколько чаще товары встречаются вместе по сравнению со 
   случайным совпадением. Lift > 1 = положительная связь, Lift = 1 = случайность.
   Это ГЛАВНАЯ метрика для оценки "интересности" правила.

В: Что такое алгоритм Apriori?
О: Алгоритм для эффективного поиска частых наборов и ассоциативных правил.
   Использует принцип: "если набор редкий, все его надмножества тоже редкие".

В: Почему высокий Confidence не всегда означает интересное правило?
О: Если товар B очень популярный (его покупает 90% людей), то любое правило
   {X} => {B} будет иметь высокий confidence просто из-за популярности B,
   а не из-за реальной связи с X. Нужно смотреть на Lift!

В: Как выбрать минимальный Support?
О: Зависит от задачи:
   - Высокий support (5-10%): только частые, надёжные правила
   - Низкий support (0.1-1%): находим редкие, но возможно интересные связи
   - Слишком низкий: много шума и случайных правил

В: Что означает Lift = 1.5?
О: Товары встречаются вместе в 1.5 раза чаще, чем если бы были независимы.
   Есть умеренная положительная связь.

В: Что означает Lift < 1?
О: Товары встречаются вместе РЕЖЕ, чем ожидалось.
   Возможно, они заменяют друг друга (товары-субституты).

В: Зачем делить данные на train/test?
О: Чтобы проверить, что найденные правила реальны, а не случайны.
   Если правило есть в обоих наборах — это настоящая закономерность.

В: Какие правила считаются "хорошими"?
О: Lift > 1.5-2, Confidence > 50%, Support достаточный для бизнеса.
   Но зависит от контекста и целей анализа.

В: Недостатки алгоритма Apriori?
О: 1. Генерирует много "экстренных" (неинтересных) правил
   2. При низком support - экспоненциальный рост правил
   3. Не учитывает время и последовательность покупок
   4. Может быть медленным на больших данных

В: Где применяются ассоциативные правила?
О: - Рекомендательные системы ("с этим товаром покупают...")
   - Расстановка товаров в магазине
   - Кросс-продажи и маркетинг
   - Анализ поведения пользователей на сайте


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                         ЧАСТЬ 5: ФОРМУЛЫ
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

Support(A => B) = P(A ∩ B) = count(A,B) / N

Confidence(A => B) = P(B|A) = Support(A,B) / Support(A)

Lift(A => B) = Confidence(A => B) / Support(B)
             = Support(A,B) / (Support(A) × Support(B))


ИНТЕРПРЕТАЦИЯ LIFT:
───────────────────
Lift > 1  →  Положительная связь (покупают вместе чаще ожидаемого)
Lift = 1  →  Независимы (случайное совпадение)
Lift < 1  →  Отрицательная связь (избегают друг друга)


ТИПИЧНЫЕ ПОРОГИ:
────────────────
Support:    0.01 - 0.05 (1-5%)
Confidence: 0.5 - 0.8 (50-80%)
Lift:       > 1.5 - 2 (интересные правила)


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
                 ЧАСТЬ 6: CHECK YOUR KNOWLEDGE (5 вопросов)
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

═══════════════════════════════════════════════════════════════════════════════
1. What is the Apriori property and how is it used in the Apriori algorithm?
═══════════════════════════════════════════════════════════════════════════════

APRIORI PROPERTY (Принцип Apriori):
"Если набор товаров (itemset) является редким (infrequent), то любой набор,
содержащий его, тоже будет редким."

Или наоборот:
"Все подмножества частого набора тоже должны быть частыми."

КАК ИСПОЛЬЗУЕТСЯ В АЛГОРИТМЕ:
1. Сначала находим все ОДИНОЧНЫЕ товары с support >= минимального
2. Комбинируем их в ПАРЫ и проверяем support
3. Если пара {A,B} редкая → НЕ проверяем {A,B,C}, {A,B,D} и т.д.
4. Это ОТСЕКАЕТ миллионы ненужных проверок!

ПРИМЕР:
- Минимальный support = 1%
- {молоко, хлеб} встречается в 0.5% транзакций (< 1%)
- Значит {молоко, хлеб, масло} ТОЧНО будет встречаться ещё реже
- Не нужно проверять все комбинации с {молоко, хлеб}

ПОЧЕМУ ЭТО ВАЖНО:
- Без Apriori property нужно проверить 2^n комбинаций (для 100 товаров = 2^100!)
- С Apriori property проверяем только "перспективные" комбинации
- Алгоритм работает за разумное время


═══════════════════════════════════════════════════════════════════════════════
2. List three popular use cases of the Association Rules mining algorithms.
═══════════════════════════════════════════════════════════════════════════════

1. MARKET BASKET ANALYSIS (Анализ корзины покупок)
   - Какие товары покупают вместе?
   - Размещение товаров на полках магазина
   - Формирование акционных наборов
   - Пример: "Пиво и чипсы часто покупают вместе → поставить рядом"

2. RECOMMENDATION SYSTEMS (Рекомендательные системы)
   - "Клиенты, купившие X, также купили Y"
   - Amazon, Netflix, Spotify используют подобные алгоритмы
   - Cross-selling и up-selling в e-commerce
   - Пример: "Вы купили телефон → вам может понравиться чехол"

3. MEDICAL DIAGNOSIS (Медицинская диагностика)
   - Поиск связей между симптомами и болезнями
   - Анализ побочных эффектов лекарств
   - Выявление факторов риска заболеваний
   - Пример: "Пациенты с симптомами A и B часто имеют диагноз C"

ДОПОЛНИТЕЛЬНО (если спросят больше):
4. Web Usage Mining - анализ поведения на сайтах
5. Fraud Detection - выявление мошенничества
6. Bioinformatics - анализ генных последовательностей


═══════════════════════════════════════════════════════════════════════════════
3. What is the difference between Lift and Leverage? How is Lift used?
═══════════════════════════════════════════════════════════════════════════════

LIFT (Подъём):
─────────────
Формула: Lift(A => B) = Support(A,B) / (Support(A) × Support(B))
         или: Lift = Confidence(A => B) / Support(B)

Интерпретация:
- Lift = 1: A и B независимы (встречаются вместе случайно)
- Lift > 1: A и B встречаются вместе ЧАЩЕ, чем ожидалось
- Lift < 1: A и B встречаются вместе РЕЖЕ, чем ожидалось

Диапазон: от 0 до +∞


LEVERAGE (Рычаг):
────────────────
Формула: Leverage(A => B) = Support(A,B) - Support(A) × Support(B)

Интерпретация:
- Leverage = 0: A и B независимы
- Leverage > 0: A и B встречаются вместе ЧАЩЕ, чем ожидалось
- Leverage < 0: A и B встречаются вместе РЕЖЕ, чем ожидалось

Диапазон: от -0.25 до +0.25


КЛЮЧЕВОЕ РАЗЛИЧИЕ:
──────────────────
| Характеристика | Lift           | Leverage              |
|----------------|----------------|-----------------------|
| Тип            | ОТНОШЕНИЕ      | РАЗНОСТЬ              |
| Формула        | Observed/Expected | Observed - Expected |
| Диапазон       | [0, +∞)        | [-0.25, +0.25]        |
| Независимость  | = 1            | = 0                   |
| Масштаб        | Относительный  | Абсолютный            |

Lift показывает "во сколько раз чаще"
Leverage показывает "на сколько чаще в абсолютных числах"


КАК LIFT ИСПОЛЬЗУЕТСЯ ДЛЯ ОЦЕНКИ КАЧЕСТВА ПРАВИЛ:
─────────────────────────────────────────────────
1. Фильтрация: отбираем правила с Lift > 1 (или > 1.5, > 2)
2. Ранжирование: сортируем правила по Lift (выше = интереснее)
3. Исключение случайных связей: Lift ≈ 1 = правило бесполезно
4. Валидация: высокий Lift = реальная связь, не случайность

ПРИМЕР:
- Правило {A} => {B} с Lift = 5
- Значит: A и B покупают вместе В 5 РАЗ ЧАЩЕ, чем если бы были независимы
- Это сильная, интересная связь!


═══════════════════════════════════════════════════════════════════════════════
4. Define Support and Confidence
═══════════════════════════════════════════════════════════════════════════════

SUPPORT (Поддержка):
───────────────────
ОПРЕДЕЛЕНИЕ: 
Доля транзакций, содержащих данный набор товаров, от общего числа транзакций.

ФОРМУЛА:
Support(A => B) = |Транзакции с A и B| / |Все транзакции|
Support(A => B) = P(A ∩ B)

ПРИМЕР:
- 10000 транзакций всего
- 300 содержат и хлеб, и масло
- Support({хлеб} => {масло}) = 300/10000 = 0.03 = 3%

ИНТЕРПРЕТАЦИЯ:
- Показывает КАК ЧАСТО правило встречается в данных
- Высокий support = правило применимо ко многим клиентам
- Низкий support = редкое правило (может быть интересным для нишевых рекомендаций)

ИСПОЛЬЗОВАНИЕ:
- Устанавливаем минимальный порог (напр. 1%)
- Отсекаем слишком редкие правила


CONFIDENCE (Достоверность/Уверенность):
──────────────────────────────────────
ОПРЕДЕЛЕНИЕ:
Условная вероятность того, что транзакция содержит B, при условии что она 
уже содержит A.

ФОРМУЛА:
Confidence(A => B) = Support(A и B) / Support(A)
Confidence(A => B) = P(B | A)

ПРИМЕР:
- 1000 транзакций содержат хлеб
- 300 из них также содержат масло
- Confidence({хлеб} => {масло}) = 300/1000 = 0.3 = 30%

ИНТЕРПРЕТАЦИЯ:
- Показывает НАСКОЛЬКО НАДЁЖНО правило
- Confidence 30% = в 30% случаев когда покупают хлеб, берут и масло
- Confidence 90% = очень надёжное предсказание

ИСПОЛЬЗОВАНИЕ:
- Устанавливаем минимальный порог (напр. 50%)
- Отсекаем ненадёжные правила

ВАЖНО:
Высокий Confidence НЕ гарантирует интересное правило!
Если B и так покупает 90% людей, Confidence будет высоким просто из-за
популярности B, а не из-за связи с A. Поэтому нужен LIFT!


═══════════════════════════════════════════════════════════════════════════════
5. How do you use a "hold-out" dataset to evaluate the effectiveness of rules?
═══════════════════════════════════════════════════════════════════════════════

ЧТО ТАКОЕ HOLD-OUT DATASET:
──────────────────────────
Это часть данных, которую мы НЕ используем для построения модели,
а оставляем для проверки (валидации) найденных правил.

ПРОЦЕДУРА:
─────────
1. РАЗДЕЛЕНИЕ ДАННЫХ:
   - Training set (80%): 8000 транзакций → для поиска правил
   - Test/Hold-out set (20%): 2000 транзакций → для проверки

2. ПОСТРОЕНИЕ МОДЕЛИ:
   - Запускаем Apriori на training set
   - Получаем набор правил с их support, confidence, lift

3. ВАЛИДАЦИЯ НА HOLD-OUT:
   - Запускаем Apriori на test set с теми же параметрами
   - ИЛИ проверяем, встречаются ли найденные правила в test set

4. СРАВНЕНИЕ РЕЗУЛЬТАТОВ:
   - Сколько правил есть в ОБОИХ наборах?
   - Похожи ли support/confidence/lift?
   - Если да → правила РЕАЛЬНЫ, не случайны

ЗАЧЕМ ЭТО НУЖНО:
───────────────
1. ИЗБЕЖАТЬ OVERFITTING:
   - Модель может "запомнить" случайные паттерны в training data
   - Hold-out показывает, работают ли правила на новых данных

2. ПРОВЕРИТЬ ОБОБЩАЕМОСТЬ:
   - Если правило работает на hold-out → оно скорее всего истинно
   - Если правило НЕ работает → возможно, это артефакт данных

3. ОЦЕНИТЬ СТАБИЛЬНОСТЬ:
   - Сравниваем метрики (support, confidence, lift) между наборами
   - Похожие значения = стабильные, надёжные правила

КРИТЕРИИ УСПЕХА:
───────────────
✓ Большинство правил из training есть в test
✓ Support/Confidence/Lift похожи в обоих наборах
✓ Топ-правила по lift совпадают

ПРИМЕР ИЗ НАШЕЙ РАБОТЫ:
───────────────────────
- Training (8000): нашли ~1000 правил
- Testing (2000): нашли ~900 правил
- Общих правил: ~70-80%
- Средний lift похож в обоих наборах
→ ВЫВОД: правила реальны и применимы к популяции!


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
     ЧАСТЬ 7: TRAINING/TESTING - ПРОСТОЕ ОБЪЯСНЕНИЕ (ДЛЯ ЧАЙНИКОВ)
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

ЗАЧЕМ ВООБЩЕ ДЕЛИТЬ ДАННЫЕ НА 8000 И 2000?
──────────────────────────────────────────

Представь ситуацию:
1. У тебя 10000 чеков из магазина
2. Ты нашёл правило: "Кто покупает пиво, покупает чипсы" (lift = 5)
3. ВОПРОС: это РЕАЛЬНАЯ закономерность или просто СЛУЧАЙНОСТЬ?

Может быть так:
- Просто в этих 10000 чеках случайно так совпало
- На следующей неделе всё будет по-другому
- Правило НЕ РАБОТАЕТ в реальности


КАК ПРОВЕРИТЬ ЧТО ПРАВИЛО РЕАЛЬНОЕ?
───────────────────────────────────

ИДЕЯ: Разделить данные на две части и проверить отдельно!

┌─────────────────────────────────────────────────────────────┐
│                   ВСЕ ДАННЫЕ (10000)                        │
│  ┌──────────────────────┐  ┌─────────────────────┐          │
│  │  TRAINING (8000)     │  │  TESTING (2000)     │          │
│  │  Ищем правила тут    │  │  Проверяем тут      │          │
│  └──────────────────────┘  └─────────────────────┘          │
└─────────────────────────────────────────────────────────────┘

ШАГ 1: Берём первые 8000 транзакций (TRAINING)
        → Запускаем Apriori
        → Находим правила

ШАГ 2: Берём последние 2000 транзакций (TESTING)
        → Запускаем Apriori
        → Находим правила

ШАГ 3: СРАВНИВАЕМ!
        → Если правило есть в ОБОИХ наборах → оно РЕАЛЬНОЕ
        → Если только в одном → возможно СЛУЧАЙНОЕ


АНАЛОГИЯ ДЛЯ ПОНИМАНИЯ:
───────────────────────

Это как проверка лекарства:
- Группа 1 (training): даём лекарство → смотрим эффект
- Группа 2 (testing): даём лекарство → смотрим эффект
- Если эффект ОДИНАКОВЫЙ в обеих группах → лекарство РАБОТАЕТ
- Если только в одной группе → возможно, это случайность


ПОЧЕМУ ИМЕННО 8000 И 2000?
──────────────────────────

80% / 20% - это стандартное соотношение:
- 80% данных для обучения (чтобы найти достаточно правил)
- 20% для проверки (чтобы было достаточно данных для валидации)

Можно было бы 70/30 или 90/10, но 80/20 - классика.


В КОДЕ ЭТО ВЫГЛЯДИТ ТАК:
────────────────────────

train_trans <- trans[1:8000]        # Берём транзакции 1-8000
test_trans <- trans[8001:10000]     # Берём транзакции 8001-10000

# Ищем правила на TRAINING
rules_train <- apriori(train_trans, parameter = list(
  support = 0.01, 
  confidence = 0.5
))

# Ищем правила на TESTING
rules_test <- apriori(test_trans, parameter = list(
  support = 0.01, 
  confidence = 0.5
))

# Сравниваем: сколько правил совпало?
common_rules <- intersect(train_rules, test_rules)


ЧТО МЫ ПОЛУЧИЛИ В НАШЕЙ РАБОТЕ:
──────────────────────────────

Training (8000 транзакций):
- Нашли ~1000 правил
- Средний lift = X

Testing (2000 транзакций):
- Нашли ~900 правил
- Средний lift = Y (похожий на X!)

Общих правил: ~70-80%

ВЫВОД: Большинство правил ПОВТОРЯЮТСЯ!
       Значит они РЕАЛЬНЫЕ, а не случайные!


ОТВЕТЫ НА ВОПРОСЫ ТЕСТА (q, r):
──────────────────────────────

q) "Justify that the relationships we see are not just an artifact"
   (Обоснуйте, что связи не являются артефактом данных)
   
   ОТВЕТ: Большинство правил из training set также присутствуют 
   в testing set с похожими значениями support и confidence.
   Это доказывает, что правила РЕАЛЬНЫ, а не случайны.

r) "Can we conclude that the association rules are actually true?"
   (Можем ли мы заключить, что правила истинны для популяции?)
   
   ОТВЕТ: ДА! Потому что:
   1. Правила повторяются в независимых подвыборках
   2. Метрики (support, confidence, lift) похожи в обоих наборах
   3. Это стандартный метод валидации в Data Science


▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
     ЧАСТЬ 8: ПОЧЕМУ "APRIORI"? + АЛГОРИТМ FP-GROWTH
▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀

═══════════════════════════════════════════════════════════════════════════════
ВОПРОС 1: ПОЧЕМУ АЛГОРИТМ НАЗЫВАЕТСЯ "APRIORI"?
═══════════════════════════════════════════════════════════════════════════════

ПРОИСХОЖДЕНИЕ СЛОВА:
────────────────────
"A PRIORI" - латинское выражение, означающее:
- "ИЗ ПРЕДШЕСТВУЮЩЕГО" 
- "ЗАРАНЕЕ ИЗВЕСТНЫЙ"
- "ДО ОПЫТА"

В философии "априорное знание" - это знание, которое можно получить
БЕЗ опыта, только ЛОГИЧЕСКИМ путём.

Пример априорного знания:
- "2 + 2 = 4" - это мы знаем без экспериментов
- "Если А > B и B > C, то А > C" - логический вывод


ПОЧЕМУ АЛГОРИТМ ТАК НАЗВАЛИ?
────────────────────────────

Потому что он использует АПРИОРНОЕ ЗНАНИЕ - принцип, который мы знаем
ЗАРАНЕЕ, без проверки данных:

╔═══════════════════════════════════════════════════════════════════════════╗
║                         APRIORI PRINCIPLE                                  ║
║              (Принцип Априори / Свойство Антимонотонности)                ║
║                                                                            ║
║   "Если набор товаров РЕДКИЙ, то ЛЮБОЙ набор, СОДЕРЖАЩИЙ его,             ║
║    тоже ОБЯЗАТЕЛЬНО будет РЕДКИМ"                                          ║
║                                                                            ║
║   Или наоборот:                                                            ║
║   "Все ПОДМНОЖЕСТВА частого набора тоже должны быть ЧАСТЫМИ"              ║
╚═══════════════════════════════════════════════════════════════════════════╝

ПОЧЕМУ ЭТО ЛОГИЧЕСКИ ВЕРНО?
───────────────────────────

Представь:
- Набор {молоко, хлеб} встречается в 100 транзакциях
- Набор {молоко, хлеб, масло} - это ТОТ ЖЕ набор + ещё масло
- Он может встречаться ТОЛЬКО в тех транзакциях, где УЖЕ есть {молоко, хлеб}
- Значит, максимум 100 транзакций, а скорее МЕНЬШЕ!

Визуально:
┌─────────────────────────────────────────────────────────────────┐
│                    ВСЕ ТРАНЗАКЦИИ (10000)                       │
│  ┌────────────────────────────────────────────────────┐         │
│  │      Транзакции с {молоко, хлеб} (100)             │         │
│  │   ┌─────────────────────────────────────────┐      │         │
│  │   │  Транзакции с {молоко, хлеб, масло}     │      │         │
│  │   │  (максимум 100, обычно меньше!)         │      │         │
│  │   └─────────────────────────────────────────┘      │         │
│  └────────────────────────────────────────────────────┘         │
└─────────────────────────────────────────────────────────────────┘


ПРИМЕР ИСПОЛЬЗОВАНИЯ В АЛГОРИТМЕ:
─────────────────────────────────

Минимальный support = 1% (100 из 10000 транзакций)

ШАГ 1: Проверяем одиночные товары
       {молоко} = 5% ✓
       {хлеб} = 4% ✓
       {масло} = 3% ✓
       {икра} = 0.1% ✗ (отбрасываем)

ШАГ 2: Проверяем пары (только из частых одиночных!)
       {молоко, хлеб} = 2% ✓
       {молоко, масло} = 1.5% ✓
       {хлеб, масло} = 0.5% ✗ (отбрасываем)

ШАГ 3: Проверяем тройки
       {молоко, хлеб, масло} - НЕ ПРОВЕРЯЕМ!
       
       ПОЧЕМУ? Априорное знание говорит:
       → Этот набор содержит {хлеб, масло}
       → {хлеб, масло} редкий (0.5%)
       → Значит {молоко, хлеб, масло} ТОЧНО редкий
       → Нет смысла тратить время на проверку!


ПОЧЕМУ ЭТО ТАК ВАЖНО?
─────────────────────

Без Apriori Principle:
- 100 товаров в магазине
- Нужно проверить ВСЕ возможные комбинации
- 2^100 = 1,267,650,600,228,229,401,496,703,205,376 комбинаций
- Это БОЛЬШЕ чем атомов во Вселенной!
- Компьютер будет считать МИЛЛИАРДЫ лет

С Apriori Principle:
- Отсекаем "ветки" с редкими наборами на ранних этапах
- Проверяем только "перспективные" комбинации
- Алгоритм работает за СЕКУНДЫ или МИНУТЫ

ИТОГ: Алгоритм назван APRIORI потому что использует
      АПРИОРНОЕ (заранее известное) знание о том,
      что редкие наборы нельзя "расширить" в частые.


═══════════════════════════════════════════════════════════════════════════════
ВОПРОС 2: АЛГОРИТМ FP-GROWTH (Frequent Pattern Growth)
═══════════════════════════════════════════════════════════════════════════════

ЧТО ЭТО ТАКОЕ?
──────────────
FP-Growth - это УЛУЧШЕННЫЙ алгоритм для поиска частых наборов.
Разработан Хань, Пей и Йин (Han, Pei, Yin) в 2000 году.

Название расшифровывается как "Frequent Pattern Growth" - 
"Рост Частых Паттернов"


ГЛАВНАЯ ПРОБЛЕМА APRIORI:
─────────────────────────

Apriori работает так:
1. Сгенерировать КАНДИДАТОВ (возможные наборы)
2. Просканировать ВСЮ базу данных
3. Подсчитать support для каждого кандидата
4. Повторить для больших наборов

ПРОБЛЕМА: Если товаров много → генерируется ОГРОМНОЕ количество кандидатов
          Каждый раз нужно сканировать ВСЮ базу данных!


ИДЕЯ FP-GROWTH:
───────────────

╔═══════════════════════════════════════════════════════════════════════════╗
║  СЖАТЬ всю базу данных в компактную структуру FP-TREE,                    ║
║  а потом искать паттерны БЕЗ ГЕНЕРАЦИИ КАНДИДАТОВ!                        ║
╚═══════════════════════════════════════════════════════════════════════════╝


ЧТО ТАКОЕ FP-TREE?
──────────────────

FP-Tree (Frequent Pattern Tree) - это специальное ДЕРЕВО, которое:
- Хранит ВСЮ информацию о транзакциях
- Занимает МЕНЬШЕ места чем исходные данные
- Позволяет быстро находить частые паттерны

ПРИМЕР:
───────

Исходные транзакции:
┌──────────────┬─────────────────────────┐
│ Транзакция   │ Товары                  │
├──────────────┼─────────────────────────┤
│ T1           │ хлеб, молоко, масло     │
│ T2           │ хлеб, молоко            │
│ T3           │ хлеб, сыр               │
│ T4           │ молоко, масло           │
│ T5           │ хлеб, молоко, масло     │
└──────────────┴─────────────────────────┘

ШАГ 1: Подсчитать частоту каждого товара
       хлеб:4, молоко:4, масло:3, сыр:1
       
       Сортируем по убыванию: хлеб > молоко > масло > сыр

ШАГ 2: Упорядочить товары в каждой транзакции (по частоте)
       T1: хлеб, молоко, масло
       T2: хлеб, молоко
       T3: хлеб, сыр
       T4: молоко, масло
       T5: хлеб, молоко, масло

ШАГ 3: Построить FP-Tree (добавляя транзакции одну за другой)

                         [root]
                        /      \
                   хлеб:4     молоко:1
                      |           |
                  молоко:3     масло:1
                   /    \
              масло:2   сыр:1

Что это означает:
- "хлеб:4" = хлеб встречается в 4 транзакциях
- Путь "хлеб → молоко → масло" встречается 2 раза (T1, T5)
- Путь "хлеб → молоко" встречается 3 раза (T1, T2, T5)
- Путь "хлеб → сыр" встречается 1 раз (T3)


КАК FP-GROWTH ИЩЕТ ПАТТЕРНЫ:
────────────────────────────

1. Начинает с НАИМЕНЕЕ частого товара (снизу вверх)

2. Для каждого товара строит "условное" FP-дерево
   (только пути, ведущие к этому товару)

3. Рекурсивно находит паттерны в условном дереве

4. НЕ ГЕНЕРИРУЕТ КАНДИДАТОВ! Просто обходит дерево.


ПРЕИМУЩЕСТВА FP-GROWTH:
───────────────────────

✓ Только 2 прохода по базе данных:
  1-й проход: подсчёт частот товаров
  2-й проход: построение дерева

✓ НЕ генерирует кандидатов (главное преимущество!)
  
✓ Сжимает данные:
  - Общие префиксы хранятся ОДИН раз
  - Экономия памяти в 10-100 раз

✓ На практике в 10-100 раз БЫСТРЕЕ чем Apriori


НЕДОСТАТКИ FP-GROWTH:
─────────────────────

✗ Сложная реализация (дерево + рекурсия)

✗ FP-tree может не поместиться в память
  (если данных ОЧЕНЬ много или они разнородные)

✗ Не подходит для потоковых данных
  (нужны ВСЕ данные сразу для построения дерева)


СРАВНЕНИЕ APRIORI И FP-GROWTH:
─────────────────────────────

┌─────────────────────┬──────────────────┬──────────────────┐
│ Характеристика      │ APRIORI          │ FP-GROWTH        │
├─────────────────────┼──────────────────┼──────────────────┤
│ Проходов по БД      │ Много (K+1)      │ Только 2         │
│ Генерация кандидатов│ ДА (много!)      │ НЕТ              │
│ Структура данных    │ Хэш-таблицы      │ FP-tree          │
│ Скорость            │ Медленный        │ Быстрый          │
│ Память              │ Много            │ Меньше           │
│ Сложность кода      │ Простой          │ Сложный          │
│ Масштабируемость    │ Плохая           │ Хорошая          │
└─────────────────────┴──────────────────┴──────────────────┘

КОГДА ИСПОЛЬЗОВАТЬ:
───────────────────
• APRIORI - для обучения, небольших данных, простых задач
• FP-GROWTH - для реальных больших данных, продакшена

В R (пакет arules):
- apriori() - алгоритм Apriori
- По умолчанию apriori() может использовать оптимизации близкие к FP-Growth


═══════════════════════════════════════════════════════════════════════════════
FP-GROWTH: ПОДРОБНЫЙ ПОШАГОВЫЙ ПРИМЕР
═══════════════════════════════════════════════════════════════════════════════

Разберём FP-Growth на конкретном примере от начала до конца.

ИСХОДНЫЕ ДАННЫЕ:
────────────────
Минимальный support = 2 транзакции (из 5)

┌──────────────┬─────────────────────────────────┐
│ Транзакция   │ Товары                          │
├──────────────┼─────────────────────────────────┤
│ T1           │ A, B, D, E                      │
│ T2           │ B, C, E                         │
│ T3           │ A, B, D, E                      │
│ T4           │ A, B, C, E                      │
│ T5           │ A, B, C, D, E                   │
└──────────────┴─────────────────────────────────┘


ШАГ 1: ПЕРВЫЙ ПРОХОД - ПОДСЧЁТ ЧАСТОТ
──────────────────────────────────────

Считаем сколько раз встречается каждый товар:

┌────────┬──────────┬─────────────────────┐
│ Товар  │ Частота  │ Частый? (≥2)        │
├────────┼──────────┼─────────────────────┤
│ A      │ 4        │ ДА ✓                │
│ B      │ 5        │ ДА ✓                │
│ C      │ 3        │ ДА ✓                │
│ D      │ 3        │ ДА ✓                │
│ E      │ 5        │ ДА ✓                │
└────────┴──────────┴─────────────────────┘

Сортируем по УБЫВАНИЮ частоты:
B:5 → E:5 → A:4 → C:3 → D:3

Порядок важен! Более частые товары будут БЛИЖЕ к корню дерева.


ШАГ 2: УПОРЯДОЧИВАНИЕ ТРАНЗАКЦИЙ
────────────────────────────────

Переписываем каждую транзакцию, располагая товары по частоте:

┌──────────────┬─────────────────┬─────────────────────────┐
│ Транзакция   │ Исходные        │ Упорядоченные (B>E>A>C>D)│
├──────────────┼─────────────────┼─────────────────────────┤
│ T1           │ A, B, D, E      │ B, E, A, D              │
│ T2           │ B, C, E         │ B, E, C                 │
│ T3           │ A, B, D, E      │ B, E, A, D              │
│ T4           │ A, B, C, E      │ B, E, A, C              │
│ T5           │ A, B, C, D, E   │ B, E, A, C, D           │
└──────────────┴─────────────────┴─────────────────────────┘


ШАГ 3: ПОСТРОЕНИЕ FP-TREE (ВТОРОЙ ПРОХОД)
─────────────────────────────────────────

Добавляем транзакции ОДНУ ЗА ДРУГОЙ:

После T1 (B, E, A, D):
                    [root]
                       │
                     B:1
                       │
                     E:1
                       │
                     A:1
                       │
                     D:1

После T2 (B, E, C):
                    [root]
                       │
                     B:2          ← увеличили счётчик
                       │
                     E:2          ← увеличили счётчик
                      / \
                   A:1   C:1      ← новая ветка для C
                    │
                   D:1

После T3 (B, E, A, D):
                    [root]
                       │
                     B:3
                       │
                     E:3
                      / \
                   A:2   C:1      ← увеличили A
                    │
                   D:2            ← увеличили D

После T4 (B, E, A, C):
                    [root]
                       │
                     B:4
                       │
                     E:4
                      / \
                   A:3   C:1
                   / \
                 D:2  C:1         ← новая ветка C под A

После T5 (B, E, A, C, D):
                    [root]
                       │
                     B:5
                       │
                     E:5
                      / \
                   A:4   C:1
                   / \
                 D:2  C:2         ← увеличили C под A
                       │
                      D:1         ← новый D под C


ФИНАЛЬНОЕ FP-TREE:
──────────────────

                         [root]
                            │
                          B:5
                            │
                          E:5
                         /   \
                      A:4     C:1
                     /   \
                   D:2    C:2
                           │
                          D:1

Также создаём HEADER TABLE (связные списки для быстрого доступа):

┌────────┬──────────┬─────────────────────────┐
│ Товар  │ Частота  │ Указатели на узлы       │
├────────┼──────────┼─────────────────────────┤
│ B      │ 5        │ → B:5                   │
│ E      │ 5        │ → E:5                   │
│ A      │ 4        │ → A:4                   │
│ C      │ 3        │ → C:1 → C:2             │
│ D      │ 3        │ → D:2 → D:1             │
└────────┴──────────┴─────────────────────────┘


ШАГ 4: МАЙНИНГ ПАТТЕРНОВ (снизу вверх по Header Table)
──────────────────────────────────────────────────────

Начинаем с НАИМЕНЕЕ частого товара и идём вверх.

МАЙНИНГ ДЛЯ D (частота 3):
──────────────────────────

Находим все пути к D в дереве:

Путь 1: root → B:5 → E:5 → A:4 → D:2
        Условный паттерн: {B, E, A} : 2

Путь 2: root → B:5 → E:5 → A:4 → C:2 → D:1
        Условный паттерн: {B, E, A, C} : 1

Строим УСЛОВНОЕ FP-дерево для D:
(берём только пути с support ≥ 2)

Условная база: {B, E, A}:2, {B, E, A, C}:1

Частоты в условной базе:
B:3, E:3, A:3, C:1

C отбрасываем (частота < 2)

Условное FP-дерево для D:
        [root]
           │
         B:3
           │
         E:3
           │
         A:3

Частые паттерны с D:
- {D} : 3
- {B, D} : 3
- {E, D} : 3
- {A, D} : 3
- {B, E, D} : 3
- {B, A, D} : 3
- {E, A, D} : 3
- {B, E, A, D} : 3


МАЙНИНГ ДЛЯ C (частота 3):
──────────────────────────

Пути к C:
Путь 1: root → B → E → C:1 (support 1)
Путь 2: root → B → E → A → C:2 (support 2)

Условная база для C: {B, E}:1, {B, E, A}:2

Частоты: B:3, E:3, A:2

Условное FP-дерево для C:
        [root]
           │
         B:3
           │
         E:3
           │
         A:2

Частые паттерны с C:
- {C} : 3
- {B, C} : 3
- {E, C} : 3
- {A, C} : 2
- {B, E, C} : 3
- {B, A, C} : 2
- {E, A, C} : 2
- {B, E, A, C} : 2


И ТАК ДАЛЕЕ ДЛЯ A, E, B...


ИТОГОВЫЕ ЧАСТЫЕ НАБОРЫ:
───────────────────────

┌─────────────────────┬──────────┐
│ Набор               │ Support  │
├─────────────────────┼──────────┤
│ {B}                 │ 5        │
│ {E}                 │ 5        │
│ {A}                 │ 4        │
│ {C}                 │ 3        │
│ {D}                 │ 3        │
│ {B, E}              │ 5        │
│ {B, A}              │ 4        │
│ {B, C}              │ 3        │
│ {B, D}              │ 3        │
│ {E, A}              │ 4        │
│ {E, C}              │ 3        │
│ {E, D}              │ 3        │
│ {A, C}              │ 2        │
│ {A, D}              │ 3        │
│ {B, E, A}           │ 4        │
│ {B, E, C}           │ 3        │
│ {B, E, D}           │ 3        │
│ {B, A, C}           │ 2        │
│ {B, A, D}           │ 3        │
│ {E, A, C}           │ 2        │
│ {E, A, D}           │ 3        │
│ {B, E, A, C}        │ 2        │
│ {B, E, A, D}        │ 3        │
│ ...                 │ ...      │
└─────────────────────┴──────────┘


ПОЧЕМУ FP-GROWTH БЫСТРЕЕ APRIORI?
─────────────────────────────────

1. СЖАТИЕ ДАННЫХ:
   - 5 транзакций превратились в компактное дерево
   - Общие префиксы хранятся ОДИН раз
   - Чем больше данных, тем больше экономия

2. НЕТ ГЕНЕРАЦИИ КАНДИДАТОВ:
   - Apriori: генерирует {A,B}, {A,C}, {A,D}... → проверяет каждый
   - FP-Growth: просто обходит дерево

3. МЕНЬШЕ ПРОХОДОВ ПО ДАННЫМ:
   - Apriori: K+1 проходов (K = максимальный размер набора)
   - FP-Growth: всего 2 прохода

4. ЛОКАЛЬНОСТЬ:
   - Все данные о товаре собраны вместе (header table)
   - Эффективное использование кэша процессора


ПСЕВДОКОД FP-GROWTH:
────────────────────

function FP_Growth(transactions, min_support):
    
    # ШАГ 1: Подсчёт частот
    item_counts = count_items(transactions)
    frequent_items = filter(item_counts, >= min_support)
    
    # ШАГ 2: Построение FP-tree
    fp_tree = build_FP_tree(transactions, frequent_items)
    
    # ШАГ 3: Майнинг паттернов
    patterns = mine_patterns(fp_tree, min_support)
    
    return patterns


function mine_patterns(fp_tree, min_support, suffix = {}):
    patterns = []
    
    # Для каждого товара (снизу вверх по частоте)
    for item in fp_tree.header_table (reverse order):
        
        # Новый паттерн = текущий товар + суффикс
        new_pattern = {item} ∪ suffix
        patterns.add(new_pattern)
        
        # Построить условную базу паттернов
        conditional_base = get_conditional_base(fp_tree, item)
        
        # Построить условное FP-дерево
        conditional_tree = build_FP_tree(conditional_base)
        
        # Рекурсивно майнить
        if conditional_tree is not empty:
            patterns += mine_patterns(conditional_tree, min_support, new_pattern)
    
    return patterns


ВАЖНЫЕ ОПТИМИЗАЦИИ FP-GROWTH:
─────────────────────────────

1. SINGLE-PATH OPTIMIZATION:
   Если дерево состоит из одного пути → 
   все комбинации узлов — частые паттерны

2. HEADER TABLE LINKS:
   Связные списки позволяют быстро найти ВСЕ узлы товара

3. NODE MERGING:
   Узлы с одинаковым родителем можно объединять

4. PARALLEL FP-GROWTH:
   Разные ветви дерева можно обрабатывать параллельно


================================================================================
                              УДАЧИ НА ЗАЩИТЕ!
================================================================================

