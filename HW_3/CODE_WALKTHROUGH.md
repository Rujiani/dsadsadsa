# Пошаговое объяснение кода: Что происходит в программе

## Общая структура проекта

Проект состоит из двух основных частей:
1. **Часть 1**: One-vs-All логистическая регрессия (`ex3.m`)
2. **Часть 2**: Нейронная сеть (`ex3_nn.m`)

---

## Часть 1: One-vs-All Логистическая Регрессия

### Файл: `ex3.m` - Главный скрипт

```matlab
load('ex3data1.mat');  % Загружаем данные
```

**Что происходит:**
- Загружается файл с 5000 примерами рукописных цифр
- `X` - матрица 5000×400 (5000 примеров, каждый - 400 пикселей)
- `y` - вектор 5000×1 с метками (1-10, где 10 = цифра 0)

### Функция 1: `lrCostFunction.m` - Вычисление стоимости и градиента

#### Что делает функция:
Вычисляет, насколько хорошо модель предсказывает, и как нужно изменить параметры для улучшения.

#### Пошаговый разбор кода:

```matlab
h = sigmoid(X * theta);
```

**Что происходит:**
- `X * theta` - матричное умножение:
  - `X`: m×n (m примеров, n признаков)
  - `theta`: n×1 (параметры модели)
  - Результат: m×1 (предсказание для каждого примера)
- `sigmoid(...)` - применяет сигмоидную функцию к каждому элементу
- **Результат:** вектор вероятностей для всех примеров одновременно (векторизация!)

**Пример:**
```
Если X = [пример1; пример2; пример3] и theta = [θ₀; θ₁; ...]
То X * theta = [θᵀ·пример1; θᵀ·пример2; θᵀ·пример3]
И h = [вероятность1; вероятность2; вероятность3]
```

```matlab
J = (1/m) * sum(-y .* log(h) - (1-y) .* log(1-h));
```

**Что происходит:**
- `y .* log(h)` - поэлементное умножение (element-wise)
- Для каждого примера вычисляется: `-y·log(h) - (1-y)·log(1-h)`
- `sum(...)` - суммирует по всем примерам
- `(1/m)` - усреднение

**Зачем это нужно:**
- Если `y=1` и `h=1` (правильно предсказали) → стоимость ≈ 0
- Если `y=1` и `h=0` (неправильно) → стоимость → ∞
- Это **бинарная кросс-энтропия** - мера ошибки

```matlab
J = J + (lambda/(2*m)) * sum(theta(2:end).^2);
```

**Что происходит:**
- `theta(2:end)` - все параметры кроме первого (θ₀ не регуляризуем!)
- `.^2` - возведение в квадрат каждого элемента
- `sum(...)` - сумма квадратов
- Добавляем штраф за большие веса

**Зачем регуляризация:**
- Предотвращает переобучение
- Заставляет модель использовать меньшие веса
- `lambda` контролирует силу регуляризации

```matlab
grad = (1/m) * (X' * (h - y));
```

**Что происходит:**
- `(h - y)` - ошибка для каждого примера (m×1)
- `X'` - транспонированная матрица X (n×m)
- `X' * (h - y)` - вектор градиента (n×1)
- Каждый элемент - это производная по соответствующему параметру

**Математически:**
```
grad[j] = (1/m) * Σ (h(x⁽ⁱ⁾) - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾
```

Это говорит нам: "в какую сторону нужно изменить каждый параметр θⱼ"

```matlab
temp = theta;
temp(1) = 0;  % Don't regularize theta0
grad = grad + (lambda/m) * temp;
```

**Что происходит:**
- Создаем копию theta
- Обнуляем первый элемент (θ₀ не регуляризуем)
- Добавляем регуляризационный член к градиенту

**Результат:** Градиент теперь учитывает регуляризацию для всех параметров кроме θ₀

---

### Функция 2: `oneVsAll.m` - Обучение множества классификаторов

#### Что делает функция:
Обучает 10 отдельных бинарных классификаторов, каждый различает одну цифру от всех остальных.

#### Пошаговый разбор:

```matlab
X = [ones(m, 1) X];  % Добавляем столбец единиц (bias term)
```

**Что происходит:**
- Добавляем столбец единиц в начало матрицы X
- Теперь X: m×(n+1) вместо m×n
- Это позволяет модели иметь "свободный член" (bias)

**Пример:**
```
Было: X = [x₁₁ x₁₂ ... x₁₄₀₀]
Стало: X = [1 x₁₁ x₁₂ ... x₁₄₀₀]
```

```matlab
for c = 1:num_labels
    y_binary = (y == c);
```

**Что происходит:**
- Для каждой цифры (1-10) создаем бинарные метки
- `y == c` - логическое сравнение, возвращает вектор 0 и 1
- Если пример принадлежит классу c → 1, иначе → 0

**Пример для c=3:**
```
y = [1; 3; 2; 3; 1; ...]
y_binary = [0; 1; 0; 1; 0; ...]  (1 только там, где y==3)
```

```matlab
[theta] = fmincg(@(t)(lrCostFunction(t, X, y_binary, lambda)), ...
                 initial_theta, options);
```

**Что происходит:**
- `fmincg` - функция оптимизации (аналог градиентного спуска)
- Ищет theta, который минимизирует функцию стоимости
- `@(t)(lrCostFunction(...))` - анонимная функция (cost function)
- `initial_theta` - начальные значения (обычно нули)
- `options` - настройки (максимум итераций, использовать градиент)

**Процесс:**
1. Начинаем с `initial_theta = zeros(n+1, 1)`
2. Вычисляем стоимость и градиент через `lrCostFunction`
3. Обновляем theta в направлении, противоположном градиенту
4. Повторяем до сходимости (50 итераций)

```matlab
all_theta(c, :) = theta(:)';
```

**Что происходит:**
- Сохраняем найденные параметры в строку матрицы `all_theta`
- `theta(:)'` - преобразует столбец в строку
- Результат: матрица 10×(n+1), где каждая строка - параметры для одного класса

---

### Функция 3: `predictOneVsAll.m` - Предсказание

#### Что делает функция:
Использует все 10 обученных классификаторов для предсказания цифры.

#### Пошаговый разбор:

```matlab
X = [ones(m, 1) X];  % Добавляем bias term
```

**Что происходит:** То же самое - добавляем столбец единиц

```matlab
predictions = sigmoid(X * all_theta');
```

**Что происходит:**
- `all_theta'` - транспонируем матрицу параметров
  - Было: 10×(n+1)
  - Стало: (n+1)×10
- `X * all_theta'` - матричное умножение
  - X: m×(n+1)
  - all_theta': (n+1)×10
  - Результат: m×10

**Что содержит результат:**
Каждая строка - это вероятности принадлежности к каждому из 10 классов для одного примера.

**Пример:**
```
predictions = [
  0.1  0.05  0.8  0.02  ...  (пример 1: вероятнее всего класс 3)
  0.9  0.05  0.02  0.01  ...  (пример 2: вероятнее всего класс 1)
  ...
]
```

```matlab
[~, p] = max(predictions, [], 2);
```

**Что происходит:**
- `max(..., [], 2)` - находит максимум по строкам (dimension 2)
- Первый выход `~` - максимальное значение (игнорируем)
- Второй выход `p` - индекс максимума (номер класса!)

**Пример:**
```
predictions = [0.1  0.05  0.8  0.02  ...]
              ↑     ↑     ↑     ↑
              1     2     3     4    (индексы)

max = 0.8, индекс = 3 → предсказание: цифра 3
```

---

## Часть 2: Нейронная Сеть (ДЕТАЛЬНО)

### Файл: `ex3_nn.m` - Главный скрипт

```matlab
load('ex3weights.mat');  % Загружаем предобученные веса
```

**Что происходит:**
- Загружаются уже обученные параметры нейронной сети
- `Theta1`: 25×401 (веса между входным и скрытым слоем)
- `Theta2`: 10×26 (веса между скрытым и выходным слоем)

**Почему такие размерности?**
- Входной слой: 400 признаков + 1 bias = 401
- Скрытый слой: 25 нейронов
- Выходной слой: 10 нейронов (по одному на каждую цифру)

---

### Функция: `predict.m` - Feedforward Propagation

#### Что делает функция:
Проходит данные через нейронную сеть от входа к выходу, вычисляя предсказание.

#### Архитектура сети:

```
Входной слой (401) → Скрытый слой (25) → Выходной слой (10)
     a⁽¹⁾                  a⁽²⁾              a⁽³⁾
```

#### Пошаговый разбор кода:

### Шаг 1: Подготовка входных данных

```matlab
X = [ones(m, 1) X];
```

**Что происходит:**
- Добавляем столбец единиц (bias unit)
- Теперь X: m×401 (было m×400)

**Зачем нужен bias:**
- Позволяет модели "сдвигать" активацию
- Аналогично свободному члену в линейной регрессии

**Визуализация:**
```
Было:  [пиксель₁  пиксель₂  ...  пиксель₄₀₀]
Стало: [1         пиксель₁  ...  пиксель₄₀₀]
       ↑
    bias unit
```

### Шаг 2: Входной слой → Скрытый слой

```matlab
z2 = X * Theta1';
```

**Что происходит:**
- `Theta1'` - транспонируем веса (было 25×401, стало 401×25)
- `X * Theta1'` - матричное умножение
  - X: m×401
  - Theta1': 401×25
  - Результат z2: m×25

**Математически:**
```
z²ⱼ = θⱼ₀·1 + θⱼ₁·x₁ + θⱼ₂·x₂ + ... + θⱼ₄₀₀·x₄₀₀
```

Для каждого из 25 нейронов скрытого слоя вычисляется взвешенная сумма входов.

**Почему транспонируем:**
- В `Theta1` каждая **строка** соответствует одному нейрону
- Для матричного умножения нужно, чтобы столбцы первой матрицы совпадали со строками второй
- Поэтому транспонируем: строки становятся столбцами

**Пример для одного примера:**
```
X = [1 x₁ x₂ ... x₄₀₀]  (1×401)
Theta1 = [
  θ₁₀  θ₁₁  θ₁₂  ...  θ₁₄₀₀  (веса для нейрона 1)
  θ₂₀  θ₂₁  θ₂₂  ...  θ₂₄₀₀  (веса для нейрона 2)
  ...
]  (25×401)

z2 = X * Theta1' = [
  θ₁₀·1 + θ₁₁·x₁ + ...  (вход для нейрона 1)
  θ₂₀·1 + θ₂₁·x₁ + ...  (вход для нейрона 2)
  ...
]  (1×25)
```

```matlab
a2 = sigmoid(z2);
```

**Что происходит:**
- Применяем сигмоидную функцию к каждому элементу z2
- Результат: активации скрытого слоя (a2: m×25)

**Зачем сигмоид:**
- Преобразует линейную комбинацию в значение от 0 до 1
- Это "активация" нейрона - насколько он "возбужден"
- Позволяет сети моделировать нелинейные зависимости

**Визуализация:**
```
z2 = [-2.5, 1.3, -0.8, ...]  (взвешенные суммы)
     ↓ sigmoid
a2 = [0.08, 0.79, 0.31, ...]  (активации, 0-1)
```

### Шаг 3: Добавление bias к скрытому слою

```matlab
a2 = [ones(m, 1) a2];
```

**Что происходит:**
- Добавляем столбец единиц в начало матрицы активаций
- Теперь a2: m×26 (было m×25)

**Зачем:**
- Bias unit нужен для следующего слоя
- Позволяет выходному слою иметь "свободный член"

**Визуализация:**
```
Было:  [активация₁  активация₂  ...  активация₂₅]
Стало: [1           активация₁  ...  активация₂₅]
       ↑
    bias unit
```

### Шаг 4: Скрытый слой → Выходной слой

```matlab
z3 = a2 * Theta2';
```

**Что происходит:**
- `Theta2'` - транспонируем веса (было 10×26, стало 26×10)
- `a2 * Theta2'` - матричное умножение
  - a2: m×26
  - Theta2': 26×10
  - Результат z3: m×10

**Математически:**
```
z³ₖ = θₖ₀·1 + θₖ₁·a²₁ + θₖ₂·a²₂ + ... + θₖ₂₅·a²₂₅
```

Для каждого из 10 выходных нейронов (по одному на каждую цифру) вычисляется взвешенная сумма активаций скрытого слоя.

**Что это означает:**
- Каждый выходной нейрон "голосует" за свою цифру
- Чем больше активация, тем больше уверенность, что это именно эта цифра

**Пример для одного примера:**
```
a2 = [1 a²₁ a²₂ ... a²₂₅]  (1×26)
Theta2 = [
  θ₁₀  θ₁₁  θ₁₂  ...  θ₁₂₅  (веса для "цифра 1")
  θ₂₀  θ₂₁  θ₂₂  ...  θ₂₂₅  (веса для "цифра 2")
  ...
]  (10×26)

z3 = a2 * Theta2' = [
  θ₁₀·1 + θ₁₁·a²₁ + ...  (голос за "цифра 1")
  θ₂₀·1 + θ₂₁·a²₁ + ...  (голос за "цифра 2")
  ...
]  (1×10)
```

```matlab
a3 = sigmoid(z3);
```

**Что происходит:**
- Применяем сигмоидную функцию
- Результат: вероятности для каждого класса (a3: m×10)

**Интерпретация:**
- Каждый элемент a3 - это вероятность того, что входное изображение - это соответствующая цифра
- Сумма всех вероятностей не обязательно равна 1 (это не softmax!)

**Пример:**
```
a3 = [0.1, 0.05, 0.8, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005, 0.005]
      ↑    ↑     ↑     ↑     ↑     ↑     ↑      ↑      ↑      ↑
      1    2     3     4     5     6     7      8      9      0
```

Здесь модель наиболее уверена, что это цифра 3 (вероятность 0.8).

### Шаг 5: Выбор предсказания

```matlab
[~, p] = max(a3, [], 2);
```

**Что происходит:**
- `max(..., [], 2)` - находит максимум по строкам
- Первый выход `~` - максимальное значение (игнорируем)
- Второй выход `p` - индекс (номер класса с максимальной вероятностью)

**Результат:**
- Вектор p: m×1
- Каждый элемент - предсказанная цифра для соответствующего примера

**Пример:**
```
a3 = [
  0.1  0.05  0.8  ...  (пример 1)
  0.9  0.05  0.02 ...  (пример 2)
  ...
]

p = [3; 1; ...]  (пример 1 → цифра 3, пример 2 → цифра 1)
```

---

## Визуализация полного процесса (Нейронная сеть)

### Для одного примера изображения:

```
1. ВХОДНОЙ СЛОЙ (400 пикселей + 1 bias)
   [1, пиксель₁, пиксель₂, ..., пиксель₄₀₀]
   
2. УМНОЖЕНИЕ НА ВЕСА Theta1 (25×401)
   → Получаем 25 взвешенных сумм
   [z²₁, z²₂, ..., z²₂₅]
   
3. ПРИМЕНЕНИЕ СИГМОИДА
   → Получаем 25 активаций
   [a²₁, a²₂, ..., a²₂₅]
   
4. ДОБАВЛЕНИЕ BIAS
   [1, a²₁, a²₂, ..., a²₂₅]
   
5. УМНОЖЕНИЕ НА ВЕСА Theta2 (10×26)
   → Получаем 10 взвешенных сумм
   [z³₁, z³₂, ..., z³₁₀]
   
6. ПРИМЕНЕНИЕ СИГМОИДА
   → Получаем 10 вероятностей
   [p₁, p₂, ..., p₁₀]
   
7. ВЫБОР МАКСИМУМА
   → Предсказание: argmax(p₁, p₂, ..., p₁₀)
```

### Для всех примеров одновременно (векторизация):

```
X (5000×401) 
  → z2 (5000×25) 
    → a2 (5000×25) 
      → a2 с bias (5000×26)
        → z3 (5000×10)
          → a3 (5000×10)
            → p (5000×1)
```

**Преимущество векторизации:**
- Обрабатываем все 5000 примеров одновременно
- Используем оптимизированные матричные операции
- Намного быстрее, чем цикл по примерам

---

## Ключевые отличия: Логистическая регрессия vs Нейронная сеть

### Логистическая регрессия:
- **Один слой:** вход → выход
- **Линейные границы решений**
- **Проще, быстрее обучается**
- **Точность: ~94.9%**

### Нейронная сеть:
- **Три слоя:** вход → скрытый → выход
- **Нелинейные границы решений** (благодаря скрытому слою)
- **Сложнее, но мощнее**
- **Точность: ~97.5%**

**Почему нейронная сеть лучше:**
- Скрытый слой извлекает сложные признаки из пикселей
- Может моделировать нелинейные зависимости
- Лучше справляется с вариациями в написании цифр

---

## Матричные размерности: Справочник

### One-vs-All:
- `X`: m×400 → m×401 (после добавления bias)
- `theta`: (n+1)×1 для каждого класса
- `all_theta`: 10×(n+1)
- `predictions`: m×10
- `p`: m×1

### Нейронная сеть:
- `X`: m×400 → m×401 (после добавления bias)
- `Theta1`: 25×401
- `z2`, `a2`: m×25 → m×26 (после добавления bias)
- `Theta2`: 10×26
- `z3`, `a3`: m×10
- `p`: m×1

---

## Почему именно 25 нейронов в скрытом слое?

### Краткий ответ

В данном задании архитектура нейронной сети (включая 25 нейронов в скрытом слое) **уже предопределена** авторами курса. Веса были предобучены и предоставлены в файле `ex3weights.mat`. Однако есть несколько важных причин, почему выбрано именно это число.

### Теоретические соображения

#### 1. **Компромисс между сложностью и эффективностью**

```
Входной слой: 400 признаков
Скрытый слой: 25 нейронов  ← "сжатие" информации
Выходной слой: 10 классов
```

**Что происходит:**
- Скрытый слой выполняет **сжатие размерности** (dimensionality reduction)
- 400 признаков → 25 признаков высокого уровня
- Это позволяет сети извлекать **существенные паттерны**, игнорируя шум

**Аналогия:**
Представьте, что вы описываете изображение цифры:
- **400 пикселей** = все детали (точные координаты каждого пикселя)
- **25 признаков** = важные характеристики (есть ли круг, есть ли прямая линия, угол наклона и т.д.)

#### 2. **Количество параметров**

Давайте посчитаем параметры:

**Слой 1 → Слой 2:**
- Theta1: 25 × 401 = **10,025 параметров**

**Слой 2 → Слой 3:**
- Theta2: 10 × 26 = **260 параметров**

**Всего:** 10,285 параметров

**Почему это важно:**
- С 5000 примерами это разумное соотношение
- Слишком много параметров → переобучение (overfitting)
- Слишком мало → недообучение (underfitting)

#### 3. **Эмпирические правила**

В практике машинного обучения есть несколько эмпирических правил:

**Правило 1:** Размер скрытого слоя обычно между размером входного и выходного слоя
```
400 (вход) > 25 (скрытый) > 10 (выход) ✓
```

**Правило 2:** Размер скрытого слоя ≈ √(входной × выходной)
```
√(400 × 10) ≈ 63
```
25 меньше, но это нормально для более простой модели.

**Правило 3:** Для задач классификации часто используют скрытый слой в 2-10 раз меньше входного
```
400 / 16 = 25 ✓ (примерно в 16 раз меньше)
```

#### 4. **Вычислительная эффективность**

- **25 нейронов** - достаточно для извлечения признаков
- **Не слишком много** - быстрые вычисления
- **Хороший баланс** между точностью и скоростью

### Что было бы, если изменить размер?

#### Слишком мало нейронов (например, 5):

```
Входной (400) → Скрытый (5) → Выходной (10)
```

**Проблемы:**
- Недостаточно "емкости" для извлечения сложных признаков
- Модель слишком простая
- Низкая точность

**Параметры:** 5 × 401 + 10 × 6 = 2,065 (слишком мало)

#### Слишком много нейронов (например, 200):

```
Входной (400) → Скрытый (200) → Выходной (10)
```

**Проблемы:**
- Риск переобучения
- Медленные вычисления
- Больше параметров, чем нужно

**Параметры:** 200 × 401 + 10 × 201 = 82,210 (слишком много для 5000 примеров)

#### Текущий выбор (25):

```
Входной (400) → Скрытый (25) → Выходной (10)
```

**Преимущества:**
- Достаточно для извлечения признаков
- Не переобучается
- Быстрые вычисления
- Хорошая точность (~97.5%)

### Как выбирают размер скрытого слоя на практике?

В реальных проектах размер скрытого слоя выбирают через:

1. **Гиперпараметрический поиск (Grid Search):**
   - Пробуют разные размеры: 10, 25, 50, 100, 200...
   - Выбирают тот, который дает лучшую точность на валидационной выборке

2. **Эвристики:**
   - Начинают с √(входной × выходной)
   - Пробуют значения в диапазоне от этого числа

3. **Эмпирический опыт:**
   - Для изображений 20×20 часто используют 20-50 нейронов
   - Для более сложных задач - больше

4. **Архитектурные паттерны:**
   - Некоторые используют степени двойки: 16, 32, 64, 128...
   - Другие предпочитают числа, кратные 5 или 10

### Визуализация: Что делают 25 нейронов?

Каждый из 25 нейронов в скрытом слое может "отвечать" за определенный признак:

```
Нейрон 1:  "Есть ли вертикальная линия слева?"
Нейрон 2:  "Есть ли горизонтальная линия сверху?"
Нейрон 3:  "Есть ли круг?"
Нейрон 4:  "Есть ли диагональная линия?"
Нейрон 5:  "Есть ли изгиб?"
...
Нейрон 25: "Есть ли пересечение линий?"
```

Затем выходной слой комбинирует эти признаки для распознавания цифр:
- Если активированы нейроны "круг" и "вертикальная линия" → возможно цифра 6 или 8
- Если активированы "горизонтальная линия" и "вертикальная линия" → возможно цифра 4 или 7

### Заключение

**25 нейронов** - это разумный выбор для данной задачи, который:
- Обеспечивает хорошую точность (97.5%)
- Не переобучается
- Эффективен по вычислениям
- Достаточен для извлечения важных признаков из изображений 20×20

В реальных проектах вы бы экспериментировали с разными размерами, но для учебного задания 25 - отличный компромисс!

---

## Часто задаваемые вопросы

### Почему транспонируем веса?
- В матрицах весов каждая **строка** = один нейрон
- Для матричного умножения нужны **столбцы** = веса
- Транспонирование преобразует строки в столбцы

### Зачем нужен bias unit?
- Позволяет модели "сдвигать" активацию
- Аналогично свободному члену в линейной регрессии
- Без него модель была бы менее гибкой

### Почему сигмоид, а не другая функция?
- В этом задании используется сигмоид
- В современных сетях часто используют ReLU, tanh и др.
- Сигмоид хорош для бинарной классификации

### Почему нейронная сеть точнее?
- Скрытый слой извлекает сложные признаки
- Может моделировать нелинейные зависимости
- Больше параметров = больше выразительная сила

---

## Заключение

Код реализует два подхода к классификации:
1. **One-vs-All** - простой, но эффективный
2. **Нейронная сеть** - более сложный, но точный

Оба используют **векторизацию** для эффективных вычислений и обрабатывают все примеры одновременно, что делает код быстрым и эффективным.

