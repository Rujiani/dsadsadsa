# Объяснение лабораторной работы: Классификация рукописных цифр

## Обзор задачи

В этой лабораторной работе мы решаем задачу **многоклассовой классификации** - распознавание рукописных цифр от 0 до 9. У нас есть 5000 обучающих примеров, где каждый пример - это изображение 20×20 пикселей (400 признаков).

## Часть 1: Логистическая регрессия с One-vs-All подходом

### 1.1 Теоретическая основа

#### Логистическая регрессия

**Логистическая регрессия** - это алгоритм классификации, который использует сигмоидную функцию для предсказания вероятности принадлежности к классу:
k
\[
h_\theta(x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}
\]

где:
- \(g(z) = \frac{1}{1 + e^{-z}}\) - сигмоидная функция
- \(\theta\) - параметры модели (веса)
- \(x\) - входные признаки

#### Функция стоимости (Cost Function)

Для логистической регрессии функция стоимости определяется как:

\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
\]

Это выражение представляет собой **бинарную кросс-энтропию** (binary cross-entropy), которая измеряет разницу между предсказанными вероятностями и реальными метками.

#### Регуляризация

Чтобы предотвратить переобучение (overfitting), добавляется **L2-регуляризация**:

\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
\]

Важно: **не регуляризуем** \(\theta_0\) (bias term), так как он не связан с признаками.

#### Градиент функции стоимости

Градиент для нерегуляризованной части:

\[
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
\]

С учетом регуляризации:

\[
\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \quad \text{(без регуляризации)}
\]

\[
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \left[ \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j \right] \quad \text{для } j \geq 1
\]

### 1.2 Векторизация

**Векторизация** - это процесс преобразования циклов в матричные операции для ускорения вычислений.

#### Векторизация функции стоимости

Вместо цикла по всем примерам, мы используем матричные операции:

```matlab
h = sigmoid(X * theta);  % X: m×n, theta: n×1, результат: m×1
```

Это вычисляет \(h_\theta(x^{(i)})\) для всех примеров одновременно!

Функция стоимости векторизуется как:

```matlab
J = (1/m) * sum(-y .* log(h) - (1-y) .* log(1-h));
```

где `.*` - поэлементное умножение.

#### Векторизация градиента

Градиент можно вычислить как:

\[
\nabla_\theta J = \frac{1}{m} X^T (h_\theta(x) - y)
\]

Это эквивалентно вычислению всех частных производных одновременно!

**Доказательство:**
- \(X^T\) имеет размерность \(n \times m\)
- \((h - y)\) имеет размерность \(m \times 1\)
- Результат: \(n \times 1\) - вектор градиента

### 1.3 Реализация: `lrCostFunction.m`

```matlab
function [J, grad] = lrCostFunction(theta, X, y, lambda)
```

**Что делает функция:**
1. Вычисляет гипотезу для всех примеров: `h = sigmoid(X * theta)`
2. Вычисляет функцию стоимости с регуляризацией
3. Вычисляет градиент с регуляризацией

**Ключевые моменты:**
- Используется векторизация (без циклов)
- Регуляризация применяется только к \(\theta_1, \theta_2, ..., \theta_n\) (не к \(\theta_0\))
- Используется `temp(1) = 0` для исключения \(\theta_0\) из регуляризации

### 1.4 One-vs-All классификация

#### Теоретическая основа

Для **многоклассовой классификации** (K классов) мы обучаем K отдельных бинарных классификаторов:

- Классификатор 1: класс 1 vs все остальные
- Классификатор 2: класс 2 vs все остальные
- ...
- Классификатор K: класс K vs все остальные

Для каждого классификатора мы создаем бинарные метки:
- \(y^{(i)} = 1\) если пример принадлежит классу k
- \(y^{(i)} = 0\) если пример принадлежит другому классу

#### Предсказание

При предсказании:
1. Вычисляем вероятность для каждого класса
2. Выбираем класс с максимальной вероятностью

\[
\text{prediction} = \arg\max_{k} h_{\theta_k}(x)
\]

### 1.5 Реализация: `oneVsAll.m`

```matlab
function [all_theta] = oneVsAll(X, y, num_labels, lambda)
```

**Что делает функция:**
1. Для каждого класса \(c = 1, 2, ..., K\):
   - Создает бинарные метки: `y_binary = (y == c)`
   - Обучает логистическую регрессию с этими метками
   - Сохраняет параметры \(\theta_c\) в строку матрицы `all_theta`

**Результат:** Матрица `all_theta` размерности \(K \times (n+1)\), где каждая строка - параметры классификатора для одного класса.

### 1.6 Реализация: `predictOneVsAll.m`

```matlab
function p = predictOneVsAll(all_theta, X)
```

**Что делает функция:**
1. Вычисляет вероятности для всех классов: `predictions = sigmoid(X * all_theta')`
   - Результат: матрица \(m \times K\), где каждая строка - вероятности для одного примера
2. Находит класс с максимальной вероятностью: `[~, p] = max(predictions, [], 2)`
   - `max(..., [], 2)` находит максимум по строкам
   - Второй выход - индекс (номер класса)

## Часть 2: Нейронные сети

### 2.1 Теоретическая основа

#### Архитектура нейронной сети

В этой работе используется **нейронная сеть с одним скрытым слоем**:

```
Входной слой (400 нейронов) 
    ↓
Скрытый слой (25 нейронов)
    ↓
Выходной слой (10 нейронов)
```

**Почему такая архитектура?**
- Входной слой: 400 признаков (20×20 пикселей)
- Скрытый слой: 25 нейронов для извлечения признаков
- Выходной слой: 10 нейронов (по одному на каждую цифру 0-9)

#### Feedforward Propagation (Прямое распространение)

**Алгоритм:**
1. **Слой 1 → Слой 2 (скрытый):**
   \[
   z^{(2)} = \Theta^{(1)} a^{(1)}
   \]
   \[
   a^{(2)} = g(z^{(2)}) = \text{sigmoid}(z^{(2)})
   \]

2. **Добавляем bias unit:** \(a^{(2)} = [1; a^{(2)}]\)

3. **Слой 2 → Слой 3 (выходной):**
   \[
   z^{(3)} = \Theta^{(2)} a^{(2)}
   \]
   \[
   a^{(3)} = g(z^{(3)}) = \text{sigmoid}(z^{(3)})
   \]

4. **Предсказание:** выбираем нейрон с максимальным выходом

#### Матричные размерности

- \(X\): \(m \times 400\) (m примеров, 400 признаков)
- После добавления bias: \(X\): \(m \times 401\)
- \(\Theta^{(1)}\): \(25 \times 401\) (25 нейронов, 401 вход)
- \(\Theta^{(2)}\): \(10 \times 26\) (10 нейронов, 26 входов)

**Почему такие размерности?**
- \(\Theta^{(1)}\) связывает входной слой (401) со скрытым (25)
- \(\Theta^{(2)}\) связывает скрытый слой (26) с выходным (10)

**Почему именно 25 нейронов в скрытом слое?**
- Это компромисс между сложностью модели и эффективностью
- 25 нейронов достаточно для извлечения важных признаков из 400 пикселей
- Слишком мало (например, 5) → недостаточно "емкости" для сложных паттернов
- Слишком много (например, 200) → риск переобучения и медленные вычисления
- 25 обеспечивает хорошую точность (~97.5%) при разумном количестве параметров (10,285)
- В реальных проектах размер выбирают через эксперименты, но для данной задачи 25 - оптимальный выбор

### 2.2 Реализация: `predict.m`

```matlab
function p = predict(Theta1, Theta2, X)
```

**Что делает функция:**

1. **Добавляет bias unit к входному слою:**
   ```matlab
   X = [ones(m, 1) X];  % m × 401
   ```

2. **Вычисляет активации скрытого слоя:**
   ```matlab
   z2 = X * Theta1';      % m × 25
   a2 = sigmoid(z2);       % m × 25
   a2 = [ones(m, 1) a2];   % m × 26 (добавляем bias)
   ```

3. **Вычисляет активации выходного слоя:**
   ```matlab
   z3 = a2 * Theta2';      % m × 10
   a3 = sigmoid(z3);       % m × 10
   ```

4. **Находит класс с максимальной вероятностью:**
   ```matlab
   [~, p] = max(a3, [], 2);  % m × 1
   ```

**Важно:** Используется транспонирование матриц весов (`Theta1'`, `Theta2'`), потому что:
- В матрицах весов каждая **строка** соответствует одному нейрону
- Для матричного умножения нужно транспонировать

## Сравнение подходов

### Логистическая регрессия (One-vs-All)
- **Точность:** ~94.9%
- **Преимущества:** Простота, интерпретируемость
- **Недостатки:** Линейные границы решений

### Нейронная сеть
- **Точность:** ~97.5%
- **Преимущества:** Нелинейные границы решений, лучшее качество
- **Недостатки:** Сложнее, требует больше вычислений

## Ключевые концепции

### 1. Векторизация
- **Зачем:** Ускорение вычислений (использование оптимизированных библиотек)
- **Как:** Замена циклов матричными операциями
- **Пример:** `X * theta` вместо цикла по примерам

### 2. Регуляризация
- **Зачем:** Предотвращение переобучения
- **Как:** Добавление штрафа за большие веса
- **Параметр:** \(\lambda\) контролирует силу регуляризации

### 3. One-vs-All
- **Зачем:** Преобразование многоклассовой задачи в несколько бинарных
- **Как:** Обучаем K классификаторов, каждый различает один класс от остальных

### 4. Feedforward Propagation
- **Зачем:** Вычисление предсказаний нейронной сети
- **Как:** Последовательное применение матричных умножений и активаций

## Заключение

Эта лабораторная работа демонстрирует:
1. **Векторизацию** для эффективных вычислений
2. **Регуляризацию** для предотвращения переобучения
3. **One-vs-All** подход для многоклассовой классификации
4. **Нейронные сети** для нелинейной классификации

Оба подхода успешно решают задачу распознавания рукописных цифр, но нейронная сеть показывает лучшие результаты благодаря способности моделировать нелинейные зависимости.

