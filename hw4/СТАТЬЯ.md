# Обучение нейронных сетей методом обратного распространения ошибки: практическое исследование

## Введение

Нейронные сети представляют собой мощный инструмент машинного обучения, способный решать сложные задачи классификации. В данной работе рассматривается практическая реализация алгоритма обратного распространения ошибки (backpropagation) для обучения нейронной сети, предназначенной для распознавания рукописных цифр.

Задача классификации рукописных цифр является классической проблемой в области машинного обучения. Каждое изображение представляет собой матрицу 20×20 пикселей (400 признаков), которую необходимо отнести к одному из 10 классов (цифры от 0 до 9). Для решения этой задачи используется трехслойная нейронная сеть с одним скрытым слоем.

## Архитектура нейронной сети

### Структура сети

Исследуемая нейронная сеть состоит из трех слоев:

- **Входной слой**: 400 единиц, соответствующих пикселям изображения 20×20
- **Скрытый слой**: 30 единиц (оптимальное значение, определенное экспериментально)
- **Выходной слой**: 10 единиц, каждая из которых соответствует вероятности принадлежности к одному из классов

Каждый слой соединен с последующим посредством матриц весов. Матрица весов между входным и скрытым слоем (Theta1) имеет размерность 30×401, где дополнительный столбец соответствует смещению (bias). Аналогично, матрица весов между скрытым и выходным слоем (Theta2) имеет размерность 10×31.

### Роль смещения (bias)

Смещение представляет собой дополнительный параметр, позволяющий нейрону смещать активационную функцию. Это критически важно для способности сети моделировать сложные зависимости в данных. Визуально bias можно представить как дополнительный вход, всегда равный единице, с собственным весом.

## Прямой проход: вычисление предсказаний

Процесс получения предсказания от нейронной сети называется прямым проходом (feedforward propagation). Этот процесс включает последовательное вычисление активаций каждого слоя.

### Математическая модель

Для каждого слоя вычисления выполняются в два этапа:

1. **Линейная комбинация**: вычисляется взвешенная сумма входов
   ```
   z^(l) = Θ^(l-1) × a^(l-1)
   ```

2. **Активация**: к результату применяется функция активации (сигмоида)
   ```
   a^(l) = g(z^(l))
   ```

### Преобразование меток

Для многоклассовой классификации метки преобразуются в формат one-hot encoding. Вместо числового значения (например, 5) используется бинарный вектор, где единица стоит на позиции соответствующего класса: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].

### Реализация прямого прохода

Входные данные дополняются столбцом единиц для учета смещения. Затем последовательно вычисляются активации скрытого и выходного слоев. Результатом является вектор из 10 вероятностей, каждая из которых соответствует вероятности принадлежности входного изображения к соответствующему классу.

## Функция стоимости

### Логарифмическая функция потерь

Для измерения качества предсказаний используется логарифмическая функция потерь (log loss). Эта функция обладает важным свойством: она сильно штрафует уверенные неправильные предсказания, что способствует более точному обучению модели.

Математически функция стоимости без регуляризации записывается как:

```
J(θ) = (1/m) × Σ[i=1 to m] Σ[k=1 to K] [
    -y_k^(i) × log(h_θ(x^(i))_k) - (1 - y_k^(i)) × log(1 - h_θ(x^(i))_k)
]
```

где m — количество примеров, K — количество классов, y_k^(i) — истинная метка, h_θ(x^(i))_k — предсказание сети.

### Интерпретация функции стоимости

Когда предсказание идеально совпадает с истиной, функция стоимости равна нулю. При полном несовпадении стоимость стремится к бесконечности. Это свойство делает функцию стоимости чувствительной к ошибкам и эффективной для обучения.

## Алгоритм обратного распространения ошибки: детальный анализ

### Исторический контекст и значение

Алгоритм обратного распространения ошибки (backpropagation) был разработан в 1980-х годах и стал прорывом в области машинного обучения. До его появления обучение многослойных нейронных сетей было практически невозможным. Backpropagation решил эту проблему, предоставив эффективный способ вычисления градиентов функции стоимости по отношению ко всем весам сети.

### Концептуальное понимание

Обратное распространение ошибки представляет собой метод вычисления градиентов функции стоимости путем распространения ошибки от выходного слоя обратно к входному. Ключевая идея заключается в том, что каждый вес вносит определенный вклад в общую ошибку сети, и этот вклад можно вычислить, используя цепное правило дифференциального исчисления.

### Интуитивное объяснение через аналогию

Представим нейронную сеть как систему взаимосвязанных рычагов. Когда мы получаем неправильный результат:
1. **Прямой проход** — мы нажимаем на рычаги в определенной последовательности и получаем результат
2. **Вычисление ошибки** — мы видим, насколько результат отличается от желаемого
3. **Обратное распространение** — мы идем назад по системе рычагов и определяем, какие из них нужно подкрутить и насколько
4. **Обновление** — мы корректируем положение рычагов
5. **Повторение** — процесс повторяется до достижения желаемого результата

### Математические основы: цепное правило

Алгоритм обратного распространения основан на фундаментальном принципе математического анализа — цепном правиле (chain rule). Если функция f зависит от переменной x через промежуточную переменную y, то производная вычисляется как:

```
df/dx = (df/dy) × (dy/dx)
```

В контексте нейронных сетей это означает, что градиент функции стоимости по отношению к весам скрытого слоя можно вычислить, зная градиенты выходного слоя и производные функций активации.

### Детальный пошаговый алгоритм

#### Шаг 1: Прямой проход для каждого примера

Для каждого обучающего примера выполняется прямой проход через сеть:

```
Входное изображение (400 пикселей)
    ↓
[Добавление bias: 401 вход]
    ↓
Скрытый слой: z² = Θ¹ × a¹
    ↓
[Применение сигмоиды: a² = g(z²)]
    ↓
[Добавление bias: 26 активаций]
    ↓
Выходной слой: z³ = Θ² × a²
    ↓
[Применение сигмоиды: a³ = g(z³)]
    ↓
Вектор вероятностей (10 значений)
```

**Пример:** Для изображения цифры "5" сеть может выдать:
```
[0.02, 0.01, 0.03, 0.01, 0.85, 0.02, 0.01, 0.02, 0.01, 0.02]
```
Это означает, что сеть с вероятностью 85% считает, что это цифра 5.

#### Шаг 2: Вычисление ошибки выходного слоя

Ошибка для выходного слоя вычисляется как разность между предсказанием и истинным значением:

```
δ³ = a³ - y
```

где:
- `a³` — вектор предсказаний сети (10 значений)
- `y` — вектор истинных меток в формате one-hot encoding (10 значений)

**Конкретный пример:**
- Предсказание для цифры 5: `[0.02, 0.01, 0.03, 0.01, 0.85, 0.02, 0.01, 0.02, 0.01, 0.02]`
- Истинная метка: `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`
- Ошибка: `[0.02, 0.01, 0.03, 0.01, -0.15, 0.02, 0.01, 0.02, 0.01, 0.02]`

Отрицательное значение -0.15 для позиции 5 означает, что сеть недооценила вероятность этого класса. Положительные значения для других позиций означают, что сеть переоценила вероятность неправильных классов.

#### Шаг 3: Распространение ошибки на скрытый слой

Это самый важный и сложный шаг алгоритма. Ошибка распространяется назад через веса с учетом производной функции активации:

```
δ² = (Θ²)ᵀ × δ³ ⊙ g'(z²)
```

где:
- `(Θ²)ᵀ` — транспонированная матрица весов выходного слоя
- `δ³` — ошибка выходного слоя
- `⊙` — поэлементное умножение (Hadamard product)
- `g'(z²)` — производная сигмоиды для скрытого слоя

**Почему нужна производная сигмоиды?**

Производная сигмоиды показывает, насколько чувствительна активация к изменению входного сигнала. Если производная мала (активация близка к 0 или 1), то градиент будет мал, и вес будет обновляться медленно. Это явление называется "затуханием градиентов" (vanishing gradient problem).

**Математическое обоснование:**

Производная сигмоиды `g(z) = 1/(1 + e^(-z))` равна:
```
g'(z) = g(z) × (1 - g(z))
```

Максимальное значение производной (0.25) достигается при z = 0, когда g(z) = 0.5. При больших по модулю значениях z производная стремится к нулю, что может замедлить обучение.

**Пример вычисления:**

Если ошибка выходного слоя `δ³ = [-0.15, 0.02, ...]`, а вес между скрытым нейроном j и выходным нейроном k равен `Θ²[k,j] = 0.5`, то вклад этого веса в ошибку скрытого нейрона j будет пропорционален `-0.15 × 0.5 = -0.075`.

#### Шаг 4: Накопление градиентов

Для каждого веса вычисляется его вклад в общую ошибку. Градиент веса показывает, насколько нужно изменить этот вес, чтобы уменьшить ошибку.

Градиенты вычисляются через внешнее произведение:

```
Δ¹ = Δ¹ + δ² × (a¹)ᵀ
Δ² = Δ² + δ³ × (a²)ᵀ
```

**Интуитивное объяснение:**

Если ошибка нейрона большая (`δ²[j]` большое) и входной сигнал тоже большой (`a¹[k]` большое), то вес между ними `Θ¹[j,k]` сильно влияет на ошибку и должен быть значительно изменен.

**Пример:**
- Ошибка скрытого нейрона: `δ²[10] = 0.3` (большая ошибка)
- Активация входного пикселя: `a¹[150] = 0.9` (яркий пиксель)
- Вклад в градиент: `0.3 × 0.9 = 0.27` (большой градиент)

Это означает, что вес `Θ¹[10, 150]` нужно значительно изменить.

#### Шаг 5: Усреднение градиентов

После обработки всех примеров обучающей выборки градиенты усредняются:

```
Θ¹_grad = (1/m) × Δ¹
Θ²_grad = (1/m) × Δ²
```

где `m` — количество примеров в обучающей выборке.

Усреднение необходимо, чтобы градиенты отражали общую тенденцию по всем примерам, а не зависели от конкретного примера.

### Обновление весов: градиентный спуск

После вычисления градиентов оптимизатор обновляет веса. В данной работе используется алгоритм сопряженных градиентов (fmincg), который является более эффективным, чем простой градиентный спуск.

**Базовая формула градиентного спуска:**

```
Θ_new = Θ_old - α × ∇J(Θ)
```

где:
- `α` — скорость обучения (learning rate)
- `∇J(Θ)` — градиент функции стоимости

**Алгоритм сопряженных градиентов** использует более сложную стратегию, которая учитывает направление предыдущих обновлений, что позволяет быстрее достигать минимума функции стоимости.

### Вычислительная сложность

Обратное распространение имеет линейную сложность O(n × m), где:
- `n` — количество весов в сети
- `m` — количество примеров в обучающей выборке

Это делает алгоритм эффективным даже для больших сетей. Для сети с 30 нейронами в скрытом слое:
- Количество весов: (30 × 401) + (10 × 31) = 12,030 + 310 = 12,340
- Для 5000 примеров: 12,340 × 5000 = 61,700,000 операций за одну итерацию

Несмотря на большое число операций, современные вычислительные системы справляются с этой задачей за приемлемое время.

### Проблемы и решения

#### Проблема затухающих градиентов

При глубоких сетях градиенты могут экспоненциально затухать при распространении назад, что делает обучение первых слоев практически невозможным. В данной работе используется сеть с одним скрытым слоем, поэтому эта проблема не проявляется, но она критична для глубоких сетей.

**Решение:** Использование других функций активации (ReLU, Leaky ReLU) или специальных техник инициализации.

#### Проблема взрывающихся градиентов

Противоположная проблема — градиенты могут экспоненциально расти, что приводит к нестабильности обучения.

**Решение:** Ограничение градиентов (gradient clipping) или нормализация.

### Визуализация процесса обратного распространения

Процесс можно визуализировать как волну ошибки, распространяющуюся от выхода к входу:

```
Выходной слой:  [ошибка = 0.15]  ← начальная точка
                    ↓
Скрытый слой:   [ошибка = 0.08]  ← распространяется через веса
                    ↓
Входной слой:   [градиенты]      ← финальный результат
```

Каждый слой "передает" ошибку предыдущему, масштабируя ее в соответствии с весами и производными функций активации.

### Практические аспекты реализации

#### Векторизация вычислений

Для эффективной реализации все вычисления векторизованы. Вместо цикла по каждому примеру отдельно, все примеры обрабатываются одновременно в матричных операциях. Это значительно ускоряет вычисления благодаря оптимизациям линейной алгебры.

#### Проверка правильности реализации

Для проверки корректности реализации backpropagation используется численное вычисление градиентов:

```
∇J(θᵢ) ≈ [J(θ + ε·eᵢ) - J(θ - ε·eᵢ)] / (2ε)
```

где `eᵢ` — единичный вектор в направлении i-й координаты, а `ε` — малое число (обычно 10⁻⁴).

Если аналитические градиенты (из backpropagation) совпадают с численными с точностью до 4-5 значащих цифр, реализация считается корректной.

### Влияние на результаты эксперимента

Без алгоритма обратного распространения эксперимент по определению оптимального размера скрытого слоя был бы невозможен. Backpropagation позволяет:

1. **Эффективно обучать сети различной сложности**: от 5 до 50 нейронов в скрытом слое
2. **Вычислять градиенты для всех весов одновременно**: что делает обучение практичным
3. **Сравнивать разные архитектуры**: что необходимо для выбора оптимального размера

Результаты эксперимента показывают, что backpropagation успешно обучает все протестированные архитектуры, позволяя выявить оптимальный размер скрытого слоя (40 нейронов) с точностью 94.50% на валидационной выборке.

## Регуляризация

### Проблема переобучения

При обучении нейронных сетей возникает риск переобучения (overfitting) — ситуации, когда модель слишком хорошо запоминает обучающие данные, но плохо обобщается на новые примеры. Это особенно актуально для сетей с большим количеством параметров.

### Метод регуляризации

Регуляризация решает эту проблему путем добавления штрафа за большие значения весов. Регуляризованная функция стоимости имеет вид:

```
J_reg(θ) = J(θ) + (λ/(2m)) × [Σ(Θ₁²) + Σ(Θ₂²)]
```

где λ — параметр регуляризации, контролирующий силу штрафа.

### Исключение смещения из регуляризации

Важно отметить, что смещения (bias) не включаются в регуляризацию. Это связано с тем, что смещения не связаны напрямую с входными признаками и их регуляризация может ухудшить способность модели к обучению.

### Влияние параметра λ

Выбор параметра регуляризации λ является важным решением:
- Слишком малый λ: модель может переобучиться
- Слишком большой λ: модель может недообучиться
- Оптимальный λ: баланс между точностью и обобщающей способностью

В данном исследовании использовалось значение λ = 1, которое показало хорошие результаты.

## Экспериментальное исследование оптимального размера скрытого слоя

### Постановка задачи

Одной из ключевых задач при проектировании нейронной сети является выбор оптимального количества нейронов в скрытом слое. Слишком малое количество нейронов приводит к недообучению, слишком большое — к переобучению.

### Методология эксперимента

Для определения оптимального размера скрытого слоя был проведен систематический эксперимент. Обучающая выборка из 5000 примеров была разделена на обучающую (80%) и валидационную (20%) части. Для каждого размера скрытого слоя из набора [5, 10, 15, 20, 25, 30, 40, 50] была обучена отдельная сеть с одинаковыми параметрами (λ = 1, 50 итераций).

### Результаты эксперимента

Эксперимент показал четкую зависимость точности от размера скрытого слоя:

| Размер скрытого слоя | Точность (обучение) | Точность (валидация) | Вывод |
|---------------------|---------------------|---------------------|-------|
| 5                   | 73.38%              | 69.20%              | Недообучение |
| 10                  | 94.23%              | 91.40%              | Хороший результат |
| 15                  | 94.50%              | 92.30%              | Улучшение |
| 20                  | 95.12%              | 92.40%              | Продолжающийся рост |
| 25                  | 95.60%              | 93.90%              | Хорошая точность |
| 30                  | 95.47%              | 93.40%              | Небольшое снижение |
| **40**              | **96.20%**          | **94.50%**          | **Оптимальный размер** |
| 50                  | 96.33%              | 93.90%              | Переобучение |

### Анализ результатов

Результаты демонстрируют классическую картину зависимости точности от сложности модели:

1. **Недообучение (5 нейронов)**: Низкая точность на обеих выборках указывает на недостаточную сложность модели для решения задачи.

2. **Быстрый рост (10-25 нейронов)**: Увеличение размера скрытого слоя приводит к значительному улучшению точности, что свидетельствует о том, что модель получает достаточно выразительности для выучивания паттернов.

3. **Оптимальная точка (40 нейронов)**: Максимальная точность на валидационной выборке (94.50%) при минимальном переобучении (разница между обучением и валидацией составляет всего 1.7%).

4. **Переобучение (50 нейронов)**: Падение точности на валидации при росте точности на обучении указывает на то, что модель начинает запоминать шум в данных.

### Роль обратного распространения в эксперименте

Без алгоритма обратного распространения ошибки эксперимент был бы невозможен. Backpropagation позволяет:
- Эффективно вычислять градиенты для всех весов одновременно
- Обновлять веса в правильном направлении
- Обучать сети различной сложности

Именно благодаря backpropagation удалось обнаружить, что 40 нейронов обеспечивают оптимальный баланс между точностью и обобщающей способностью.

## Визуализация обученных весов

### Интерпретация весов скрытого слоя

После обучения сети веса скрытого слоя можно визуализировать как изображения 20×20 пикселей. Каждое такое изображение показывает, какие паттерны активируют соответствующий нейрон.

### Наблюдаемые паттерны

Визуализация обученных весов выявляет интересные закономерности:
- Некоторые нейроны специализируются на вертикальных линиях
- Другие реагируют на горизонтальные линии
- Третьи распознают диагонали и углы
- Некоторые нейроны извлекают более сложные комбинации признаков

Эти паттерны напоминают части цифр, что подтверждает способность сети автоматически извлекать релевантные признаки из данных.

## Практические рекомендации

### Выбор размера скрытого слоя

На основе проведенного исследования можно сформулировать следующие рекомендации:

1. Начинать с малых значений и постепенно увеличивать
2. Использовать валидационную выборку для оценки обобщающей способности
3. Обращать внимание на разницу между точностью на обучении и валидации
4. Учитывать вычислительную сложность при выборе размера

### Инициализация весов

Правильная инициализация весов критически важна для успешного обучения. Веса должны быть:
- Случайными (для разбиения симметрии)
- Малыми по абсолютной величине (для предотвращения насыщения сигмоиды)
- Равномерно распределенными в диапазоне [-0.12, 0.12]

### Количество итераций обучения

Количество итераций обучения влияет на качество модели:
- Слишком мало итераций: модель не успевает обучиться
- Слишком много итераций: риск переобучения
- Оптимальное значение: баланс между временем обучения и качеством

В данном исследовании использовалось 50 итераций, что обеспечило хороший баланс.

## Заключение

В данной работе была реализована и исследована нейронная сеть для распознавания рукописных цифр. Ключевые достижения:

1. **Реализация алгоритма обратного распространения**: Успешно реализован и протестирован алгоритм backpropagation, который является основой обучения нейронных сетей.

2. **Определение оптимальной архитектуры**: Экспериментально установлено, что скрытый слой из 40 нейронов обеспечивает оптимальный баланс между точностью (94.50% на валидации) и обобщающей способностью.

3. **Практическое применение регуляризации**: Продемонстрировано влияние регуляризации на предотвращение переобучения.

4. **Визуализация обученных признаков**: Показано, что сеть автоматически извлекает релевантные признаки из данных.

### Значение для практики

Результаты исследования имеют практическое значение для разработки систем распознавания образов. Показано, что даже относительно простая архитектура (три слоя) способна достигать высокой точности при правильном выборе параметров.

### Направления дальнейших исследований

Возможные направления для дальнейшего развития:
- Исследование влияния различных функций активации
- Эксперименты с различными алгоритмами оптимизации
- Изучение влияния dropout и других техник регуляризации
- Применение к более сложным задачам классификации

---

*Работа выполнена в рамках изучения алгоритмов машинного обучения и нейронных сетей.*

