ЛАБОРАТОРНАЯ РАБОТА 4: K-MEANS КЛАСТЕРИЗАЦИЯ
=============================================

═══════════════════════════════════════════════════════════════════════
О ЧЁМ ЭТА ЛАБОРАТОРНАЯ? (КРАТКОЕ ВВЕДЕНИЕ)
═══════════════════════════════════════════════════════════════════════

ЗАДАЧА:
-------
У нас есть данные о 52 штатах США:
- Средний доход домохозяйств (income)
- Среднее потребление электроэнергии (elec)

Нужно СГРУППИРОВАТЬ штаты по этим показателям, чтобы понять:
- Какие штаты похожи друг на друга?
- Сколько групп (кластеров) существует?
- Есть ли выбросы (аномальные штаты)?


ЧТО МЫ ДЕЛАЕМ:
--------------
1. Загружаем данные из файла income_elec_state.Rdata
2. Кластеризуем штаты методом K-means с разными k
3. Определяем оптимальное количество кластеров (методы локтя и силуэта)
4. Применяем log-трансформацию и смотрим как меняются результаты
5. Находим и удаляем выбросы (Puerto Rico)
6. Рисуем карту США, раскрашенную по кластерам
7. Сравниваем с иерархической кластеризацией


ЗАЧЕМ ЭТО НУЖНО В РЕАЛЬНОЙ ЖИЗНИ?
---------------------------------
- Маркетинг: сегментация клиентов по поведению
- Медицина: группировка пациентов по симптомам
- Финансы: классификация компаний по показателям
- География: выделение регионов с похожими характеристиками
- Биология: группировка генов/белков по экспрессии


КАКИЕ МЕТОДЫ ИЗУЧАЕМ:
---------------------
1. K-MEANS — быстрый метод, нужно указать k заранее
2. ИЕРАРХИЧЕСКАЯ КЛАСТЕРИЗАЦИЯ — строит дерево, k выбираем потом
3. МЕТОД ЛОКТЯ — как выбрать оптимальное k (ищем "изгиб" на графике WSS)
4. МЕТОД СИЛУЭТА — как оценить качество кластеризации:
   
   Силуэт показывает насколько точка "своя" в кластере:
   - Считаем a = среднее расстояние до СВОИХ соседей
   - Считаем b = среднее расстояние до БЛИЖАЙШЕГО чужого кластера
   - Силуэт = (b - a) / max(a, b)
   
   Значения:
   +1 = точка идеально в своём кластере (далеко от чужих)
    0 = точка на границе (одинаково близка к своим и чужим)
   -1 = точка в чужом кластере (ближе к чужим чем к своим)
   
   Выбираем k с максимальным СРЕДНИМ силуэтом по всем точкам


РЕЗУЛЬТАТ ЛАБЫ:
---------------
- Понимание как работает кластеризация
- Умение выбирать оптимальное k
- Знание когда использовать log-трансформацию
- Навык визуализации результатов (графики, карты)
- Понимание разницы между K-means и иерархической кластеризацией


ФАЙЛЫ КОТОРЫЕ СОЗДАЁМ:
----------------------
- Графики K-means с k=10 и k=3 (оптимальное)
- Графики методов локтя и силуэта
- Карта США с раскраской по кластерам
- Дендрограммы иерархической кластеризации (4 метода связи)


═══════════════════════════════════════════════════════════════════════
БАЗОВЫЕ ПОНЯТИЯ (могут спросить!)
═══════════════════════════════════════════════════════════════════════

ЧТО ТАКОЕ КЛАСТЕРИЗАЦИЯ?
------------------------
Кластеризация — это метод машинного обучения БЕЗ учителя (unsupervised).
Мы не знаем заранее, к какому классу относятся данные — алгоритм сам 
находит группы похожих объектов.

Пример: группировка клиентов магазина по поведению покупок.


ЧТО ТАКОЕ K-MEANS?
------------------
K-means — алгоритм кластеризации, который:
1. Случайно выбирает k начальных центроидов (центров кластеров)
2. Присваивает каждую точку ближайшему центроиду
3. Пересчитывает центроиды как среднее точек в кластере
4. Повторяет шаги 2-3 пока центроиды не перестанут двигаться

ЦЕНТРОИД — это центр кластера (среднее значение всех точек в нём).


ЗАЧЕМ НУЖЕН ПАРАМЕТР k?
-----------------------
k — количество кластеров, которое МЫ задаём заранее.
Проблема: как выбрать правильное k?

Методы выбора k:
1. Метод локтя (Elbow) — смотрим где график "сгибается"
2. Метод силуэта (Silhouette) — ищем максимум качества кластеризации


ЧТО ТАКОЕ WSS (Within-cluster Sum of Squares)?
----------------------------------------------
WSS — сумма квадратов расстояний от точек до центроидов их кластеров.
Чем меньше WSS — тем компактнее кластеры (лучше).

НО! При увеличении k WSS всегда уменьшается (при k=n каждая точка — 
отдельный кластер, WSS=0). Поэтому ищем "локоть" — точку где уменьшение 
WSS замедляется.


ЗАЧЕМ НУЖНА ЛОГАРИФМИЗАЦИЯ (LOG10)?
-----------------------------------
Проблема: доход может быть от $20,000 до $100,000, а электричество 
от $500 до $1500. Доход "перевешивает" при расчёте расстояний.

Log-трансформация:
- Сжимает большие значения (100000 → 5)
- Расширяет малые значения (1000 → 3)
- Делает данные более симметричными
- Уравнивает влияние переменных

Пример:
  До log:    $20,000 и $100,000 — разница 80,000
  После log: 4.3 и 5.0 — разница 0.7

Когда использовать log:
- Данные скошены вправо (много малых, мало больших значений)
- Переменные имеют разные масштабы
- Есть экспоненциальный рост


ЧТО ТАКОЕ set.seed()? (ВАЖНО!)
------------------------------
set.seed(123) — фиксирует генератор ПСЕВДОСЛУЧАЙНЫХ чисел.

КАК РАБОТАЕТ:
Компьютер не умеет генерировать "настоящие" случайные числа.
Он использует формулу, которая по начальному числу (seed) выдаёт
последовательность чисел, ВЫГЛЯДЯЩИХ случайными.

Один и тот же seed → одна и та же последовательность → одинаковые результаты!

ПРИМЕР:
  set.seed(123)
  runif(3)  # выдаст: 0.2875775, 0.7883051, 0.4089769
  
  set.seed(123)  # сбросили seed
  runif(3)  # выдаст ТЕ ЖЕ: 0.2875775, 0.7883051, 0.4089769
  
  set.seed(456)  # другой seed
  runif(3)  # выдаст ДРУГИЕ: 0.2261391, 0.1235464, 0.1836819

ЧТО БУДЕТ ЕСЛИ ПОМЕНЯТЬ 123 НА ДРУГОЕ ЧИСЛО?
--------------------------------------------
- set.seed(123) и set.seed(456) дадут РАЗНЫЕ результаты кластеризации
- НО! При одном и том же seed результаты ВСЕГДА одинаковые
- Число 123 — просто популярный выбор, можно использовать любое

Пример с k-means:
  set.seed(123)
  km1 <- kmeans(data, 3)  # результат А
  
  set.seed(123)
  km2 <- kmeans(data, 3)  # результат А (такой же!)
  
  set.seed(999)
  km3 <- kmeans(data, 3)  # результат Б (может отличаться!)

ЗАЧЕМ ЭТО НУЖНО?
----------------
1. ВОСПРОИЗВОДИМОСТЬ — другой человек с тем же seed получит те же результаты
2. ОТЛАДКА — можно повторить эксперимент
3. НАУЧНАЯ РАБОТА — результаты можно проверить

БЕЗ set.seed():
- Каждый запуск k-means даст разные кластеры
- Нельзя повторить эксперимент
- Преподаватель не сможет проверить твои результаты!

ВАЖНО: set.seed() нужно вызывать ПЕРЕД каждым случайным действием,
       а не один раз в начале программы (если хотите точное повторение)


ЧТО ТАКОЕ nstart?
-----------------
nstart=25 означает: запустить k-means 25 раз с разными начальными 
центроидами и выбрать лучший результат (с минимальным WSS).

Зачем: k-means может "застрять" в локальном оптимуме. Много запусков 
увеличивает шанс найти глобальный оптимум.


ЧТО ТАКОЕ ВЫБРОС (OUTLIER)?
---------------------------
Выброс — точка, сильно отличающаяся от остальных.
В нашем случае: Puerto Rico (PR) — очень низкий доход.

Проблема выбросов:
- Искажают центроиды
- Могут создавать отдельные кластеры из одной точки
- Влияют на выбор оптимального k

Как находить: метод IQR (межквартильный размах)
- Q1 = 25-й перцентиль
- Q3 = 75-й перцентиль  
- IQR = Q3 - Q1
- Выброс: < Q1 - 1.5*IQR или > Q3 + 1.5*IQR


═══════════════════════════════════════════════════════════════════════
ИЕРАРХИЧЕСКАЯ КЛАСТЕРИЗАЦИЯ (ПОДРОБНО)
═══════════════════════════════════════════════════════════════════════

ЧТО ЭТО ТАКОЕ?
--------------
Иерархическая кластеризация — метод, который строит ДЕРЕВО (дендрограмму),
показывающее иерархию объединения объектов в кластеры.

В отличие от k-means:
- НЕ требует указывать k заранее
- Показывает ВСЮ структуру данных
- Можно "резать" на любом уровне


ДВА ПОДХОДА:
------------
1. АГЛОМЕРАТИВНЫЙ (снизу вверх) — мы используем этот:
   - Начинаем: каждая точка = отдельный кластер
   - Шаг: объединяем два ближайших кластера
   - Повторяем пока не останется 1 кластер

2. ДИВИЗИВНЫЙ (сверху вниз):
   - Начинаем: все точки в одном кластере
   - Шаг: делим кластер на два
   - Повторяем пока каждая точка не станет отдельным кластером


КАК ЧИТАТЬ ДЕНДРОГРАММУ?
------------------------
         |
    _____|_____
   |           |
 __|__       __|__
|     |     |     |
A     B     C     D

- Внизу: названия объектов (штаты)
- Высота (ось Y): расстояние при объединении
- Чем выше объединение — тем "дальше" друг от друга кластеры
- Горизонтальная линия на высоте h делит на кластеры

Пример чтения:
- A и B близки (объединены низко)
- C и D близки (объединены низко)  
- Группа {A,B} далека от {C,D} (объединены высоко)


КАК ОПРЕДЕЛЯЕТСЯ ПОРЯДОК ОБЪЕКТОВ В ДЕНДРОГРАММЕ?
-------------------------------------------------

ВАЖНО ПОНИМАТЬ: Порядок слева-направо НЕ означает близость!

Алгоритм построения:
1. Сначала считаем матрицу расстояний между ВСЕМИ парами объектов
   (функция dist() в R)

2. Находим два БЛИЖАЙШИХ объекта → объединяем в первый кластер
   
3. Пересчитываем расстояния (по методу linkage: single/complete/average/ward)

4. Повторяем: находим ближайшие объекты/кластеры → объединяем

5. Продолжаем пока всё не объединится в один кластер

ПОРЯДОК ЛИСТЬЕВ (объектов внизу):
- Определяется порядком объединения
- Можно "переворачивать" ветки без изменения смысла!
- R старается минимизировать пересечения линий

Пример:
         |                    |
    _____|_____          _____|_____
   |           |        |           |
 __|__       __|__    __|__       __|__
|     |     |     |  |     |     |     |
A     B     C     D  B     A     D     C
ОБЕ дендрограммы ОДИНАКОВЫ по смыслу!
(A-B вместе, C-D вместе — это главное)


КАК ВЫБРАТЬ КОЛИЧЕСТВО КЛАСТЕРОВ ПО ДЕНДРОГРАММЕ?
-------------------------------------------------

Способ 1: ГОРИЗОНТАЛЬНАЯ ЛИНИЯ (ВИЗУАЛЬНЫЙ!)
---------------------------------------------
Проводим горизонтальную линию на нужной высоте.
Сколько вертикальных линий пересекает — столько кластеров!

ПРИМЕР С 5 КЛАСТЕРАМИ:

Height
  |
100|         _____|_____
   |        |           |
 80|      __|__         |
   |     |     |        |
 60|     |   __|__    __|__
   |     |  |     |  |     |
 40|   __|  |     |  |     |
   |  |   | |     |  |     |
 20|  |   | |     |  |     |
   |  |   | |     |  |     |
  0|  A   B C     D  E     F
   +--------------------------------

Хотим 5 кластеров? Проводим линию между высотами 40 и 60:

Height
  |
100|         _____|_____
   |        |           |
 80|      __|__         |
   |     |     |        |
 60|     |   __|__    __|__
   |     |  |     |  |     |
 40|---|-|--|-----|--|-----|---  ← линия на высоте ~50
   |  |   | |     |  |     |
   |  A   B C     D  E     F

Считаем пересечения: 5 вертикальных линий → 5 кластеров!
Кластеры: {A}, {B}, {C,D}, {E}, {F}

Хотим 3 кластера? Проводим линию выше (на высоте ~70):
Пересечений: 3 → кластеры: {A,B}, {C,D}, {E,F}

Хотим 2 кластера? Проводим линию на высоте ~90:
Пересечений: 2 → кластеры: {A,B,C,D}, {E,F}


ПРАВИЛО: Чем ВЫШЕ линия — тем МЕНЬШЕ кластеров!
           Чем НИЖЕ линия — тем БОЛЬШЕ кластеров!


rect.hclust() В R — РИСУЕТ ПРЯМОУГОЛЬНИКИ:
------------------------------------------
rect.hclust(hc, k = 5, border = "red")

Эта функция визуально показывает 5 кластеров красными прямоугольниками.
Именно это мы видим на наших дендрограммах!


Способ 2: ИЩЕМ БОЛЬШОЙ "ПРЫЖОК"
-------------------------------
Смотрим где большой разрыв по высоте между объединениями.
Режем ПЕРЕД этим прыжком.

Если объединения идут: 10, 15, 20, 80, 100
                                   ↑
                              большой прыжок!
Значит оптимально 2 кластера (до объединения на 80).


Способ 3: ФУНКЦИЯ cutree() в R
------------------------------
cutree(hc, k = 5)  — разрезать на 5 кластеров
cutree(hc, h = 50) — разрезать на высоте 50

Пример:
  hc <- hclust(dist(data), method = "ward.D2")
  clusters <- cutree(hc, k = 5)
  # clusters = вектор: какой объект в каком кластере


ПРЕИМУЩЕСТВА ДЕНДРОГРАММЫ:
--------------------------
1. ВИДНО ВСЮ СТРУКТУРУ — не надо заранее выбирать k
2. МОЖНО ЭКСПЕРИМЕНТИРОВАТЬ — легко попробовать разные k
3. ВИДНЫ ВЫБРОСЫ — они объединяются последними (высоко)
4. ВИДНА ИЕРАРХИЯ — какие группы внутри каких
5. ВОСПРОИЗВОДИМОСТЬ — всегда один результат (нет случайности)

НЕДОСТАТКИ ДЕНДРОГРАММЫ:
------------------------
1. НЕЧИТАЕМА для >100 объектов (всё сливается)
2. МЕДЛЕННО для больших данных O(n²) или O(n³)
3. НЕЛЬЗЯ ИСПРАВИТЬ — если объекты объединены, не разъединить
4. ЧУВСТВИТЕЛЬНА к методу linkage


МАТРИЦА РАССТОЯНИЙ — начало всего:
----------------------------------
Пример для 4 штатов:

        AL    AZ    AR    CA
   AL    0    50   120    80
   AZ   50     0    90    40
   AR  120    90     0   100
   CA   80    40   100     0

Читаем: расстояние AL-AZ = 50, AL-AR = 120 и т.д.
Минимум = 40 (AZ-CA) → первыми объединяем AZ и CA


МЕТОДЫ СВЯЗИ (LINKAGE) — ПОДРОБНО
---------------------------------
Вопрос: как измерить расстояние между КЛАСТЕРАМИ (не точками)?

1. SINGLE LINKAGE (одиночная связь)
   --------------------------------
   Расстояние = МИНИМУМ между любыми двумя точками из разных кластеров
   
   Кластер A: {a1, a2, a3}
   Кластер B: {b1, b2}
   d(A,B) = min(d(a1,b1), d(a1,b2), d(a2,b1), d(a2,b2), d(a3,b1), d(a3,b2))
   
   Плюсы:
   + Находит кластеры произвольной формы (вытянутые, изогнутые)
   + Хорош для обнаружения "мостиков" между группами
   
   Минусы:
   - ЭФФЕКТ ЦЕПОЧКИ (chaining) — главная проблема!
     Один большой кластер "засасывает" всё, остаются одиночки
   - На нашей дендрограмме видно: PR отдельно, остальные в одном
   
   Когда использовать:
   - Когда ожидаются вытянутые кластеры
   - Для поиска "мостов" в данных


2. COMPLETE LINKAGE (полная связь)
   -------------------------------
   Расстояние = МАКСИМУМ между любыми двумя точками из разных кластеров
   
   d(A,B) = max(d(ai, bj)) для всех i,j
   
   Плюсы:
   + Компактные, сферические кластеры
   + Нет эффекта цепочки
   + Более сбалансированные размеры кластеров
   
   Минусы:
   - Чувствителен к выбросам (один далёкий объект влияет сильно)
   - Может разбивать вытянутые кластеры
   
   Когда использовать:
   - Когда ожидаются компактные, округлые кластеры
   - Когда нет сильных выбросов


3. AVERAGE LINKAGE (средняя связь)
   -------------------------------
   Расстояние = СРЕДНЕЕ между всеми парами точек из разных кластеров
   
   d(A,B) = (1/(|A|*|B|)) * Σ d(ai, bj)
   
   Плюсы:
   + Компромисс между single и complete
   + Менее чувствителен к выбросам чем complete
   + Менее подвержен цепочке чем single
   
   Минусы:
   - "Средний" результат — не лучший ни в чём
   
   Когда использовать:
   - Когда не уверены какой метод выбрать
   - Как "безопасный" вариант


4. WARD (метод Уорда) — ward.D2
   ----------------------------
   Минимизирует РОСТ внутрикластерной дисперсии при объединении.
   
   При объединении A и B смотрим: насколько увеличится суммарный
   разброс точек относительно центроидов?
   
   Плюсы:
   + Создаёт компактные, сферические кластеры ОДИНАКОВОГО размера
   + Результаты очень похожи на K-means!
   + Минимизирует ту же функцию что и k-means (WSS)
   
   Минусы:
   - Чувствителен к выбросам
   - "Заставляет" кластеры быть примерно равными
   
   Когда использовать:
   - Когда хотите результат похожий на k-means
   - Когда ожидаете кластеры примерно равного размера
   - РЕКОМЕНДУЕТСЯ как метод по умолчанию


СРАВНЕНИЕ МЕТОДОВ НА НАШИХ ДАННЫХ (k=4):
----------------------------------------
Метод        | Размеры кластеров | Особенности
-------------|-------------------|----------------------------------
SINGLE       | 1, 1, 1, 49       | Эффект цепочки! PR + ещё 2 отдельно
COMPLETE     | 13, 15, 12, 12    | Сбалансированно
AVERAGE      | 10, 18, 12, 12    | Близко к complete
WARD         | 14, 13, 13, 12    | Самые равные размеры

ВЫВОД: Ward даёт наиболее интерпретируемые результаты.


ФУНКЦИЯ cutree() — КАК "РЕЗАТЬ" ДЕНДРОГРАММУ
--------------------------------------------
cutree(hc, k = 4)  — разрезать на 4 кластера
cutree(hc, h = 50) — разрезать на высоте 50

Пример:
hc <- hclust(dist(data), method = "ward.D2")
clusters <- cutree(hc, k = 4)
# clusters — вектор принадлежности к кластерам


ОГРАНИЧЕНИЯ K-MEANS (ВАЖНО ЗНАТЬ!)
----------------------------------

1. ТОЛЬКО СФЕРИЧЕСКИЕ КЛАСТЕРЫ (ПОДРОБНО!)
   ----------------------------------------
   
   K-means ХОРОШО работает (сферические/круглые кластеры):
   
       Кластер 1        Кластер 2
         * * *            * * *
        * * * *          * * * *
        *  ●  *          *  ●  *     ● = центроид
        * * * *          * * * *
         * * *            * * *
   
   Точки равномерно вокруг центра → K-means легко находит центроид.
   
   
   K-means ПЛОХО работает:
   
   a) ВЫТЯНУТЫЕ КЛАСТЕРЫ (эллипсы):
   
        * * * * * * * * * * * *
         *  *  *  ●  *  *  *  *      ← один кластер
        * * * * * * * * * * * *
   
      Проблема: центроид в середине, но крайние точки далеко!
      K-means может разрезать эллипс пополам.
   
   
   b) "БАНАНЫ" (изогнутые):
   
            * * *
          * * *
        * * *
      * * *                    * * *
        * * *                * * *
          * * *            * * *
            * * *        * * *
   
      Проблема: центроид будет между двумя "бананами"!
      K-means объединит концы разных бананов.
   
   
   c) КОЛЬЦА / КОНЦЕНТРИЧЕСКИЕ КРУГИ:
   
              * * * * *
           *           *
          *   * * * *   *
         *   *       *   *
         *   *   ●   *   *       ← внутренний круг = один кластер
         *   *       *   *          внешнее кольцо = другой кластер
          *   * * * *   *
           *           *
              * * * * *
   
      Проблема: оба кластера имеют ОДИН центр!
      K-means не может их разделить.
   
   
   d) ПОЛУМЕСЯЦЫ (moons):
   
           * * * * * *
         *             *
        *               *
                              *               *
                               *             *
                                * * * * * *
   
      Проблема: центроиды будут в "пустоте" между полумесяцами.
   
   
   ПОЧЕМУ ТАК ПРОИСХОДИТ?
   ----------------------
   K-means минимизирует СУММУ КВАДРАТОВ расстояний до центроида.
   
   Формула: Σ ||x_i - μ_k||²
   
   Это работает только когда точки РАВНОМЕРНО распределены вокруг центра!
   
   
   ЧТО ИСПОЛЬЗОВАТЬ ВМЕСТО K-MEANS?
   --------------------------------
   
   DBSCAN — кластеры по плотности:
   + Находит кластеры любой формы
   + Автоматически определяет выбросы
   - Сложнее настраивать параметры
   
   Спектральная кластеризация:
   + Работает с кольцами, полумесяцами
   + Использует граф связей
   - Медленнее K-means
   
   Gaussian Mixture Models (GMM):
   + Работает с эллипсами
   + Даёт вероятность принадлежности
   - Сложнее интерпретировать
   
   
   В НАШЕЙ ЛАБЕ: данные (доход, электричество) образуют достаточно
   "круглые" облака точек → K-means подходит!

2. КЛАСТЕРЫ ДОЛЖНЫ БЫТЬ ПРИМЕРНО РАВНОГО РАЗМЕРА
   ----------------------------------------------
   Если один кластер = 1000 точек, другой = 10 точек:
   - K-means может "украсть" точки у маленького
   - Центроид большого кластера сильнее влияет
   
   Решение: нормализация данных, другие алгоритмы

3. ЧУВСТВИТЕЛЬНОСТЬ К НАЧАЛЬНЫМ ЦЕНТРОИДАМ
   ----------------------------------------
   Разные начальные точки → разные результаты!
   Может застрять в локальном оптимуме.
   
   Решение: set.seed() + nstart=25

4. ЧУВСТВИТЕЛЬНОСТЬ К ВЫБРОСАМ
   ----------------------------
   Один выброс может сильно сдвинуть центроид.
   Puerto Rico "тянет" центроид кластера в сторону.
   
   Решение: удалить выбросы или использовать K-medoids

5. НУЖНО УКАЗЫВАТЬ k ЗАРАНЕЕ
   --------------------------
   Не всегда известно сколько кластеров.
   
   Решение: методы локтя и силуэта, или иерархическая кластеризация

6. ТОЛЬКО ЧИСЛОВЫЕ ДАННЫЕ
   -----------------------
   Нельзя напрямую кластеризовать:
   - Категории (цвет, тип)
   - Текст
   - Смешанные данные
   
   Решение: преобразование в числа, K-modes, K-prototypes

7. ВСЕ ПРИЗНАКИ РАВНОЗНАЧНЫ
   -------------------------
   Если доход в долларах (10000-100000), а возраст в годах (20-80):
   - Доход будет доминировать (больший разброс)
   
   Решение: нормализация/стандартизация данных, log-трансформация

8. ЧУВСТВИТЕЛЬНОСТЬ К МАСШТАБУ
   ----------------------------
   Разные единицы измерения → разное влияние на расстояние.
   
   Решение: стандартизация (scale() в R)


КОГДА K-MEANS РАБОТАЕТ ХОРОШО:
------------------------------
✓ Кластеры примерно сферические
✓ Кластеры примерно одного размера
✓ Нет сильных выбросов
✓ Данные числовые и нормализованные
✓ Известно (или легко определить) количество кластеров


СРАВНЕНИЕ K-MEANS И ИЕРАРХИЧЕСКОЙ
---------------------------------
Критерий          | K-MEANS              | ИЕРАРХИЧЕСКАЯ
------------------|----------------------|------------------------
Указывать k       | Да, заранее          | Нет (режем потом)
Скорость          | O(n*k*iter)          | O(n²) или O(n³)
Масштабируемость  | Хорошая              | Плохая для >10000 точек
Результат         | Только кластеры      | Дендрограмма (дерево)
Форма кластеров   | Сферические          | Зависит от linkage
Воспроизводимость | Зависит от seed      | Всегда одинаково
Чувствительность  | К начальным точкам   | К выбросам и linkage


═══════════════════════════════════════════════════════════════════════
ОТВЕТЫ НА ЗАДАНИЯ
═══════════════════════════════════════════════════════════════════════

ЗАДАНИЕ 4.1a - K-means с k=10
-----------------------------
Кластеризуем 52 штата США по:
- Средний доход домохозяйств (income)
- Среднее потребление электроэнергии (elec)

На графике:
- Прозрачные круги с цветными краями = штаты
- Звёздочки = центроиды кластеров
- Цвет = принадлежность к кластеру


ЗАДАНИЕ 4.1b - Воспроизводимость
--------------------------------
ВОПРОС: Что меняется при повторном запуске?

ОТВЕТ:
1. Принадлежность точек к кластерам
2. Позиции центроидов
3. Значение WSS

ПОЧЕМУ: Случайные начальные центроиды → разные локальные оптимумы

КАК ИСПРАВИТЬ:
1. set.seed(123) — фиксирует генератор случайных чисел
   - 123 можно заменить на любое число (456, 999, 42...)
   - Разные числа → разные результаты
   - Одинаковые числа → одинаковые результаты (воспроизводимость!)
   - Подробнее см. раздел "ЧТО ТАКОЕ set.seed()?" выше
   
2. nstart=25 — запустить k-means 25 раз, выбрать лучший результат
   - Каждый запуск с разными начальными центроидами
   - Выбирается тот, где WSS минимальный


ЗАДАНИЕ 4.1c - Оптимальное k
----------------------------

ДВА МЕТОДА ВЫБОРА k:
--------------------

1. МЕТОД ЛОКТЯ (Elbow Method):
   - Строим график WSS (внутрикластерная сумма квадратов) от k
   - Ищем "локоть" — где кривая резко изгибается
   - На нашем графике: изгиб около k=3
   - Идея: после "локтя" добавление кластеров не сильно улучшает WSS

2. МЕТОД СИЛУЭТА (Silhouette Method):
   - Silhouette score = насколько хорошо точка "вписывается" в свой кластер
   - Формула: s(i) = (b(i) - a(i)) / max(a(i), b(i))
     где a(i) = среднее расстояние до точек СВОЕГО кластера
         b(i) = среднее расстояние до точек БЛИЖАЙШЕГО чужого кластера
   - Значение от -1 до +1:
     * +1 = точка идеально в своём кластере
     *  0 = точка на границе между кластерами
     * -1 = точка в "чужом" кластере
   - Ищем k с МАКСИМАЛЬНЫМ средним силуэтом


ПОЧЕМУ СИЛУЭТ ПОКАЗАЛ k=8, А МЫ ВЗЯЛИ k=3?
------------------------------------------

На графике силуэта видно: максимум при k=8 (≈0.60)

НО мы выбрали k=3 по следующим причинам:

1. ИНТЕРПРЕТИРУЕМОСТЬ:
   - k=8 → 52/8 ≈ 6.5 штатов на кластер — сложно объяснить
   - k=3 → понятные группы: бедные, средние, богатые
   - Вопрос: "что общего у штатов в кластере 7?" — сложно ответить

2. ПРАКТИЧЕСКАЯ ЦЕННОСТЬ:
   - Кластеризация нужна для принятия решений
   - 3 группы легко использовать (разная политика для каждой)
   - 8 групп — слишком много для практики

3. МЕТОД ЛОКТЯ ТОЖЕ ВАЖЕН:
   - Elbow показал k=3
   - Два метода часто дают разные результаты
   - Нужно выбирать с учётом контекста задачи

4. РАЗНИЦА В СИЛУЭТЕ НЕБОЛЬШАЯ:
   - k=8: silhouette ≈ 0.60
   - k=3: silhouette ≈ 0.57
   - Разница ~5% — не критична

5. ПЕРЕОБУЧЕНИЕ:
   - Много кластеров = подгонка под шум в данных
   - На новых данных k=8 может работать хуже

КОГДА ИСПОЛЬЗОВАТЬ СИЛУЭТ k=8?
------------------------------
- Если нужна максимальная точность разделения
- Если есть реальные 8 групп в данных
- Для исследовательских целей (понять структуру данных)

КОГДА ИСПОЛЬЗОВАТЬ ЛОКОТЬ k=3?
------------------------------
- Для практических задач
- Когда важна интерпретируемость
- Когда нужно объяснить результаты руководству/заказчику

ВЫВОД: Выбор k — это КОМПРОМИСС между:
- Математической оптимальностью (силуэт → k=8)
- Практической полезностью (локоть → k=3)
- Интерпретируемостью результатов

В лабораторной работе мы выбрали k=3 для наглядности и простоты объяснения.


ПОЧЕМУ НЕ k=10?
---------------
- k=10 для 52 штатов — ПЕРЕОБУЧЕНИЕ
- В среднем 5 штатов на кластер — слишком мало
- Кластеры теряют смысл (сложно интерпретировать)
- WSS при k=10 меньше, но это НЕ значит лучше!

ПОЧЕМУ k=3 ЛОГИЧНО?
-------------------
Штаты естественно делятся на:
1. Бедные (низкий доход) — южные штаты, PR
2. Средние — большинство штатов
3. Богатые (высокий доход) — CA, NJ, CT, MA и др.


ЗАДАНИЕ 4.1d - Log10 трансформация
----------------------------------
ИЗМЕНЕНИЯ после log:
1. Точки распределены равномернее
2. Кластеры сбалансированнее по размеру
3. Выбросы меньше влияют

ПОЧЕМУ:
- Log сжимает большие значения
- Данные о доходах скошены вправо → log делает симметричнее
- Расстояния становятся пропорциональнее


ЗАДАНИЕ 4.1e - Переоценка k после log
-------------------------------------
После log-трансформации оптимальное k может УМЕНЬШИТЬСЯ:
- Данные однороднее
- Различия в доходах нормализованы
- "Локоть" виднее
- k=3 достаточно


ЗАДАНИЕ 4.1f - Выбросы
----------------------
ВЫБРОС: Puerto Rico (PR)
- Доход ~$25,000 (остальные $40,000-$85,000)

После удаления PR:
- Данные однороднее
- k=3-4 достаточно
- Кластеры компактнее


ЗАДАНИЕ 4.1g - Карта США
------------------------
48 континентальных штатов раскрашены по кластерам.
Видно географические паттерны: 
- Юг — ниже доход, выше потребление электричества (кондиционеры)
- Северо-восток — выше доход


ЗАДАНИЕ 4.1b (часть 2) - Иерархическая кластеризация
----------------------------------------------------
Строим 4 дендрограммы с разными методами связи (linkage).

COMPLETE (полная связь):
- Использует МАКСИМАЛЬНОЕ расстояние между точками кластеров
- Результат: сбалансированные кластеры примерно равного размера
- Компактные, сферические группы
- На дендрограмме: равномерное "дерево"

SINGLE (одиночная связь):
- Использует МИНИМАЛЬНОЕ расстояние
- Результат: ЭФФЕКТ ЦЕПОЧКИ — один огромный кластер + одиночки
- Puerto Rico (PR) сразу отделяется (сильно отличается)
- На дендрограмме: "лестница" с одной стороны

AVERAGE (средняя связь):
- Использует СРЕДНЕЕ расстояние между всеми парами
- Результат: компромисс между single и complete
- Кластеры более сбалансированы чем single
- На дендрограмме: похоже на complete

WARD (метод Уорда):
- Минимизирует рост ДИСПЕРСИИ при объединении
- Результат: компактные кластеры ОДИНАКОВОГО размера
- САМЫЙ ПОХОЖИЙ на K-means!
- На дендрограмме: симметричное "дерево"

ПОЧЕМУ WARD ЛУЧШЕ?
------------------
1. Минимизирует ту же функцию что и k-means (WSS)
2. Даёт интерпретируемые кластеры равного размера
3. Результаты согласуются с k-means кластеризацией
4. Меньше проблем с выбросами чем single

РАЗМЕРЫ КЛАСТЕРОВ (k=4):
- SINGLE:   1, 1, 1, 49   (цепочка!)
- COMPLETE: 13, 15, 12, 12
- AVERAGE:  10, 18, 12, 12
- WARD:     14, 13, 13, 12 (самые равные!)


═══════════════════════════════════════════════════════════════════════
ВОЗМОЖНЫЕ ВОПРОСЫ НА ЗАЩИТЕ
═══════════════════════════════════════════════════════════════════════

В: Почему k-means даёт разные результаты?
О: Случайные начальные центроиды. Решение: set.seed() и nstart.

В: Как выбрать k?
О: Метод локтя (ищем изгиб WSS) или метод силуэта (максимум).

В: Почему выбрали k=3, а не k=10?
О: k=3 — оптимум по методу локтя. k=10 — переобучение, слишком 
   много мелких кластеров без смысла.

В: Зачем логарифмировать данные?
О: Чтобы уравнять влияние переменных с разными масштабами и 
   уменьшить влияние выбросов.

В: Чем k-means отличается от иерархической?
О: K-means требует k заранее, быстрее. Иерархическая строит дерево,
   k можно выбрать потом.

В: Что такое центроид?
О: Центр кластера — среднее значение всех точек в нём.

В: Почему Puerto Rico — выброс?
О: Значительно ниже доход по сравнению с остальными штатами.

В: Какой linkage лучше?
О: Ward — создаёт компактные кластеры, похожие на k-means.

В: Что такое эффект цепочки?
О: При single linkage один кластер "засасывает" все точки по цепочке,
   остаются только одиночные выбросы.

В: Что показывает дендрограмма?
О: Дерево объединения объектов. Высота = расстояние при объединении.
   Чем выше "режем" — тем меньше кластеров.

В: Почему Ward похож на k-means?
О: Оба минимизируют внутрикластерную дисперсию (WSS).


═══════════════════════════════════════════════════════════════════════
СОЗДАННЫЕ ФАЙЛЫ
═══════════════════════════════════════════════════════════════════════
- kmeans_k10.png           — K-means k=10 (оригинальная шкала)
- kmeans_k3_optimal.png    — K-means k=3 ОПТИМАЛЬНЫЙ
- kmeans_k10_log.png       — K-means k=10 (log шкала)
- kmeans_k3_log_optimal.png — K-means k=3 log ОПТИМАЛЬНЫЙ
- us_map_clusters.png      — Карта США с кластерами
- hclust_complete.png      — Дендрограмма (complete)
- hclust_single.png        — Дендрограмма (single)
- hclust_average.png       — Дендрограмма (average)
- hclust_ward.D2.png       — Дендрограмма (Ward)


═══════════════════════════════════════════════════════════════════════
ШПАРГАЛКА ПО ФУНКЦИЯМ R
═══════════════════════════════════════════════════════════════════════
kmeans(data, centers=k, nstart=25)   — k-means кластеризация
hclust(dist(data), method="ward.D2") — иерархическая кластеризация
cutree(hc, k=4)                      — "разрезать" дендрограмму на k
silhouette(clusters, dist(data))     — оценка качества кластеризации
log10(data)                          — логарифмирование
set.seed(123)                        — фиксация случайности
dist(data)                           — матрица расстояний
rect.hclust(hc, k=4, border="red")   — нарисовать прямоугольники кластеров
